# üìã Ce qu'il faut ajouter apr√®s la configuration Loki

**Date:** 8 F√©vrier 2026

---

## ‚úÖ √âtape 1: V√©rifier que Loki fonctionne

### Tests de base

```bash
# 1. V√©rifier que Loki est d√©marr√©
ssh ubuntu@16.170.74.58
docker ps | grep loki

# 2. Tester l'endpoint
curl http://localhost:3100/ready
# R√©sultat: "ready"

# 3. V√©rifier Promtail
docker ps | grep promtail
docker logs promtail --tail 20
```

### Test dans Grafana

1. Aller sur http://16.170.74.58:3000
2. **Connections > Data sources > Loki**
3. Cliquer sur **Save & test**
4. V√©rifier: "Data source is working" ‚úÖ

---

## üìä √âtape 2: Cr√©er des Dashboards de Logs

### Dashboard 1: Logs Backend en temps r√©el

**Cr√©ation:**
1. **Dashboards > New dashboard**
2. **Add visualization > Logs**
3. **Datasource:** Loki
4. **Requ√™te:** `{container="hotel-ticket-hub-backend-staging"}`
5. **Refresh:** 5s

**Panels √† ajouter:**
- ‚úÖ Panel Logs: Tous les logs
- ‚úÖ Panel Stat: Comptage d'erreurs
- ‚úÖ Panel Pie Chart: Logs par niveau
- ‚úÖ Panel Logs: Erreurs uniquement

### Dashboard 2: Analyse des erreurs

**Requ√™tes utiles:**
```logql
# Toutes les erreurs
{container="hotel-ticket-hub-backend-staging"} |= "ERROR"

# Erreurs avec stack trace
{container="hotel-ticket-hub-backend-staging"} |= "ERROR" | json

# Comptage d'erreurs par minute
sum(count_over_time({container="hotel-ticket-hub-backend-staging"} |= "ERROR" [1m]))
```

---

## üö® √âtape 3: Configurer des Alertes

### Alerte 1: Trop d'erreurs

**Configuration:**
1. **Alerting > Alert rules > New alert rule**
2. **Query:**
   ```logql
   sum(count_over_time({container="hotel-ticket-hub-backend-staging"} |= "ERROR" [5m]))
   ```
3. **Condition:** `WHEN last() OF A IS ABOVE 10`
4. **Evaluation:** Every 1m, For 2m
5. **Notifications:** Email/Slack

### Alerte 2: Service down (pas de logs)

**Configuration:**
1. **Query:**
   ```logql
   sum(count_over_time({container="hotel-ticket-hub-backend-staging"}[5m]))
   ```
2. **Condition:** `WHEN last() OF A IS BELOW 1`
3. **Message:** "Backend ne g√©n√®re plus de logs - service peut √™tre down"

### Alerte 3: Erreurs critiques

**Configuration:**
1. **Query:**
   ```logql
   {container="hotel-ticket-hub-backend-staging"} |= "OutOfMemoryError" or {container="hotel-ticket-hub-backend-staging"} |= "NullPointerException"
   ```
2. **Condition:** `WHEN count() OF A IS ABOVE 0`
3. **Message:** "Erreur critique d√©tect√©e dans les logs"

---

## üîß √âtape 4: Am√©liorer la configuration Promtail

### Ajouter des labels personnalis√©s

√âditer `promtail/promtail-config.yml`:

```yaml
scrape_configs:
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      # Ajouter des labels
      - source_labels: [__meta_docker_container_name]
        regex: '/(.*)'
        target_label: container
      - source_labels: [__meta_docker_container_label_com_docker_compose_service]
        target_label: service
      - target_label: environment
        replacement: staging
```

### Filtrer les logs inutiles

```yaml
pipeline_stages:
  - drop:
      expression: '.*health.*'  # Ignorer les health checks
```

---

## üìà √âtape 5: Corr√©lation Logs-M√©triques

### Lier les logs aux m√©triques Prometheus

Dans Grafana, cr√©er des **derived fields**:

1. **Data sources > Loki > Settings**
2. **Derived fields:**
   - **Name:** TraceID
   - **Regex:** `traceID=(\w+)`
   - **Datasource:** Prometheus
   - **URL:** `$${__value.raw}`

Cela permet de cliquer sur un log et voir les m√©triques associ√©es.

---

## üéØ √âtape 6: Dashboards avanc√©s

### Dashboard: Vue d'ensemble compl√®te

**Panels:**
1. **M√©triques Prometheus:** CPU, Memory, Requests
2. **Logs Loki:** Logs en temps r√©el
3. **Corr√©lation:** Erreurs vs m√©triques
4. **Top erreurs:** Table des erreurs les plus fr√©quentes

### Dashboard: Performance et logs

**Requ√™tes combin√©es:**
- M√©triques: `http_server_requests_seconds_count{application="hotel-ticket-hub-backend"}`
- Logs: `{container="hotel-ticket-hub-backend-staging"} |= "slow"`
- Corr√©lation: Lier les requ√™tes lentes aux logs

---

## üìù Checklist Compl√®te

### Configuration de base
- [x] Loki d√©marr√© et healthy
- [x] Promtail collecte les logs
- [x] Datasource Loki configur√©e dans Grafana
- [x] Test de connexion r√©ussi

### Dashboards
- [ ] Dashboard logs backend en temps r√©el
- [ ] Dashboard analyse des erreurs
- [ ] Dashboard logs par niveau
- [ ] Dashboard corr√©lation logs-m√©triques

### Alertes
- [ ] Alerte: Trop d'erreurs
- [ ] Alerte: Service down
- [ ] Alerte: Erreurs critiques
- [ ] Notifications configur√©es (Email/Slack)

### Am√©liorations
- [ ] Labels personnalis√©s dans Promtail
- [ ] Filtres pour logs inutiles
- [ ] Derived fields pour corr√©lation
- [ ] Dashboards avanc√©s

---

## üöÄ Prochaines √©tapes recommand√©es

1. **Cr√©er le dashboard de logs de base** (30 min)
2. **Configurer 2-3 alertes essentielles** (20 min)
3. **Tester avec des logs r√©els** (10 min)
4. **Am√©liorer la configuration Promtail** (30 min)
5. **Cr√©er des dashboards avanc√©s** (1-2h)

---

**Derni√®re mise √† jour:** 8 F√©vrier 2026
