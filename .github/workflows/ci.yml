name: Backend CI/CD Pipeline

# ⭐ AMÉLIORATIONS APPLIQUÉES (structure métier optimisée):
# 
# GRAPHE DE DÉPENDANCES:
#   lint ─┐
#   test ─┼─→ coverage ─→ sonar
#         └─→ build ─→ docker-build ─→ deploy-staging ─→ deploy-monitoring
#   dependency-check ─┘
#   security-lint ────┘
#
# 1. ✅ Sonar dépend de coverage et réutilise jacoco.xml (pas de duplication)
# 2. ✅ Tests → Coverage → Sonar (chaîne claire)
# 3. ✅ Docker-build attend security checks (DevSecOps strict)
# 4. ✅ continue-on-error conditionnel: bloquant sur main, permissif sur develop
# 5. ✅ Nettoyage Docker sécurisé (préserve volumes Grafana/Prometheus)
# 6. ✅ Build accéléré par cache Maven (chaque job = VM séparée, pas de réutilisation directe)
# 7. ✅ Staging uniquement sur develop, main pour release
# 8. ✅ OWASP Dependency Check Action @main (temporaire, Dependabot découvrira la version taguée) et Trivy @0.28.0
# 9. ✅ OWASP et security-lint bloquants sur main

on:
  push:
    branches: [ main, develop ]
    # Optimisation : déclencher uniquement sur changements pertinents
    paths:
      - 'src/**'
      - 'pom.xml'
      - 'Dockerfile'
      - 'docker-compose*.yml'
      - '.github/workflows/**'
      - 'monitoring/**'
      - 'scripts/**'
      - 'checkstyle.xml'
      - 'spotbugs-exclude.xml'
      - 'owasp-dependency-check-suppressions.xml'
      - 'infrastructure/**'
      - '.env.example'
      - 'Makefile'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'pom.xml'
      - 'Dockerfile'
      - 'docker-compose*.yml'
      - '.github/workflows/**'
      - 'monitoring/**'
      - 'scripts/**'
      - 'checkstyle.xml'
      - 'spotbugs-exclude.xml'
      - 'owasp-dependency-check-suppressions.xml'
      - 'infrastructure/**'
      - '.env.example'
      - 'Makefile'
  schedule:
    # Security scan tous les lundis à 2h du matin
    - cron: '0 2 * * 1'

permissions:
  actions: read
  contents: read
  security-events: write

env:
  JAVA_VERSION: '17'
  MAVEN_OPTS: '-Xmx2048m -XX:+UseG1GC'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/backend
  MONITORING_DIR: /opt/monitoring

jobs:
  # ============================================
  # LINT & CODE QUALITY
  # ============================================
  lint:
    name: Backend - Lint & Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # ⭐ AMÉLIORATION: Bloquer sur main, permissif sur develop
      - name: Run Maven Checkstyle
        run: mvn -B checkstyle:check
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
      
      - name: Run Maven SpotBugs
        run: mvn -B spotbugs:spotbugs -Duser.language=en -Duser.country=US
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
        env:
          JAVA_TOOL_OPTIONS: '-Duser.language=en -Duser.country=US'
      
      - name: Upload SpotBugs report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: spotbugs-report
          path: target/spotbugsXml.xml
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true
      
      - name: Upload Checkstyle report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkstyle-report
          path: target/checkstyle-result.xml
          retention-days: 7
        continue-on-error: true

  # ============================================
  # TESTS
  # ============================================
  test:
    name: Backend - Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # ⭐ AMÉLIORATION: Utiliser verify pour compiler + tester + générer coverage en une seule fois
      # Si jacoco-maven-plugin est configuré avec prepare-agent dans le POM, verify génère déjà le rapport
      # Sinon, jacoco:report est appelé explicitement après verify
      - name: Run tests and generate coverage report
        run: mvn -B clean verify -DskipITs jacoco:report
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Verify JaCoCo XML report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found after test execution!"
            exit 1
          else
            echo "✓ jacoco.xml generated: $(ls -lh target/site/jacoco/jacoco.xml)"
          fi
        continue-on-error: false
      
      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports
          path: target/surefire-reports
          retention-days: 7
      
      - name: Upload JaCoCo Report (for Sonar)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jacoco-report
          path: target/site/jacoco
          retention-days: 30

  # ============================================
  # COVERAGE (simplifié - jacoco.xml déjà généré dans test)
  # ============================================
  coverage:
    name: Backend - Code Coverage
    runs-on: ubuntu-latest
    needs: [test]
    timeout-minutes: 5
    
    steps:
      # Checkout pour Codecov (commit metadata) et stabilité générale
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download JaCoCo Report
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
      
      - name: Check coverage threshold
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found!"
            exit 1
          fi
          echo "✓ jacoco.xml found: $(ls -lh target/site/jacoco/jacoco.xml)"
          
          # Vérifier le seuil de couverture réellement
          # Parse jacoco.xml pour extraire le coverage total (utilise Python3 disponible sur ubuntu-latest)
          # Fallback sur LINE si INSTRUCTION absent (robustesse selon versions JaCoCo)
          COVERAGE_DATA=$(python3 -c "import xml.etree.ElementTree as ET; import sys; tree = ET.parse('target/site/jacoco/jacoco.xml'); root = tree.getroot(); counter = root.find('.//counter[@type=\"INSTRUCTION\"]'); counter_type = 'INSTRUCTION' if counter is not None else None; counter = counter if counter is not None else root.find('.//counter[@type=\"LINE\"]'); counter_type = counter_type if counter_type else ('LINE' if counter is not None else 'ERROR'); missed = int(counter.get('missed', 0)) if counter is not None else 0; covered = int(counter.get('covered', 0)) if counter is not None else 0; total = missed + covered; coverage_pct = (covered / total * 100) if total > 0 else 0.0; print(f'{coverage_pct:.2f}|{missed}|{covered}|{total}|{counter_type}')" 2>/dev/null || echo "0.00|0|0|0|ERROR")
          
          COVERAGE_THRESHOLD=${COVERAGE_THRESHOLD:-60.0}
          COVERAGE_NUM=$(echo "$COVERAGE_DATA" | cut -d'|' -f1)
          MISSED=$(echo "$COVERAGE_DATA" | cut -d'|' -f2)
          COVERED=$(echo "$COVERAGE_DATA" | cut -d'|' -f3)
          TOTAL=$(echo "$COVERAGE_DATA" | cut -d'|' -f4)
          COUNTER_TYPE=$(echo "$COVERAGE_DATA" | cut -d'|' -f5)
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Coverage Analysis"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Coverage Details:"
          echo "  - Counter type used: ${COUNTER_TYPE}"
          echo "  - Covered: ${COVERED}"
          echo "  - Missed: ${MISSED}"
          echo "  - Total: ${TOTAL}"
          echo "  - Coverage percentage: ${COVERAGE_NUM}%"
          echo "  - Threshold: ${COVERAGE_THRESHOLD}%"
          
          # Vérifier que le counter existe et que les données sont valides
          if [ "$TOTAL" = "0" ] || [ -z "$COVERAGE_NUM" ] || [ "$COUNTER_TYPE" = "ERROR" ]; then
            echo "❌ ERROR: Could not parse coverage data from jacoco.xml"
            echo "   Counter types INSTRUCTION and LINE not found, or file format changed"
            echo "   This indicates a pipeline issue that needs investigation"
            if [ "${{ github.ref }}" == "refs/heads/main" ]; then
              echo "   ❌ Failing on main branch (DevSecOps strict: cannot proceed without coverage data)"
              exit 1
            else
              echo "   ⚠️  Warning only (not on main branch)"
            fi
          else
            # Comparaison simple avec awk (pas besoin de bc)
            COVERAGE_CHECK=$(awk "BEGIN {print ($COVERAGE_NUM >= $COVERAGE_THRESHOLD)}")
            if [ "$COVERAGE_CHECK" = "1" ]; then
              echo "✓ Coverage ${COVERAGE_NUM}% meets threshold ${COVERAGE_THRESHOLD}%"
            else
              echo "❌ Coverage ${COVERAGE_NUM}% is below threshold ${COVERAGE_THRESHOLD}%"
              if [ "${{ github.ref }}" == "refs/heads/main" ]; then
                exit 1
              else
                echo "⚠️  Warning only (not on main branch)"
              fi
            fi
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        env:
          COVERAGE_THRESHOLD: 60.0
        # continue-on-error géré par la logique interne du script (fail sur main, warn sur develop)
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./target/site/jacoco/jacoco.xml
          flags: backend
          name: backend-coverage
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}
        continue-on-error: true

  # ============================================
  # BUILD
  # ============================================
  build:
    name: Backend - Build
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # Note: Chaque job GitHub Actions s'exécute sur une VM séparée
      # Maven utilise le cache (~/.m2) pour accélérer la compilation
      # mais les classes compilées du job test ne sont pas réutilisées directement
      - name: Build JAR
        run: mvn -B package -DskipTests -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Check JAR size
        run: |
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            SIZE=$(du -sh "$JAR_FILE" | cut -f1)
            echo "JAR size: $SIZE"
            echo "Build successful: $JAR_FILE"
          else
            echo "Build failed - JAR file not found"
            exit 1
          fi
      
      - name: Prepare JAR artifact
        run: |
          mkdir -p artifacts
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            cp "$JAR_FILE" artifacts/
            echo "JAR artifact prepared: $(basename $JAR_FILE)"
          else
            echo "No JAR file found"
            exit 1
          fi
      
      - name: Upload JAR artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-jar
          path: artifacts/*.jar
          retention-days: 7

  # ============================================
  # SONARCloud ANALYSIS
  # ============================================
  sonar:
    name: Backend - SonarCloud Analysis
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # ⭐ AMÉLIORATION TOP 2: Bloquer sur main si Quality Gate fail, permissif sur develop
    continue-on-error: ${{ github.ref != 'refs/heads/main' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # ⭐ AMÉLIORATION TOP 1: Réutiliser jacoco.xml du job test/coverage au lieu de refaire les tests
      - name: Download JaCoCo Report from test job
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
      
      - name: Verify JaCoCo Report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found! Test job may have failed."
            echo "The test job should generate jacoco.xml via 'mvn verify jacoco:report'"
            exit 1
          else
            echo "✓ jacoco.xml found: $(ls -lh target/site/jacoco/jacoco.xml)"
            echo "✓ Coverage report will be used by SonarCloud"
          fi
      
      # Compiler les classes principales ET les classes de test pour Sonar
      # SonarCloud a besoin de target/classes ET target/test-classes
      # Pas besoin d'exécuter les tests car jacoco.xml est déjà disponible
      # Utiliser -Dmaven.test.skip=true en plus de -DskipTests pour éviter tout hook de test
      - name: Compile classes and test classes for Sonar
        run: mvn -B test-compile -DskipTests=true -Dmaven.test.skip=true -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      # Vérifier que tout est prêt pour SonarCloud (classes + coverage)
      - name: Verify SonarCloud prerequisites
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Verifying SonarCloud prerequisites..."
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Vérifier les classes compilées
          if [ ! -d "target/classes" ]; then
            echo "❌ ERROR: target/classes not found!"
            exit 1
          fi
          CLASS_COUNT=$(find target/classes -name "*.class" | wc -l)
          echo "✓ Compiled classes found: $CLASS_COUNT"
          
          # Vérifier le rapport JaCoCo
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "❌ ERROR: jacoco.xml not found!"
            echo "   Expected path: target/site/jacoco/jacoco.xml"
            exit 1
          fi
          echo "✓ JaCoCo XML report found: $(ls -lh target/site/jacoco/jacoco.xml)"
          
          # Vérifier les test classes (requis par SonarCloud)
          if [ ! -d "target/test-classes" ]; then
            echo "❌ ERROR: target/test-classes not found!"
            echo "   SonarCloud requires test classes to be compiled"
            exit 1
          fi
          TEST_CLASS_COUNT=$(find target/test-classes -name "*.class" | wc -l)
          echo "✓ Test classes found: $TEST_CLASS_COUNT"
          
          echo ""
          echo "✓ All prerequisites verified - SonarCloud can analyze with coverage"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      
      # ⭐ AMÉLIORATION TOP 1: Sonar utilise jacoco.xml du job test (pas besoin de refaire les tests)
      # ⚠️ IMPORTANT: Si vous voyez l'erreur "Automatic Analysis is enabled", 
      # désactivez l'analyse automatique dans SonarCloud :
      # 1. Allez sur https://sonarcloud.io
      # 2. Projet > Administration > Analysis Method
      # 3. Désactivez "Automatic Analysis"
      # Voir SONARCLOUD-FIX.md pour plus de détails
      # Note: Le chemin JaCoCo est configuré dans sonar-project.properties:
      # sonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml
      # ⭐ AMÉLIORATION TOP 2: Utiliser Maven SonarScanner (recommandé par SonarSource pour Maven)
      # Skip si PR depuis un fork (secrets.SONAR_TOKEN non disponible)
      - name: SonarCloud Analysis (Maven)
        id: sonarcloud-scan
        if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Starting SonarCloud Analysis..."
          echo "Organization: oumaymasaoudi"
          echo "Project Key: oumaymasaoudi_hotel-tickets-backend"
          echo "Branch: ${{ github.ref_name }}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Vérifier que le token est disponible
          if [ -z "$SONAR_TOKEN" ]; then
            echo "❌ ERROR: SONAR_TOKEN is not set!"
            exit 1
          fi
          
          # Vérifier que jacoco.xml existe
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "❌ ERROR: jacoco.xml not found at target/site/jacoco/jacoco.xml"
            exit 1
          fi
          
          mvn -B sonar:sonar \
            -DskipTests \
            -Dsonar.organization=oumaymasaoudi \
            -Dsonar.projectKey=oumaymasaoudi_hotel-tickets-backend \
            -Dsonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml \
            -Dsonar.host.url=https://sonarcloud.io
          
          SONAR_EXIT_CODE=$?
          if [ $SONAR_EXIT_CODE -ne 0 ]; then
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️ SonarCloud analysis completed with exit code: $SONAR_EXIT_CODE"
            echo "This may indicate:"
            echo "  - Quality Gate failed (check https://sonarcloud.io)"
            echo "  - Analysis error (check logs above)"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            exit $SONAR_EXIT_CODE
          fi
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      # Note: Le Quality Gate SonarCloud est vérifié automatiquement par l'action sonarcloud-github-action
      # Le continue-on-error au niveau job permet de bloquer sur main et d'être permissif sur develop
      # Si le Quality Gate échoue, le job échouera automatiquement (sauf si continue-on-error est true)
      
      - name: Verify SonarCloud Analysis Sent
        if: always()
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "SonarCloud Analysis Verification"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Project Key: oumaymasaoudi_hotel-tickets-backend"
          echo "Organization: oumaymasaoudi"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo ""
          echo "Check SonarCloud dashboard:"
          echo "https://sonarcloud.io/project/overview?id=oumaymasaoudi_hotel-tickets-backend&branch=${{ github.ref_name }}"
          echo ""
          if [ "${{ job.status }}" == "success" ]; then
            echo "Status: Analysis sent successfully to SonarCloud"
            echo "The analysis should appear in SonarCloud within a few minutes"
          else
            echo "Status: Analysis may have failed - check logs above"
            echo "Outcome: ${{ steps.sonarcloud-scan.outcome || 'skipped' }}"
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      

  # ============================================
  # SECURITY SCAN
  # ============================================
  dependency-check:
    name: Backend - OWASP Dependency Check
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # ⭐ AMÉLIORATION: Bloquer sur main si CVSS >= 7, permissif sur develop
    continue-on-error: ${{ github.ref != 'refs/heads/main' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ⭐ AMÉLIORATION: Version pinée pour stabilité (main - dernière version stable)
      # ⭐ AMÉLIORATION: Bloquer sur main si CVSS >= 7, permissif sur develop
      # Note: L'action OWASP Dependency Check utilise Docker et n'a pas besoin de Java installé sur le runner
      # Le path '.' scanne le repo entier, mais dependency-check détecte automatiquement pom.xml
      # et scanne uniquement les dépendances Maven (les exclusions évitent de scanner node_modules)
      - name: Run OWASP Dependency Check
        # Note: Utilisation temporaire de @main jusqu'à ce que Dependabot découvre les versions taguées disponibles
        # Dependabot proposera automatiquement une PR avec la bonne version taguée quand disponible
        # Pour vérifier les versions disponibles: https://github.com/dependency-check/Dependency-Check_Action/releases
        uses: dependency-check/Dependency-Check_Action@main
        with:
          project: 'hotel-ticket-hub-backend'
          path: '.'
          format: 'ALL'
          # Exclure node_modules et fichiers non-Java pour scanner uniquement les dépendances Maven
          # dependency-check détecte automatiquement pom.xml et scanne les dépendances Maven uniquement
          args: '--failOnCVSS=7 --enableRetired --enableExperimental --exclude "**/node_modules/**" --exclude "**/package*.json" --exclude "**/.git/**" --exclude "**/target/**"'
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}

      # Debug: Lister les fichiers générés par dependency-check (robustesse)
      - name: Debug Dependency-Check outputs
        if: always()
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Dependency-Check Output Files Debug"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Current directory: $(pwd)"
          echo ""
          echo "Listing reports directory:"
          ls -lah reports/ 2>/dev/null || echo "reports/ directory not found"
          echo ""
          echo "Searching for dependency-check files:"
          find . -maxdepth 4 -type f \( -iname "*dependency-check*" -o -iname "trivy-results.sarif" -o -iname "*.sarif" \) 2>/dev/null | head -20 || echo "No dependency-check files found"
          echo ""
          echo "Checking for SARIF file:"
          if [ -f "reports/dependency-check-report.sarif" ]; then
            echo "✓ SARIF file found: reports/dependency-check-report.sarif"
            ls -lh reports/dependency-check-report.sarif
          else
            echo "⚠️  SARIF file not found at reports/dependency-check-report.sarif"
          fi
          echo ""
          echo "Checking for HTML report:"
          if [ -f "reports/dependency-check-report.html" ]; then
            echo "✓ HTML report found: reports/dependency-check-report.html"
            ls -lh reports/dependency-check-report.html
          else
            echo "⚠️  HTML report not found at reports/dependency-check-report.html"
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Upload Dependency Check results (HTML)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dependency-check-report
          path: reports/dependency-check-report.html
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true

      - name: Check if SARIF file exists
        id: check_sarif
        if: always()
        run: |
          # Chercher dynamiquement le fichier SARIF (le chemin peut varier selon la version de l'action OWASP)
          # Chercher d'abord dans reports/ (emplacement standard), puis globalement
          SARIF=$(find reports -type f -name "*.sarif" 2>/dev/null | head -n 1 || true)
          if [ -z "$SARIF" ]; then
            # Fallback : chercher globalement tous les SARIF
            SARIF=$(find . -type f -name "*.sarif" 2>/dev/null | head -n 1 || true)
          fi
          if [ -n "$SARIF" ]; then
            echo "sarif_file=$SARIF" >> $GITHUB_OUTPUT
            echo "sarif_exists=true" >> $GITHUB_OUTPUT
            echo "✓ SARIF file found: $SARIF"
          else
            echo "sarif_exists=false" >> $GITHUB_OUTPUT
            echo "⚠️  SARIF file not found (this is normal if no vulnerabilities were found)"
          fi
        continue-on-error: true

      - name: Upload Dependency Check results to GitHub Security (SARIF)
        uses: github/codeql-action/upload-sarif@v4
        if: always() && steps.check_sarif.outputs.sarif_exists == 'true'
        with:
          sarif_file: ${{ steps.check_sarif.outputs.sarif_file }}
        continue-on-error: true

  security-lint:
    name: Backend - Security Linting (Trivy)
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # ⭐ AMÉLIORATION: Bloquer sur main, permissif sur develop
    continue-on-error: ${{ github.ref != 'refs/heads/main' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ⭐ AMÉLIORATION: Pinner version Trivy au lieu de @master (plus stable)
      # ⭐ AMÉLIORATION DevSecOps: exit-code conditionnel - bloquant sur main, permissif sur develop
      - name: Run Trivy vulnerability scanner
        id: trivy-scan
        uses: aquasecurity/trivy-action@0.28.0
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          # Sur main: exit-code=1 → pipeline bloqué si vulnérabilités CRITICAL/HIGH trouvées
          # Sur develop: exit-code=0 → pas d'erreur rouge, mais SARIF est uploadé quand même
          exit-code: ${{ github.ref == 'refs/heads/main' && '1' || '0' }}
          # Sur main: strict (ignore-unfixed=false) - toutes les vulnérabilités CRITICAL/HIGH bloquent
          # Sur develop: tolérant (ignore-unfixed=true) - ignore les vulnérabilités sans correctif (réduit le bruit)
          ignore-unfixed: ${{ github.ref != 'refs/heads/main' }}
      
      # Afficher un résumé des résultats Trivy pour diagnostic
      - name: Trivy Scan Summary
        if: always()
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Trivy Scan Results Summary"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          if [ "${{ steps.trivy-scan.outcome }}" == "failure" ]; then
            echo "❌ Trivy scan found CRITICAL or HIGH vulnerabilities"
            echo "Branch: ${{ github.ref_name }}"
            if [ "${{ github.ref }}" == "refs/heads/main" ]; then
              echo "Status: Pipeline BLOCKED (main branch - strict mode)"
            else
              echo "Status: Warning only (not on main branch)"
            fi
            echo ""
            echo "Check the Trivy logs above for details"
            echo "SARIF report uploaded to GitHub Security tab"
          else
            echo "✓ Trivy scan completed successfully"
            echo "No CRITICAL or HIGH vulnerabilities found"
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      # Vérifier que le fichier SARIF existe avant upload (robustesse)
      - name: Check if Trivy SARIF exists
        id: trivy_sarif
        if: always()
        run: |
          if [ -f "trivy-results.sarif" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "✓ Trivy SARIF file found"
            ls -lh trivy-results.sarif
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "⚠️  Trivy SARIF file not found (may occur if Trivy scan failed or timed out)"
          fi

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v4
        if: always() && steps.trivy_sarif.outputs.exists == 'true'
        with:
          sarif_file: 'trivy-results.sarif'

  # ============================================
  # DOCKER BUILD & PUSH (for develop branch)
  # ============================================
  docker-build:
    name: Backend - Docker Build & Push
    runs-on: ubuntu-latest
    # ⭐ AMÉLIORATION BONUS: Docker build attend security checks + SonarCloud Quality Gate (DevSecOps strict)
    # Sur main : pas d'image si Sonar Quality Gate KO
    needs: [build, dependency-check, security-lint, sonar]
    # ⭐ AMÉLIORATION: Docker build pour develop (staging) et main (release)
    # Guard: ne jamais lancer sur schedule (uniquement sur push)
    if: (github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main') && github.event_name == 'push'
    concurrency:
      group: docker-build-backend-${{ github.ref_name }}
      cancel-in-progress: false
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            # latest sera poussé uniquement dans docker-tag-release après succès de release

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ============================================
  # DEPLOY TO STAGING (for develop branch)
  # ============================================
  deploy-staging:
    name: Backend - Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker-build]
    # ⭐ AMÉLIORATION TOP 5: Staging uniquement sur develop (pas sur main)
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: staging
    timeout-minutes: 15
    # Permissif sur develop (staging peut échouer sans bloquer)
    continue-on-error: true
    concurrency:
      group: deploy-staging-${{ github.ref_name }}
      cancel-in-progress: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Optimisation : ne récupérer que le dernier commit

      - name: Verify docker-compose.yml exists
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found!"
            exit 1
          fi
          echo "docker-compose.yml found ($(du -h docker-compose.yml | cut -f1))"

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          # Ajouter l'host à known_hosts pour éviter les prompts
          ssh-keyscan -H ${{ secrets.STAGING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host backend-staging
            HostName ${{ secrets.STAGING_HOST }}
            User ${{ secrets.STAGING_USER }}
            IdentityFile ~/.ssh/deploy_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        continue-on-error: true
        id: ssh_test
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "SSH Connection Diagnostic"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo ""
          echo "Checking if secrets are configured..."
          if [ -z "${{ secrets.STAGING_HOST }}" ]; then
            echo "❌ ERROR: STAGING_HOST secret is not set"
            echo "   → Go to: Settings > Secrets and variables > Actions"
            echo "   → Add secret: STAGING_HOST = your-server-ip-or-domain"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          if [ -z "${{ secrets.STAGING_USER }}" ]; then
            echo "❌ ERROR: STAGING_USER secret is not set"
            echo "   → Go to: Settings > Secrets and variables > Actions"
            echo "   → Add secret: STAGING_USER = your-ssh-username (usually 'ubuntu' or 'ec2-user')"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          if [ -z "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" ]; then
            echo "❌ ERROR: STAGING_SSH_PRIVATE_KEY secret is not set"
            echo "   → Go to: Settings > Secrets and variables > Actions"
            echo "   → Add secret: STAGING_SSH_PRIVATE_KEY = your-private-key-content"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "✓ All required secrets are configured"
          echo ""
          echo "Connection details:"
          echo "  Host: ${{ secrets.STAGING_HOST }}"
          echo "  User: ${{ secrets.STAGING_USER }}"
          echo "  Key: [configured]"
          echo ""
          echo "Testing network connectivity..."
          if ping -c 3 -W 5 ${{ secrets.STAGING_HOST }} 2>&1; then
            echo "✓ Host is reachable via ping"
          else
            echo "⚠ WARNING: Host does not respond to ping"
            echo "  This might be normal if ICMP is blocked, but SSH should still work"
          fi
          echo ""
          echo "Testing SSH connection (timeout: 30s)..."
          SSH_OUTPUT=$(timeout 30 ssh -F ~/.ssh/config \
            -o ConnectTimeout=10 \
            -o StrictHostKeyChecking=no \
            -o BatchMode=yes \
            -o LogLevel=ERROR \
            -v backend-staging "echo 'SSH_SUCCESS'" 2>&1) || SSH_EXIT_CODE=$?
          
          if echo "$SSH_OUTPUT" | grep -q "SSH_SUCCESS"; then
            echo "✓ SSH connection successful!"
            echo "ssh_ok=true" >> $GITHUB_OUTPUT
          else
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "❌ SSH CONNECTION FAILED"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Exit code: ${SSH_EXIT_CODE:-unknown}"
            echo ""
            echo "SSH output:"
            echo "$SSH_OUTPUT" | tail -20
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "TROUBLESHOOTING STEPS:"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "1. VERIFY SECRETS IN GITHUB:"
            echo "   → Repository > Settings > Secrets and variables > Actions"
            echo "   → Ensure these secrets exist:"
            echo "     • STAGING_HOST (IP or domain)"
            echo "     • STAGING_USER (username)"
            echo "     • STAGING_SSH_PRIVATE_KEY (full private key)"
            echo ""
            echo "2. CHECK AWS SECURITY GROUP:"
            echo "   → AWS Console > EC2 > Security Groups"
            echo "   → Find the Security Group attached to your instance"
            echo "   → Inbound Rules > Add rule:"
            echo "     Type: SSH"
            echo "     Port: 22"
            echo "     Source: 0.0.0.0/0 (or GitHub Actions IP ranges)"
            echo ""
            echo "3. VERIFY INSTANCE STATUS:"
            echo "   → AWS Console > EC2 > Instances"
            echo "   → Ensure instance is 'running'"
            echo "   → Verify Public IP matches STAGING_HOST secret"
            echo ""
            echo "4. TEST SSH LOCALLY:"
            echo "   ssh -i your-key.pem ${{ secrets.STAGING_USER }}@${{ secrets.STAGING_HOST }}"
            echo ""
            echo "5. VERIFY SSH KEY FORMAT:"
            echo "   → STAGING_SSH_PRIVATE_KEY should start with:"
            echo "     '-----BEGIN OPENSSH PRIVATE KEY-----' or"
            echo "     '-----BEGIN RSA PRIVATE KEY-----'"
            echo "   → Include the BEGIN and END lines"
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            echo "::error::SSH connection failed. Check Security Group and secrets configuration."
            exit 1
          fi

      - name: Prepare staging directory
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p /opt/hotel-ticket-hub-backend-staging
            chmod 755 /opt/hotel-ticket-hub-backend-staging
      
      - name: Copy docker-compose to staging
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found in workspace"
            exit 1
          fi
          echo "Copying docker-compose.yml via SCP..."
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              docker-compose.yml \
              backend-staging:/opt/hotel-ticket-hub-backend-staging/docker-compose.yml; then
            echo "File copied successfully via SCP"
          else
            echo "SCP failed, trying alternative method via SSH..."
            if cat docker-compose.yml | ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=no \
                -o ConnectTimeout=10 \
                backend-staging "cat > /opt/hotel-ticket-hub-backend-staging/docker-compose.yml"; then
              echo "File copied successfully via SSH"
            else
              echo "ERROR: Both SCP and SSH copy methods failed"
              exit 1
            fi
          fi
      
      - name: Verify docker-compose file copied
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "/opt/hotel-ticket-hub-backend-staging/docker-compose.yml" ]; then
              echo "ERROR: docker-compose.yml not found after copy attempt"
              exit 1
            else
              echo "docker-compose.yml successfully copied"
              ls -lh /opt/hotel-ticket-hub-backend-staging/docker-compose.yml
            fi

      - name: Deploy to staging VM
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        env:
          GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
          GHCR_USERNAME: ${{ github.actor }}
          GITHUB_ACTOR: ${{ github.actor }}
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 600s  # 10 minutes pour la connexion SSH
          command_timeout: 300s  # 5 minutes pour le script (pull image + démarrage Spring Boot)
          debug: false
          use_insecure_cipher: false
          envs: GHCR_TOKEN,GHCR_USERNAME,GITHUB_ACTOR
          script: |
            set -euo pipefail  # Mode strict : erreur si variable non définie
            cd /opt/hotel-ticket-hub-backend-staging
            
            echo "Stopping existing services..."
            # Stop old systemd service if it exists
            sudo systemctl stop hotel-ticket-hub-backend-staging 2>/dev/null || true
            sudo systemctl disable hotel-ticket-hub-backend-staging 2>/dev/null || true
            
            # Stop container by name (plus fiable que filter publish=)
            echo "Stopping backend container by name..."
            docker stop hotel-ticket-hub-backend-staging 2>/dev/null || true
            docker rm hotel-ticket-hub-backend-staging 2>/dev/null || true
            
            # Fallback: Stop any container using port 8081 (si le nom a changé)
            CONTAINERS=$(docker ps -q --filter "publish=8081" 2>/dev/null || true)
            if [ -n "$CONTAINERS" ]; then
              echo "Found containers on port 8081, stopping them..."
              docker stop $CONTAINERS 2>/dev/null || true
              docker rm $CONTAINERS 2>/dev/null || true
            fi
            
            # Stop any container using port 9100 (Node Exporter) - fallback par port
            CONTAINERS_9100=$(docker ps -q --filter "publish=9100" 2>/dev/null || true)
            if [ -n "$CONTAINERS_9100" ]; then
              echo "Found containers on port 9100, stopping them..."
              docker stop $CONTAINERS_9100 2>/dev/null || true
              docker rm $CONTAINERS_9100 2>/dev/null || true
            fi
            
            echo "Checking .env file..."
            if [ ! -f .env ]; then
              echo "ERROR: .env file not found! Please create it first."
              exit 1
            fi
            
            echo "Connecting to GitHub Container Registry..."
            # Utiliser uniquement GHCR_TOKEN (PAT avec scope read:packages) passé via env
            # Le secret est passé via env: du step, pas interpolé dans le script (meilleure sécurité)
            # Note: GHCR_TOKEN doit avoir le scope 'read:packages' pour pull depuis GHCR sur une VM externe
            if [ -z "$GHCR_TOKEN" ]; then
              echo "ERROR: GHCR_TOKEN environment variable is required for pulling images from GHCR"
              echo "Create a PAT with 'read:packages' scope and add it as GHCR_TOKEN secret"
              exit 1
            fi
            GHCR_USER="${GHCR_USERNAME:-$GITHUB_ACTOR}"
            echo "$GHCR_TOKEN" | docker login ghcr.io -u "$GHCR_USER" --password-stdin
            
            echo "Pulling Docker image..."
            # Utiliser l'image par SHA pour garantir l'exactitude du commit déployé (zéro ambiguïté)
            # Note: docker-compose.yml doit utiliser ${DOCKER_IMAGE} pour que cette variable soit prise en compte
            # Exemple dans docker-compose.yml: services.backend.image: ${DOCKER_IMAGE}
            export DOCKER_IMAGE=ghcr.io/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-${{ github.sha }}
            docker pull $DOCKER_IMAGE || {
              echo "⚠️  Image by SHA not found, trying branch tag..."
              export DOCKER_IMAGE=ghcr.io/${{ env.IMAGE_NAME }}:${{ github.ref_name }}
              docker pull $DOCKER_IMAGE || docker pull ghcr.io/${{ env.IMAGE_NAME }}:latest
            }
            
            echo "Verifying Docker Compose version..."
            docker compose version || {
              echo "ERROR: docker compose (v2) not found. Checking docker-compose (v1)..."
              docker-compose version || {
                echo "ERROR: Neither docker compose nor docker-compose found"
                exit 1
              }
              COMPOSE_CMD="docker-compose"
            }
            COMPOSE_CMD="${COMPOSE_CMD:-docker compose}"
            
            echo "Starting/Updating container (force recreate if needed)..."
            # Vérifier que docker-compose.yml utilise bien ${DOCKER_IMAGE} (sinon le tag SHA sera ignoré)
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Verifying docker-compose.yml configuration..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Using DOCKER_IMAGE=$DOCKER_IMAGE"
            echo ""
            echo "Checking docker-compose.yml for image configuration:"
            grep -n "image:" docker-compose.yml || echo "⚠️  No 'image:' found in docker-compose.yml"
            if ! grep -q '\${DOCKER_IMAGE}' docker-compose.yml && ! grep -q '\$DOCKER_IMAGE' docker-compose.yml; then
              if [ "${{ github.ref }}" = "refs/heads/main" ]; then
                echo "❌ ERROR: main branch requires docker-compose.yml to use \${DOCKER_IMAGE} variable"
                echo "   This ensures deployment uses the exact commit SHA image (zero ambiguity)."
                echo "   Fix: Add 'services.backend.image: \${DOCKER_IMAGE}' to docker-compose.yml"
                exit 1
              else
                echo "⚠️  WARNING: docker-compose.yml may not use \${DOCKER_IMAGE} variable"
                echo "   To use the exact commit SHA image, ensure docker-compose.yml has:"
                echo "   services.backend.image: \${DOCKER_IMAGE}"
                echo "   Otherwise, Docker Compose will use a hardcoded image tag."
              fi
            else
              echo "✓ docker-compose.yml references DOCKER_IMAGE variable"
            fi
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            
            # Docker Compose charge automatiquement le fichier .env dans le même répertoire
            # Pas besoin de source .env (évite les problèmes avec valeurs complexes/commentaires)
            # On exporte uniquement DOCKER_IMAGE qui doit être disponible pour docker-compose.yml
            # Les autres variables du .env seront chargées automatiquement par Docker Compose
            
            # Utiliser un project name fixe pour éviter les surprises de noms de containers
            # Cela rend les logs et ps plus prévisibles
            COMPOSE_PROJECT_NAME="hotel-ticket-hub-backend-staging"
            
            # Utiliser --force-recreate pour recréer les conteneurs avec la nouvelle image
            # sans avoir besoin de faire un 'down' complet (plus rapide)
            $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" up -d --force-recreate --remove-orphans
            
            echo "Waiting for container to start (30s)..."
            sleep 30
            
            echo "Checking container status..."
            # Récupérer le nom/ID réel du container via docker compose ps (plus fiable que supposer un nom fixe)
            # Docker Compose peut nommer les containers comme <project>_<service>_1, donc on utilise compose ps
            echo "Listing containers managed by Docker Compose:"
            $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" ps
            
            # Récupérer le nom du service depuis docker-compose.yml (premier service sous services:, pas networks/volumes)
            # Utiliser awk pour parser correctement la structure YAML et trouver le premier service
            SERVICE_NAME=$(
              awk '
                $1=="services:" {in_services=1; next}
                in_services && $1 ~ /^[a-zA-Z0-9_-]+:$/ {gsub(":","",$1); print $1; exit}
                in_services && $1 ~ /^(networks|volumes):$/ {exit}
              ' docker-compose.yml
            )
            SERVICE_NAME="${SERVICE_NAME:-backend}"
            echo "Service name detected: $SERVICE_NAME"
            
            # Récupérer l'ID du container via compose ps (avec project name pour cohérence)
            CONTAINER_ID=$($COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" ps -q "$SERVICE_NAME" 2>/dev/null | head -n 1 || true)
            
            if [ -z "$CONTAINER_ID" ]; then
              echo "ERROR: Container not found via docker compose ps"
              echo "Checking all containers..."
              docker ps -a
              echo "Container logs (last 100 lines):"
              $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" logs --tail=100 2>&1 || true
              exit 1
            fi
            
            # Vérifier l'état du container via docker inspect
            RUNNING=$(docker inspect -f '{{.State.Running}}' "$CONTAINER_ID" 2>/dev/null || echo "false")
            CONTAINER_NAME=$(docker inspect -f '{{.Name}}' "$CONTAINER_ID" 2>/dev/null | sed 's|/||' || echo "unknown")
            
            if [ "$RUNNING" != "true" ]; then
              echo "ERROR: Container is not running"
              echo "Container ID: $CONTAINER_ID"
              echo "Container name: $CONTAINER_NAME"
              echo "Running state: $RUNNING"
              echo "Checking stopped containers..."
              docker ps -a | grep "$CONTAINER_NAME" || docker ps -a | grep "$CONTAINER_ID" || true
              echo "Container logs (last 100 lines):"
              $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" logs --tail=100 "$SERVICE_NAME" 2>&1 || docker logs "$CONTAINER_ID" --tail=100 2>&1 || true
              echo "Exit code:"
              docker inspect "$CONTAINER_ID" --format='{{.State.ExitCode}}' 2>/dev/null || echo "Container not found"
              exit 1
            fi
            
            echo "✓ Container is running"
            echo "  Container ID: $CONTAINER_ID"
            echo "  Container name: $CONTAINER_NAME"
            echo "Waiting for Spring Boot to fully start..."
            MAX_RETRIES=12  # 12 tentatives × 10s = 2 minutes max
            RETRY_COUNT=0
            HEALTHY=false
            
            # Note: Healthcheck depuis l'hôte nécessite que le port soit publié (ex: 8081:8080)
            # Si le port n'est pas publié, on utilise docker exec pour checker depuis l'intérieur du container
            echo "Note: Healthcheck requires published port 8081 (or will check inside container)"
            
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              sleep 10
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "Health check attempt $RETRY_COUNT/$MAX_RETRIES..."
              
              # Vérifier d'abord le HEALTHCHECK Docker si défini dans l'image
              HEALTH_STATUS=$(docker inspect "$CONTAINER_ID" --format='{{.State.Health.Status}}' 2>/dev/null || echo "none")
              if [ "$HEALTH_STATUS" = "healthy" ]; then
                echo "✓ Application is healthy (Docker HEALTHCHECK)!"
                HEALTHY=true
                break
              elif [ "$HEALTH_STATUS" = "starting" ]; then
                echo "  Docker HEALTHCHECK is starting (waiting for health probe)..."
              fi
              
              # Essayer depuis l'hôte (si port publié)
              if curl -f -s "http://localhost:8081/actuator/health" > /dev/null 2>&1; then
                echo "✓ Application is healthy (host check)!"
                HEALTHY=true
                break
              # Sinon, essayer depuis l'intérieur du container avec curl
              elif docker exec "$CONTAINER_ID" sh -c "command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/actuator/health >/dev/null 2>&1" 2>/dev/null; then
                echo "✓ Application is healthy (container check via curl)!"
                HEALTHY=true
                break
              # Fallback sur wget si curl n'est pas disponible
              elif docker exec "$CONTAINER_ID" sh -c "command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/actuator/health >/dev/null 2>&1" 2>/dev/null; then
                echo "✓ Application is healthy (container check via wget)!"
                HEALTHY=true
                break
              else
                echo "  Application not ready yet, waiting..."
              fi
            done
            
            if [ "$HEALTHY" = false ]; then
              echo "WARNING: Health check failed after $MAX_RETRIES attempts, but container is running"
              echo "Recent logs:"
              $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" logs --tail=50 "$SERVICE_NAME" 2>&1 || docker logs "$CONTAINER_ID" --tail=50 2>&1 || true
              echo "Container status:"
              docker ps -a | grep "$CONTAINER_NAME" || docker ps -a | grep "$CONTAINER_ID" || true
            fi
            
            echo "Backend deployed successfully!"
            
            # Nettoyer les credentials Docker (bonne pratique de sécurité)
            docker logout ghcr.io || true
            
            # Vérification finale de l'accessibilité depuis l'extérieur
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Final accessibility check"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Backend URL: http://${{ secrets.STAGING_HOST }}:8081"
            echo "Health endpoint: http://${{ secrets.STAGING_HOST }}:8081/actuator/health"
            echo "Prometheus endpoint: http://${{ secrets.STAGING_HOST }}:8081/actuator/prometheus"
            echo ""
            echo "⚠️  IMPORTANT: For Prometheus to scrape metrics, ensure:"
            echo "   1. Security Group allows port 8081 from Monitoring VM IP"
            echo "   2. Backend container stays running (check with: docker ps)"
            echo "   3. See scripts/fix-prometheus-connection-refused.md for troubleshooting"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/deploy_key ~/.ssh/config ~/.ssh/known_hosts
      
      - name: Deployment Summary
        if: always()
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          if [ "${{ job.status }}" == "success" ]; then
            echo "✅ DEPLOYMENT SUCCESSFUL"
            echo "Backend has been deployed to staging server"
          else
            echo "⚠️  DEPLOYMENT FAILED (but pipeline continues)"
            echo ""
            echo "Common causes:"
            echo "  • SSH secrets not configured in GitHub"
            echo "  • AWS Security Group blocking SSH (port 22)"
            echo "  • Server instance not running or IP changed"
            echo ""
            echo "Check the logs above for detailed diagnostics."
            echo "The pipeline will continue even if deployment fails."
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

  # ============================================
  # DEPLOY MONITORING (for develop branch)
  # Déployé après le backend pour s'assurer que le monitoring
  # peut scraper les métriques du backend déployé
  # ============================================
  deploy-monitoring:
    name: Backend - Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    # ⭐ AMÉLIORATION: Monitoring uniquement sur develop (staging)
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: monitoring
    timeout-minutes: 15
    continue-on-error: true
    concurrency:
      group: deploy-monitoring-${{ github.ref_name }}
      cancel-in-progress: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Verify monitoring files exist
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found!"
            exit 1
          fi
          echo "Monitoring directory found"
          echo "Files to deploy:"
          find monitoring -type f -name "*.yml" -o -name "*.yaml" -o -name "*.conf" | head -20

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.MONITORING_SSH_PRIVATE_KEY }}" > ~/.ssh/monitoring_key
          chmod 600 ~/.ssh/monitoring_key
          # Ajouter l'host à known_hosts
          ssh-keyscan -H ${{ secrets.MONITORING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host monitoring-vm
            HostName ${{ secrets.MONITORING_HOST }}
            User ${{ secrets.MONITORING_USER }}
            IdentityFile ~/.ssh/monitoring_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        run: |
          echo "Testing SSH connection to Monitoring VM..."
          timeout 10 ssh -F ~/.ssh/config -o ConnectTimeout=5 -o StrictHostKeyChecking=no monitoring-vm "echo 'SSH connection successful'" || {
            echo "ERROR: SSH connection failed"
            echo "Verify that:"
            echo "  - The Monitoring VM is accessible from GitHub Actions"
            echo "  - The AWS Security Group allows connections from GitHub"
            echo "  - The secrets MONITORING_HOST, MONITORING_USER, MONITORING_SSH_PRIVATE_KEY are correct"
            exit 1
          }
          echo "SSH connection OK"

      - name: Prepare monitoring directory
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p ${{ env.MONITORING_DIR }}
            chmod 755 ${{ env.MONITORING_DIR }}
            # Create grafana dashboards directory with correct permissions
            sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards
            sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards
            chmod 755 ${{ env.MONITORING_DIR }}/grafana/dashboards
      
      - name: Copy monitoring files to VM
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found in workspace"
            exit 1
          fi
          echo "Preparing directories on VM with correct permissions..."
          ssh -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              monitoring-vm "sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards" || true
          
          echo "Copying monitoring files via SCP..."
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              -r monitoring/* \
              monitoring-vm:${{ env.MONITORING_DIR }}/; then
            echo "Files copied successfully via SCP"
            # Fix permissions for grafana dashboards if needed
            ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=no \
                -o ConnectTimeout=10 \
                monitoring-vm "sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards 2>/dev/null || true"
          else
            echo "SCP failed, trying alternative method via SSH..."
            cd monitoring
            for file in $(find . -type f); do
              target_dir="${{ env.MONITORING_DIR }}/$(dirname "$file")"
              # Use sudo for grafana/dashboards directory
              if echo "$file" | grep -q "grafana/dashboards"; then
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo mkdir -p $target_dir && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo tee ${{ env.MONITORING_DIR }}/$file > /dev/null && sudo chown ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              else
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "mkdir -p $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "cat > ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              fi
            done
            echo "Files copied successfully via SSH"
          fi
      
      - name: Verify monitoring files copied
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "${{ env.MONITORING_DIR }}/docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found after copy attempt"
              exit 1
            else
              echo "Monitoring files successfully copied"
              ls -lh ${{ env.MONITORING_DIR }}/
            fi

      - name: Deploy Monitoring Stack
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 300s
          command_timeout: 120s
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail
            cd ${{ env.MONITORING_DIR }}
            
            echo "Verifying configuration files..."
            if [ ! -f "docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found!"
              exit 1
            fi
            if [ ! -f "prometheus/prometheus.yml" ]; then
              echo "ERROR: prometheus/prometheus.yml not found!"
              exit 1
            fi
            echo "Configuration files found"
            
            # OPTIMISATION: Utiliser --force-recreate au lieu de down complet
            # Cela évite de supprimer les volumes et réseaux inutilement
            echo "Updating monitoring services (force recreate if needed)..."
            # --force-recreate recrée les conteneurs avec les nouvelles images
            # --remove-orphans supprime les conteneurs orphelins
            # Pas besoin de 'down' complet sauf en cas de problème majeur
            
            echo "Checking disk space..."
            df -h / | tail -1
            
            echo "Safe Docker disk space cleanup (preserving volumes)..."
            
            # ⭐ AMÉLIORATION: Nettoyage sécurisé sans supprimer les volumes (Grafana/Prometheus)
            # Arrêter uniquement les conteneurs non critiques (pas ceux du monitoring)
            echo "Stopping non-critical containers..."
            docker ps --format "{{.Names}}" | grep -v -E "(grafana|prometheus|loki|alertmanager|node-exporter|cadvisor|promtail)" | xargs -r docker stop 2>/dev/null || true
            
            # Supprimer uniquement les conteneurs arrêtés non critiques
            echo "Removing stopped non-critical containers..."
            docker ps -a --format "{{.Names}}" | grep -v -E "(grafana|prometheus|loki|alertmanager|node-exporter|cadvisor|promtail)" | xargs -r docker rm 2>/dev/null || true
            
            # Supprimer les images non utilisées (sauf celles du monitoring)
            # Supprimer les images non utilisées de +24h (les conteneurs monitoring en cours utilisent leurs images)
            # Note: Les images utilisées par les conteneurs actifs ne seront pas supprimées
            echo "Removing unused images (older than 24h)..."
            docker image prune -af --filter "until=24h" || true
            
            # Nettoyer le build cache uniquement
            echo "Cleaning build cache..."
            docker builder prune -af --filter "until=24h" || true
            
            # Nettoyer les logs Docker (limité à 1 jour)
            echo "Cleaning Docker logs (older than 1 day)..."
            find /var/lib/docker/containers/ -type f -name "*.log" -mtime +1 -delete 2>/dev/null || true
            journalctl --vacuum-time=1d 2>/dev/null || true
            
            # ⭐ AMÉLIORATION: Ne JAMAIS supprimer les volumes (risque de perte de données Grafana/Prometheus)
            # docker volume prune est DANGEREUX - on ne le fait PAS
            
            # Nettoyer uniquement les réseaux non utilisés (pas ceux du monitoring)
            echo "Removing unused networks (preserving monitoring-network)..."
            docker network ls --format "{{.Name}}" | grep -v -E "(monitoring-network|bridge|host)" | xargs -r docker network rm 2>/dev/null || true
            
            # Nettoyage système sécurisé (SANS --volumes pour préserver les données)
            echo "Safe system prune (preserving volumes)..."
            docker system prune -af --filter "until=24h" || true
            
            # Vérifier l'espace utilisé par Docker
            echo "Docker disk usage:"
            docker system df || true
            
            echo "Disk space after cleanup:"
            df -h / | tail -1
            
            # Si le disque est toujours plein, afficher un warning (ne pas supprimer containerd snapshots - trop risqué)
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ "$DISK_USAGE" -gt 90 ]; then
              echo "⚠️  WARNING: Disk usage is still above 90% ($DISK_USAGE%)"
              echo "   Manual intervention required:"
              echo "   - Consider increasing EBS volume size (AWS)"
              echo "   - Review and remove old Docker images manually"
              echo "   - Do NOT delete containerd snapshots (can break Docker)"
              echo "   Current disk usage:"
              df -h /
              echo "   Docker disk usage:"
              docker system df || true
            fi
            
            # Vérifier l'espace disque avant de pull les images
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ "$DISK_USAGE" -gt 95 ]; then
              echo "ERROR: Disk usage is still above 95% ($DISK_USAGE%), cannot pull images safely"
              echo "Current disk usage:"
              df -h /
              echo "Docker disk usage:"
              docker system df || true
              exit 1
            fi
            
            echo "Verifying Docker Compose version..."
            docker compose version || {
              echo "ERROR: docker compose (v2) not found. Checking docker-compose (v1)..."
              docker-compose version || {
                echo "ERROR: Neither docker compose nor docker-compose found"
                exit 1
              }
              COMPOSE_CMD="docker-compose"
            }
            COMPOSE_CMD="${COMPOSE_CMD:-docker compose}"
            
            echo "Pulling Docker images (disk usage: ${DISK_USAGE}%)..."
            $COMPOSE_CMD -f docker-compose.monitoring.yml pull
            
            echo "Starting monitoring stack..."
            # Démarrer avec gestion d'erreur pour le réseau
            set +e  # Ne pas arrêter sur les erreurs
            $COMPOSE_CMD -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
            COMPOSE_EXIT_CODE=$?
            set -e  # Réactiver l'arrêt sur erreur
            
            if [ $COMPOSE_EXIT_CODE -ne 0 ]; then
              echo "WARNING: Error detected, checking network..."
              # Si l'erreur est liée au réseau, supprimer le réseau et réessayer
              if docker network ls | grep -q monitoring-network; then
                echo "Force removing problematic network..."
                # Arrêter tous les conteneurs qui utilisent ce réseau
                CONTAINER_IDS=$(docker ps --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$CONTAINER_IDS" ]; then
                  echo "$CONTAINER_IDS" | xargs docker stop 2>/dev/null || true
                  echo "$CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                OLD_CONTAINER_IDS=$(docker ps -a --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$OLD_CONTAINER_IDS" ]; then
                  echo "$OLD_CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                # Supprimer le réseau
                docker network rm monitoring-network 2>/dev/null || docker network prune -f
                sleep 3
                echo "Retrying startup..."
                $COMPOSE_CMD -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
              else
                echo "ERROR: Non-network related error, see logs"
                $COMPOSE_CMD -f docker-compose.monitoring.yml logs
                exit 1
              fi
            fi
            
            echo "Waiting for services to start (30s)..."
            sleep 30
            
            echo "Container status:"
            $COMPOSE_CMD -f docker-compose.monitoring.yml ps
            
            echo "Checking services..."
            
            # Vérifier que les conteneurs sont en cours d'exécution
            echo "Verifying containers are running..."
            
            # Vérifier Prometheus
            if docker ps | grep -q prometheus; then
              PROMETHEUS_STATUS=$(docker ps --filter "name=prometheus" --format "{{.Status}}")
              echo "Prometheus is running: $PROMETHEUS_STATUS"
            else
              echo "ERROR: Prometheus container is not running"
              $COMPOSE_CMD -f docker-compose.monitoring.yml logs prometheus | tail -30
              exit 1
            fi
            
            # Vérifier Grafana (peut prendre plus de temps pour le health check)
            GRAFANA_RUNNING=$(docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana && echo "yes" || echo "no")
            
            if [ "$GRAFANA_RUNNING" = "yes" ]; then
              GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
              echo "Grafana container found: $GRAFANA_STATUS"
              
              # Si Grafana est encore en "health: starting", attendre un peu plus
              if echo "$GRAFANA_STATUS" | grep -q "health: starting"; then
                echo "Grafana is still starting, waiting additional 30s..."
                sleep 30
                GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
                echo "Grafana status after wait: $GRAFANA_STATUS"
                
                # Vérifier à nouveau si Grafana est toujours en cours d'exécution
                if ! docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana; then
                  echo "ERROR: Grafana container stopped during startup"
                  $COMPOSE_CMD -f docker-compose.monitoring.yml logs grafana | tail -50
                  exit 1
                fi
              fi
              
              echo "Grafana is running successfully"
            else
              echo "ERROR: Grafana container is not running"
              echo "Checking all containers:"
              docker ps -a | grep grafana || echo "No grafana container found"
              echo "Grafana logs:"
              $COMPOSE_CMD -f docker-compose.monitoring.yml logs grafana | tail -50
              exit 1
            fi
            
            # Vérifier les autres services essentiels
            for service in alertmanager node-exporter cadvisor; do
              if docker ps | grep -q "$service"; then
                SERVICE_STATUS=$(docker ps --filter "name=$service" --format "{{.Status}}")
                echo "$service is running: $SERVICE_STATUS"
              else
                echo "WARNING: $service container is not running (non-critical)"
              fi
            done
            
            echo "Recent logs (last 10 lines):"
            $COMPOSE_CMD -f docker-compose.monitoring.yml logs --tail=10
            
            echo "Monitoring stack deployed successfully!"
            echo "Prometheus should now scrape the backend on ${{ secrets.STAGING_HOST }}:8081"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/monitoring_key ~/.ssh/config ~/.ssh/known_hosts

  # ============================================
  # RELEASE (semantic versioning + changelog)
  # ============================================
  release:
    name: Release
    runs-on: ubuntu-latest
    needs: [lint, test, coverage, build]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    concurrency:
      group: release-main
      cancel-in-progress: false
    outputs:
      package_exists: ${{ steps.verify_package.outputs.package_exists }}
    permissions:
      contents: write
      issues: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      
      # Sécuriser : vérifier que package.json existe (requis pour semantic-release)
      - name: Verify package.json exists
        id: verify_package
        run: |
          if [ ! -f "package.json" ]; then
            echo "package_exists=false" >> $GITHUB_OUTPUT
            echo "⚠️  package.json not found. Release job requires package.json for semantic-release."
            echo "Skipping release job (this is expected if package.json doesn't exist)."
          else
            echo "package_exists=true" >> $GITHUB_OUTPUT
            echo "✓ package.json found"
          fi

      # Indiquer clairement que le job est skip si package.json n'existe pas
      # (Les steps suivants sont conditionnels avec if: package_exists == 'true', donc ils ne s'exécuteront pas)
      - name: Skip release job if package.json missing
        if: steps.verify_package.outputs.package_exists != 'true'
        run: |
          echo "Skipping release (no package.json)."

      - name: Setup Node.js
        if: steps.verify_package.outputs.package_exists == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          # Cache npm sans cache-dependency-path pour tolérance si package-lock.json absent
          # Le cache fonctionne quand même mais est moins précis sans lockfile
          cache: 'npm'

      - name: Install semantic-release dependencies
        if: steps.verify_package.outputs.package_exists == 'true'
        run: |
          # Utiliser npm ci si package-lock.json existe (plus rapide et reproductible)
          # Sinon fallback sur npm install
          if [ -f "package-lock.json" ]; then
            echo "Using npm ci (package-lock.json found) - faster and more reproducible"
            npm ci --legacy-peer-deps --no-audit --no-fund
          else
            echo "Using npm install (no package-lock.json found)"
            rm -rf node_modules
            npm install --legacy-peer-deps --no-audit --no-fund
          fi
          
          # Vérification basique : semantic-release doit être disponible
          if [ ! -f "node_modules/.bin/semantic-release" ]; then
            echo "ERROR: semantic-release binary not found after installation"
            echo "This should not happen with npm ci. Checking installation..."
            npm list semantic-release || true
            exit 1
          fi
          
          echo "✓ Dependencies installed successfully"
          echo "Verifying semantic-release is available..."
          npm list semantic-release --depth=0 || true

      - name: Setup JDK ${{ env.JAVA_VERSION }}
        if: steps.verify_package.outputs.package_exists == 'true'
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'

      - name: Run semantic-release
        if: steps.verify_package.outputs.package_exists == 'true'
        id: semantic_release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: npx semantic-release

  # ============================================
  # DOCKER TAG RELEASE (tag versionné après semantic-release)
  # ============================================
  docker-tag-release:
    name: Backend - Docker Tag Release Version
    runs-on: ubuntu-latest
    needs: [release, docker-build]
    # Ne s'exécute que si release a réussi ET package.json existe (semantic-release a pu créer un tag)
    if: github.ref == 'refs/heads/main' && github.event_name == 'push' && needs.release.result == 'success' && needs.release.outputs.package_exists == 'true'
    concurrency:
      group: release-main
      cancel-in-progress: false
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get latest Git tag (version)
        id: git_tag
        run: |
          # Récupérer le tag Git qui correspond exactement au commit courant (plus sûr que git describe)
          # semantic-release tag toujours HEAD, donc on vérifie que le tag existe sur ce commit
          # Si plusieurs tags existent sur HEAD, prendre le plus haut (v2.3.1 > v2.3.0)
          LATEST_TAG=$(git tag --points-at HEAD --sort=-v:refname | head -n 1 || echo "")
          if [ -z "$LATEST_TAG" ]; then
            echo "tag_exists=false" >> $GITHUB_OUTPUT
            echo "⚠️  No Git tag found on current commit. Skipping Docker release tag."
            exit 0
          fi
          # Enlever le préfixe "v" pour le tag Docker (v1.2.3 -> 1.2.3)
          VERSION="${LATEST_TAG#v}"
          echo "✓ Found Git tag on current commit: $LATEST_TAG (docker tag: $VERSION)"
          echo "tag_exists=true" >> $GITHUB_OUTPUT
          echo "git_tag=$LATEST_TAG" >> $GITHUB_OUTPUT
          echo "version=$VERSION" >> $GITHUB_OUTPUT

      - name: Set up Docker Buildx
        if: steps.git_tag.outputs.tag_exists == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        if: steps.git_tag.outputs.tag_exists == 'true'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Tag Docker image with release version (from commit SHA)
        if: steps.git_tag.outputs.tag_exists == 'true'
        run: |
          # Image exacte produite par docker-build pour ce commit (plus sûr que :main)
          SOURCE_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:main-${{ github.sha }}"
          TARGET_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.git_tag.outputs.version }}"
          LATEST_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          
          echo "Tagging Docker image with version..."
          echo "Source: $SOURCE_IMAGE"
          echo "Target version: $TARGET_IMAGE"
          echo "Target latest: $LATEST_IMAGE"
          
          # Pull l'image source par SHA (garantit que c'est l'image du commit release)
          echo "Expected source tag: main-${{ github.sha }}"
          docker pull "$SOURCE_IMAGE" || {
            echo "❌ ERROR: Source image not found. docker-build may have failed for this commit."
            echo "Expected image: $SOURCE_IMAGE"
            echo "If missing: check docker-build logs and GHCR permissions."
            exit 1
          }
          
          # Tag l'image avec la version (sans le préfixe "v")
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "✓ Successfully pushed version tag: $TARGET_IMAGE"
          
          # Tag et push latest uniquement après succès de release (plus propre que dans docker-build)
          docker tag "$SOURCE_IMAGE" "$LATEST_IMAGE"
          docker push "$LATEST_IMAGE"
          echo "✓ Successfully pushed latest tag: $LATEST_IMAGE"
