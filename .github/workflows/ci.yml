name: Backend CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - '**'
      - '.github/workflows/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - '**'
  schedule:
    # Security scan tous les lundis à 2h du matin
    - cron: '0 2 * * 1'

permissions:
  actions: read
  contents: read
  security-events: write

env:
  JAVA_VERSION: '17'
  MAVEN_OPTS: '-Xmx2048m -XX:+UseG1GC'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/backend
  MONITORING_DIR: /opt/monitoring

jobs:
  # ============================================
  # LINT & CODE QUALITY
  # ============================================
  lint:
    name: Backend - Lint & Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Run Maven Checkstyle
        run: mvn checkstyle:check
        continue-on-error: true
      
      - name: Run Maven SpotBugs
        run: mvn spotbugs:spotbugs -Duser.language=en -Duser.country=US
        continue-on-error: true
        env:
          JAVA_TOOL_OPTIONS: '-Duser.language=en -Duser.country=US'
      
      - name: Upload SpotBugs report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: spotbugs-report
          path: target/spotbugsXml.xml
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true
      
      - name: Upload Checkstyle report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkstyle-report
          path: target/checkstyle-result.xml
          retention-days: 7
        continue-on-error: true

  # ============================================
  # TESTS
  # ============================================
  test:
    name: Backend - Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Run tests with JaCoCo agent
        run: mvn clean test -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports
          path: target/surefire-reports
          retention-days: 7
      
      - name: Upload compiled classes and JaCoCo data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-classes-jacoco
          path: |
            target/classes
            target/test-classes
            target/jacoco.exec
          retention-days: 7

  # ============================================
  # COVERAGE
  # ============================================
  coverage:
    name: Backend - Code Coverage
    runs-on: ubuntu-latest
    needs: [test]
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Download compiled classes and JaCoCo data
        uses: actions/download-artifact@v4
        with:
          name: test-classes-jacoco
          path: target
      
      - name: Generate coverage report
        run: mvn -B jacoco:report
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Check coverage threshold (optional)
        run: |
          if mvn -B jacoco:check; then
            echo "Coverage threshold met!"
          else
            echo "Coverage threshold not met, but continuing..."
          fi
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
        continue-on-error: true
      
      - name: Upload JaCoCo Report
        uses: actions/upload-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
          retention-days: 30
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./target/site/jacoco/jacoco.xml
          flags: backend
          name: backend-coverage
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}
        continue-on-error: true

  # ============================================
  # BUILD
  # ============================================
  build:
    name: Backend - Build
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Build with Maven
        run: mvn clean package -DskipTests -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Check JAR size
        run: |
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            SIZE=$(du -sh "$JAR_FILE" | cut -f1)
            echo "JAR size: $SIZE"
            echo "Build successful: $JAR_FILE"
          else
            echo "Build failed - JAR file not found"
            exit 1
          fi
      
      - name: Prepare JAR artifact
        run: |
          mkdir -p artifacts
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            cp "$JAR_FILE" artifacts/
            echo "JAR artifact prepared: $(basename $JAR_FILE)"
          else
            echo "No JAR file found"
            exit 1
          fi
      
      - name: Upload JAR artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-jar
          path: artifacts/*.jar
          retention-days: 7

  # ============================================
  # SONARCloud ANALYSIS
  # ============================================
  sonar:
    name: Backend - SonarCloud Analysis
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    continue-on-error: true
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Run tests with coverage
        run: mvn -B clean test jacoco:report
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Verify JaCoCo Report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: JaCoCo report not found!"
            exit 1
          else
            echo "JaCoCo report found: target/site/jacoco/jacoco.xml"
            ls -lh target/site/jacoco/jacoco.xml
          fi
      
      - name: Verify Java binaries exist
        run: |
          if [ ! -d "target/classes" ]; then
            echo "ERROR: target/classes directory not found!"
            echo "Compiling classes..."
            mvn -B compile
          else
            echo "Java binaries found: target/classes"
            ls -lh target/classes | head -10
          fi
      
      - name: SonarCloud Scan
        id: sonarcloud-scan
        uses: SonarSource/sonarcloud-github-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        continue-on-error: true
      
      - name: Verify SonarCloud Analysis Sent
        if: always()
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "SonarCloud Analysis Verification"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Project Key: oumaymasaoudi_hotel-tickets-backend"
          echo "Organization: oumaymasaoudi"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo ""
          echo "Check SonarCloud dashboard:"
          echo "https://sonarcloud.io/project/overview?id=oumaymasaoudi_hotel-tickets-backend&branch=${{ github.ref_name }}"
          echo ""
          if [ "${{ job.status }}" == "success" ]; then
            echo "Status: Analysis sent successfully to SonarCloud"
            echo "The analysis should appear in SonarCloud within a few minutes"
          else
            echo "Status: Analysis may have failed - check logs above"
            echo "Exit code: ${{ steps.sonarcloud-scan.outcome }}"
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      

  # ============================================
  # SECURITY SCAN
  # ============================================
  dependency-check:
    name: Backend - OWASP Dependency Check
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'schedule'
    continue-on-error: true
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'

      - name: Run OWASP Dependency Check
        uses: dependency-check/Dependency-Check_Action@main
        with:
          project: 'hotel-ticket-hub-backend'
          path: '.'
          format: 'HTML'
          args: >
            --failOnCVSS 7
            --suppression owasp-dependency-check-suppressions.xml
            --enableRetired
            --enableExperimental
        continue-on-error: true

      - name: Upload Dependency Check results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dependency-check-report
          path: reports/dependency-check-report.html
          retention-days: 7
          if-no-files-found: ignore

  security-lint:
    name: Backend - Security Linting (Trivy)
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'schedule'
    continue-on-error: true
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # ============================================
  # DOCKER BUILD & PUSH (for develop branch)
  # ============================================
  docker-build:
    name: Backend - Docker Build & Push
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ============================================
  # DEPLOY TO STAGING (for develop branch)
  # ============================================
  deploy-staging:
    name: Backend - Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker-build]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: staging
    timeout-minutes: 15  # Augmenté pour gérer les connexions SSH lentes
    concurrency:
      group: deploy-staging
      cancel-in-progress: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Optimisation : ne récupérer que le dernier commit

      - name: Verify docker-compose.yml exists
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found!"
            exit 1
          fi
          echo "docker-compose.yml found ($(du -h docker-compose.yml | cut -f1))"

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          # Ajouter l'host à known_hosts pour éviter les prompts
          ssh-keyscan -H ${{ secrets.STAGING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host backend-staging
            HostName ${{ secrets.STAGING_HOST }}
            User ${{ secrets.STAGING_USER }}
            IdentityFile ~/.ssh/deploy_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        continue-on-error: true
        id: ssh_test
        run: |
          echo "Testing SSH connection..."
          echo "Host: ${{ secrets.STAGING_HOST }}"
          echo "User: ${{ secrets.STAGING_USER }}"
          echo ""
          echo "Checking if secrets are set..."
          if [ -z "${{ secrets.STAGING_HOST }}" ] || [ -z "${{ secrets.STAGING_USER }}" ] || [ -z "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" ]; then
            echo "ERROR: One or more secrets are missing"
            echo "Required secrets: STAGING_HOST, STAGING_USER, STAGING_SSH_PRIVATE_KEY"
            echo "::error::Missing required secrets for SSH connection"
            exit 1
          fi
          echo "OK: Secrets are set"
          echo ""
          echo "Testing network connectivity (ping)..."
          if ping -c 2 -W 5 ${{ secrets.STAGING_HOST }} > /dev/null 2>&1; then
            echo "OK: VM responds to ping"
          else
            echo "WARNING: VM does not respond to ping (may be blocked by Security Group)"
          fi
          echo ""
          echo "Testing SSH connection..."
          if timeout 30 ssh -F ~/.ssh/config -o ConnectTimeout=10 -o StrictHostKeyChecking=no -o BatchMode=yes backend-staging "echo 'SSH connection successful'" 2>&1; then
            echo "OK: SSH connection successful"
            echo "ssh_ok=true" >> $GITHUB_OUTPUT
          else
            SSH_EXIT_CODE=$?
            echo ""
            echo "=========================================="
            echo "ERROR: SSH connection failed (exit code: $SSH_EXIT_CODE)"
            echo "=========================================="
            echo ""
            echo "Possible causes:"
            echo "1. AWS Security Group blocks SSH connections"
            echo "   → Solution: Add inbound rule for SSH (port 22) from 0.0.0.0/0"
            echo "   → AWS Console: EC2 > Security Groups > [Your SG] > Inbound rules > Add rule"
            echo ""
            echo "2. VM IP address changed or instance stopped"
            echo "   → Solution: Verify instance is running and IP is correct"
            echo "   → Current IP in secret: ${{ secrets.STAGING_HOST }}"
            echo ""
            echo "3. SSH key incorrect or expired"
            echo "   → Solution: Verify STAGING_SSH_PRIVATE_KEY secret is correct"
            echo ""
            echo "Quick fix via AWS Console:"
            echo "1. Go to https://console.aws.amazon.com/ec2"
            echo "2. Find instance with IP ${{ secrets.STAGING_HOST }}"
            echo "3. Open Security Group > Inbound rules"
            echo "4. Add rule: Type=SSH, Port=22, Source=0.0.0.0/0"
            echo ""
            echo "See scripts/fix-github-actions-ssh.md for detailed troubleshooting"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            echo "::error::SSH connection failed. Check Security Group configuration."
            exit 1
          fi

      - name: Prepare staging directory
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p /opt/hotel-ticket-hub-backend-staging
            chmod 755 /opt/hotel-ticket-hub-backend-staging
      
      - name: Copy docker-compose to staging
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found in workspace"
            exit 1
          fi
          echo "Copying docker-compose.yml via SCP..."
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              docker-compose.yml \
              backend-staging:/opt/hotel-ticket-hub-backend-staging/docker-compose.yml; then
            echo "File copied successfully via SCP"
          else
            echo "SCP failed, trying alternative method via SSH..."
            if cat docker-compose.yml | ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=no \
                -o ConnectTimeout=10 \
                backend-staging "cat > /opt/hotel-ticket-hub-backend-staging/docker-compose.yml"; then
              echo "File copied successfully via SSH"
            else
              echo "ERROR: Both SCP and SSH copy methods failed"
              exit 1
            fi
          fi
      
      - name: Verify docker-compose file copied
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "/opt/hotel-ticket-hub-backend-staging/docker-compose.yml" ]; then
              echo "ERROR: docker-compose.yml not found after copy attempt"
              exit 1
            else
              echo "docker-compose.yml successfully copied"
              ls -lh /opt/hotel-ticket-hub-backend-staging/docker-compose.yml
            fi

      - name: Deploy to staging VM
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 300s
          command_timeout: 120s
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail  # Mode strict : erreur si variable non définie
            cd /opt/hotel-ticket-hub-backend-staging
            
            echo "Stopping existing services..."
            # Stop old systemd service if it exists
            sudo systemctl stop hotel-ticket-hub-backend-staging 2>/dev/null || true
            sudo systemctl disable hotel-ticket-hub-backend-staging 2>/dev/null || true
            
            # Stop any container using port 8081
            CONTAINERS=$(docker ps -q --filter "publish=8081" 2>/dev/null || true)
            if [ -n "$CONTAINERS" ]; then
              docker stop $CONTAINERS 2>/dev/null || true
            fi
            
            # Stop any container using port 9100 (Node Exporter)
            CONTAINERS_9100=$(docker ps -q --filter "publish=9100" 2>/dev/null || true)
            if [ -n "$CONTAINERS_9100" ]; then
              docker stop $CONTAINERS_9100 2>/dev/null || true
              docker rm $CONTAINERS_9100 2>/dev/null || true
            fi
            
            OLD_CONTAINERS=$(docker ps -aq --filter "name=hotel-ticket-hub-backend-staging" --filter "name=node-exporter-backend" 2>/dev/null || true)
            if [ -n "$OLD_CONTAINERS" ]; then
              docker rm -f $OLD_CONTAINERS 2>/dev/null || true
            fi
            
            # Stop and remove old docker-compose containers
            docker compose down 2>/dev/null || true
            
            echo "Checking .env file..."
            if [ ! -f .env ]; then
              echo "ERROR: .env file not found! Please create it first."
              exit 1
            fi
            
            echo "Connecting to GitHub Container Registry..."
            echo "${{ secrets.GHCR_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
            
            echo "Pulling Docker image..."
            export DOCKER_IMAGE=ghcr.io/${{ env.IMAGE_NAME }}:develop
            docker pull $DOCKER_IMAGE || docker pull ghcr.io/${{ env.IMAGE_NAME }}:latest
            
            echo "Starting container..."
            export DOCKER_IMAGE=$DOCKER_IMAGE
            docker compose --env-file .env up -d
            
            echo "Waiting for container to start (30s)..."
            sleep 30
            
            echo "Checking container status..."
            CONTAINER_ID=$(docker ps -q --filter "name=hotel-ticket-hub-backend-staging" 2>/dev/null || true)
            if [ -z "$CONTAINER_ID" ]; then
              echo "ERROR: Container is not running"
              echo "Checking stopped containers..."
              docker ps -a | grep hotel-ticket-hub-backend-staging || true
              echo "Container logs (last 100 lines):"
              docker compose logs --tail=100 hotel-ticket-hub-backend-staging || docker logs hotel-ticket-hub-backend-staging --tail=100 2>&1 || true
              echo "Exit code:"
              docker inspect hotel-ticket-hub-backend-staging --format='{{.State.ExitCode}}' 2>/dev/null || echo "Container not found"
              exit 1
            fi
            
            echo "Container is running (ID: $CONTAINER_ID)"
            echo "Waiting for Spring Boot to fully start (30s)..."
            sleep 30
            
            echo "Checking application health..."
            HEALTH_CHECK_URL="http://localhost:8081/actuator/health"
            if curl -f -s "$HEALTH_CHECK_URL" > /dev/null 2>&1; then
              echo "Application is healthy!"
            else
              echo "WARNING: Health check failed, but container is running"
              echo "Recent logs:"
              docker compose logs --tail=50 hotel-ticket-hub-backend-staging || docker logs hotel-ticket-hub-backend-staging --tail=50 2>&1 || true
            fi
            
            echo "Backend deployed successfully!"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/deploy_key ~/.ssh/config ~/.ssh/known_hosts

  # ============================================
  # DEPLOY MONITORING (for develop branch)
  # Déployé après le backend pour s'assurer que le monitoring
  # peut scraper les métriques du backend déployé
  # ============================================
  deploy-monitoring:
    name: Backend - Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: monitoring
    timeout-minutes: 15
    concurrency:
      group: deploy-monitoring
      cancel-in-progress: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Verify monitoring files exist
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found!"
            exit 1
          fi
          echo "Monitoring directory found"
          echo "Files to deploy:"
          find monitoring -type f -name "*.yml" -o -name "*.yaml" -o -name "*.conf" | head -20

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.MONITORING_SSH_PRIVATE_KEY }}" > ~/.ssh/monitoring_key
          chmod 600 ~/.ssh/monitoring_key
          # Ajouter l'host à known_hosts
          ssh-keyscan -H ${{ secrets.MONITORING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host monitoring-vm
            HostName ${{ secrets.MONITORING_HOST }}
            User ${{ secrets.MONITORING_USER }}
            IdentityFile ~/.ssh/monitoring_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        run: |
          echo "Testing SSH connection to Monitoring VM..."
          timeout 10 ssh -F ~/.ssh/config -o ConnectTimeout=5 -o StrictHostKeyChecking=no monitoring-vm "echo 'SSH connection successful'" || {
            echo "ERROR: SSH connection failed"
            echo "Verify that:"
            echo "  - The Monitoring VM is accessible from GitHub Actions"
            echo "  - The AWS Security Group allows connections from GitHub"
            echo "  - The secrets MONITORING_HOST, MONITORING_USER, MONITORING_SSH_PRIVATE_KEY are correct"
            exit 1
          }
          echo "SSH connection OK"

      - name: Prepare monitoring directory
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p ${{ env.MONITORING_DIR }}
            chmod 755 ${{ env.MONITORING_DIR }}
            # Create grafana dashboards directory with correct permissions
            sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards
            sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards
            chmod 755 ${{ env.MONITORING_DIR }}/grafana/dashboards
      
      - name: Copy monitoring files to VM
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found in workspace"
            exit 1
          fi
          echo "Preparing directories on VM with correct permissions..."
          ssh -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              monitoring-vm "sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards" || true
          
          echo "Copying monitoring files via SCP..."
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              -r monitoring/* \
              monitoring-vm:${{ env.MONITORING_DIR }}/; then
            echo "Files copied successfully via SCP"
            # Fix permissions for grafana dashboards if needed
            ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=no \
                -o ConnectTimeout=10 \
                monitoring-vm "sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards 2>/dev/null || true"
          else
            echo "SCP failed, trying alternative method via SSH..."
            cd monitoring
            for file in $(find . -type f); do
              target_dir="${{ env.MONITORING_DIR }}/$(dirname "$file")"
              # Use sudo for grafana/dashboards directory
              if echo "$file" | grep -q "grafana/dashboards"; then
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo mkdir -p $target_dir && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo tee ${{ env.MONITORING_DIR }}/$file > /dev/null && sudo chown ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              else
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "mkdir -p $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "cat > ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              fi
            done
            echo "Files copied successfully via SSH"
          fi
      
      - name: Verify monitoring files copied
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "${{ env.MONITORING_DIR }}/docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found after copy attempt"
              exit 1
            else
              echo "Monitoring files successfully copied"
              ls -lh ${{ env.MONITORING_DIR }}/
            fi

      - name: Deploy Monitoring Stack
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 300s
          command_timeout: 120s
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail
            cd ${{ env.MONITORING_DIR }}
            
            echo "Verifying configuration files..."
            if [ ! -f "docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found!"
              exit 1
            fi
            if [ ! -f "prometheus/prometheus.yml" ]; then
              echo "ERROR: prometheus/prometheus.yml not found!"
              exit 1
            fi
            echo "Configuration files found"
            
            echo "Stopping existing services..."
            docker compose -f docker-compose.monitoring.yml down --remove-orphans 2>/dev/null || true
            
            # Nettoyer complètement : arrêter tous les conteneurs monitoring
            echo "Cleaning up monitoring containers..."
            for container in prometheus grafana alertmanager node-exporter cadvisor; do
              docker ps -a --filter "name=$container" --format "{{.ID}}" | while read id; do
                [ -n "$id" ] && docker rm -f "$id" 2>/dev/null || true
              done
            done
            
            # Supprimer le réseau existant (forcer si nécessaire)
            echo "Removing monitoring-network..."
            # Essayer plusieurs fois avec différentes méthodes
            for i in 1 2 3; do
              if docker network ls | grep -q monitoring-network; then
                echo "  Attempt $i: Removing network..."
                docker network rm monitoring-network 2>/dev/null && break || {
                  if [ $i -eq 1 ]; then
                    echo "  WARNING: Network still in use, cleaning unused networks..."
                    docker network prune -f
                    sleep 2
                  elif [ $i -eq 2 ]; then
                    echo "  Retrying after cleanup..."
                    sleep 2
                  else
                    echo "  WARNING: Cannot remove network, continuing anyway..."
                  fi
                }
              else
                echo "  Network removed"
                break
              fi
            done
            sleep 2
            
            echo "Checking disk space..."
            df -h / | tail -1
            
            echo "Aggressive Docker disk space cleanup..."
            
            # Arrêter tous les conteneurs en cours d'exécution (sauf ceux critiques)
            echo "Stopping all running containers..."
            docker ps -q | xargs -r docker stop 2>/dev/null || true
            
            # Supprimer tous les conteneurs arrêtés
            echo "Removing all stopped containers..."
            docker container prune -af || true
            
            # Supprimer toutes les images non utilisées (sans filtre de temps pour libérer plus d'espace)
            echo "Removing unused images..."
            docker image prune -af || true
            
            # Nettoyer le build cache
            echo "Cleaning build cache..."
            docker builder prune -af || true
            
            # Nettoyer les logs Docker (peuvent prendre beaucoup d'espace)
            echo "Cleaning Docker logs..."
            find /var/lib/docker/containers/ -type f -name "*.log" -delete 2>/dev/null || true
            journalctl --vacuum-time=1d 2>/dev/null || true
            
            # Nettoyer les volumes non utilisés
            echo "Removing unused volumes..."
            docker volume prune -af || true
            
            # Nettoyer les réseaux non utilisés
            echo "Removing unused networks..."
            docker network prune -f || true
            
            # Nettoyage système complet (sans filtre de temps)
            echo "Full system prune..."
            docker system prune -af --volumes || true
            
            # Vérifier l'espace utilisé par Docker
            echo "Docker disk usage:"
            docker system df || true
            
            echo "Disk space after cleanup:"
            df -h / | tail -1
            
            # Si le disque est toujours plein, essayer de nettoyer les snapshots containerd
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ "$DISK_USAGE" -gt 90 ]; then
              echo "WARNING: Disk still above 90%, cleaning containerd snapshots..."
              # Nettoyer les snapshots containerd (nécessite d'arrêter Docker, mais on essaie quand même)
              systemctl stop docker 2>/dev/null || true
              rm -rf /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/* 2>/dev/null || true
              systemctl start docker 2>/dev/null || true
              sleep 5
              echo "Final disk space:"
              df -h / | tail -1
            fi
            
            # Vérifier l'espace disque avant de pull les images
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ "$DISK_USAGE" -gt 95 ]; then
              echo "ERROR: Disk usage is still above 95% ($DISK_USAGE%), cannot pull images safely"
              echo "Current disk usage:"
              df -h /
              echo "Docker disk usage:"
              docker system df || true
              exit 1
            fi
            
            echo "Pulling Docker images (disk usage: ${DISK_USAGE}%)..."
            docker compose -f docker-compose.monitoring.yml pull
            
            echo "Starting monitoring stack..."
            # Démarrer avec gestion d'erreur pour le réseau
            set +e  # Ne pas arrêter sur les erreurs
            docker compose -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
            COMPOSE_EXIT_CODE=$?
            set -e  # Réactiver l'arrêt sur erreur
            
            if [ $COMPOSE_EXIT_CODE -ne 0 ]; then
              echo "WARNING: Error detected, checking network..."
              # Si l'erreur est liée au réseau, supprimer le réseau et réessayer
              if docker network ls | grep -q monitoring-network; then
                echo "Force removing problematic network..."
                # Arrêter tous les conteneurs qui utilisent ce réseau
                CONTAINER_IDS=$(docker ps --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$CONTAINER_IDS" ]; then
                  echo "$CONTAINER_IDS" | xargs docker stop 2>/dev/null || true
                  echo "$CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                OLD_CONTAINER_IDS=$(docker ps -a --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$OLD_CONTAINER_IDS" ]; then
                  echo "$OLD_CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                # Supprimer le réseau
                docker network rm monitoring-network 2>/dev/null || docker network prune -f
                sleep 3
                echo "Retrying startup..."
                docker compose -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
              else
                echo "ERROR: Non-network related error, see logs"
                docker compose -f docker-compose.monitoring.yml logs
                exit 1
              fi
            fi
            
            echo "Waiting for services to start (30s)..."
            sleep 30
            
            echo "Container status:"
            docker compose -f docker-compose.monitoring.yml ps
            
            echo "Checking services..."
            
            # Vérifier que les conteneurs sont en cours d'exécution
            echo "Verifying containers are running..."
            
            # Vérifier Prometheus
            if docker ps | grep -q prometheus; then
              PROMETHEUS_STATUS=$(docker ps --filter "name=prometheus" --format "{{.Status}}")
              echo "Prometheus is running: $PROMETHEUS_STATUS"
            else
              echo "ERROR: Prometheus container is not running"
              docker compose -f docker-compose.monitoring.yml logs prometheus | tail -30
              exit 1
            fi
            
            # Vérifier Grafana (peut prendre plus de temps pour le health check)
            GRAFANA_RUNNING=$(docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana && echo "yes" || echo "no")
            
            if [ "$GRAFANA_RUNNING" = "yes" ]; then
              GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
              echo "Grafana container found: $GRAFANA_STATUS"
              
              # Si Grafana est encore en "health: starting", attendre un peu plus
              if echo "$GRAFANA_STATUS" | grep -q "health: starting"; then
                echo "Grafana is still starting, waiting additional 30s..."
                sleep 30
                GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
                echo "Grafana status after wait: $GRAFANA_STATUS"
                
                # Vérifier à nouveau si Grafana est toujours en cours d'exécution
                if ! docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana; then
                  echo "ERROR: Grafana container stopped during startup"
                  docker compose -f docker-compose.monitoring.yml logs grafana | tail -50
                  exit 1
                fi
              fi
              
              echo "Grafana is running successfully"
            else
              echo "ERROR: Grafana container is not running"
              echo "Checking all containers:"
              docker ps -a | grep grafana || echo "No grafana container found"
              echo "Grafana logs:"
              docker compose -f docker-compose.monitoring.yml logs grafana | tail -50
              exit 1
            fi
            
            # Vérifier les autres services essentiels
            for service in alertmanager node-exporter cadvisor; do
              if docker ps | grep -q "$service"; then
                SERVICE_STATUS=$(docker ps --filter "name=$service" --format "{{.Status}}")
                echo "$service is running: $SERVICE_STATUS"
              else
                echo "WARNING: $service container is not running (non-critical)"
              fi
            done
            
            echo "Recent logs (last 10 lines):"
            docker compose -f docker-compose.monitoring.yml logs --tail=10
            
            echo "Monitoring stack deployed successfully!"
            echo "Prometheus should now scrape the backend on ${{ secrets.STAGING_HOST }}:8081"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/monitoring_key ~/.ssh/config ~/.ssh/known_hosts

  # ============================================
  # RELEASE (semantic versioning + changelog)
  # ============================================
  release:
    name: Release
    runs-on: ubuntu-latest
    needs: [lint, test, coverage, build]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    permissions:
      contents: write
      issues: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          # Cache désactivé car package-lock.json n'existe pas encore
          # cache: 'npm'
          # cache-dependency-path: package-lock.json

      - name: Install semantic-release dependencies
        run: npm install

      - name: Setup JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'

      - name: Run semantic-release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: npx semantic-release

