name: Backend CI/CD Pipeline

# ⭐ AMÉLIORATIONS APPLIQUÉES (structure métier optimisée):
# 
# GRAPHE DE DÉPENDANCES:
#   lint ─┐
#   test ─┼─→ coverage ─→ sonar
#         └─→ build ─→ docker-build ─→ deploy-staging ─→ deploy-monitoring
#   dependency-check ─┘
#   security-lint ────┘
#
# 1. ✅ Sonar dépend de coverage et réutilise jacoco.xml (pas de duplication)
# 2. ✅ Tests → Coverage → Sonar (chaîne claire)
# 3. ✅ Docker-build attend security checks (DevSecOps strict)
# 4. ✅ continue-on-error conditionnel: bloquant sur main, permissif sur develop
# 5. ✅ Nettoyage Docker sécurisé (préserve volumes Grafana/Prometheus)
# 6. ✅ Build optimisé (réutilise classes compilées du job test)
# 7. ✅ Staging uniquement sur develop, main pour release
# 8. ✅ OWASP @v3 et Trivy @0.28.0 (versions stables)
# 9. ✅ OWASP et security-lint bloquants sur main

on:
  push:
    branches: [ main, develop ]
    # Note: '**' signifie tous les fichiers du repo (backend est à la racine)
    paths:
      - '**'
      - '.github/workflows/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - '**'
  schedule:
    # Security scan tous les lundis à 2h du matin
    - cron: '0 2 * * 1'

permissions:
  actions: read
  contents: read
  security-events: write

env:
  JAVA_VERSION: '17'
  MAVEN_OPTS: '-Xmx2048m -XX:+UseG1GC'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/backend
  MONITORING_DIR: /opt/monitoring

jobs:
  # ============================================
  # LINT & CODE QUALITY
  # ============================================
  lint:
    name: Backend - Lint & Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # ⭐ AMÉLIORATION: Bloquer sur main, permissif sur develop
      - name: Run Maven Checkstyle
        run: mvn -B checkstyle:check
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
      
      - name: Run Maven SpotBugs
        run: mvn -B spotbugs:spotbugs -Duser.language=en -Duser.country=US
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
        env:
          JAVA_TOOL_OPTIONS: '-Duser.language=en -Duser.country=US'
      
      - name: Upload SpotBugs report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: spotbugs-report
          path: target/spotbugsXml.xml
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true
      
      - name: Upload Checkstyle report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkstyle-report
          path: target/checkstyle-result.xml
          retention-days: 7
        continue-on-error: true

  # ============================================
  # TESTS
  # ============================================
  test:
    name: Backend - Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # ⭐ AMÉLIORATION: Utiliser verify pour compiler + tester + générer coverage en une seule fois
      - name: Run tests and generate coverage report
        run: mvn -B clean verify jacoco:report -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Verify JaCoCo XML report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found after test execution!"
            exit 1
          else
            echo "✓ jacoco.xml generated: $(ls -lh target/site/jacoco/jacoco.xml)"
          fi
        continue-on-error: false
      
      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports
          path: target/surefire-reports
          retention-days: 7
      
      - name: Upload JaCoCo Report (for Sonar)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jacoco-report
          path: target/site/jacoco
          retention-days: 30

  # ============================================
  # COVERAGE (simplifié - jacoco.xml déjà généré dans test)
  # ============================================
  coverage:
    name: Backend - Code Coverage
    runs-on: ubuntu-latest
    needs: [test]
    timeout-minutes: 5
    
    steps:
      - name: Download JaCoCo Report
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
      
      - name: Check coverage threshold
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found!"
            exit 1
          fi
          echo "✓ jacoco.xml found: $(ls -lh target/site/jacoco/jacoco.xml)"
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./target/site/jacoco/jacoco.xml
          flags: backend
          name: backend-coverage
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}
        continue-on-error: true

  # ============================================
  # BUILD
  # ============================================
  build:
    name: Backend - Build
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # ⭐ AMÉLIORATION: Réutiliser les artefacts compilés du job test au lieu de refaire clean package
      # Le job test fait déjà 'mvn verify' qui compile et teste, donc on peut juste faire package
      - name: Build JAR (reusing compiled classes from test job)
        run: mvn -B package -DskipTests -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Check JAR size
        run: |
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            SIZE=$(du -sh "$JAR_FILE" | cut -f1)
            echo "JAR size: $SIZE"
            echo "Build successful: $JAR_FILE"
          else
            echo "Build failed - JAR file not found"
            exit 1
          fi
      
      - name: Prepare JAR artifact
        run: |
          mkdir -p artifacts
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            cp "$JAR_FILE" artifacts/
            echo "JAR artifact prepared: $(basename $JAR_FILE)"
          else
            echo "No JAR file found"
            exit 1
          fi
      
      - name: Upload JAR artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-jar
          path: artifacts/*.jar
          retention-days: 7

  # ============================================
  # SONARCloud ANALYSIS
  # ============================================
  sonar:
    name: Backend - SonarCloud Analysis
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # ⭐ AMÉLIORATION TOP 2: Bloquer sur main si Quality Gate fail, permissif sur develop
    continue-on-error: ${{ github.ref != 'refs/heads/main' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # ⭐ AMÉLIORATION TOP 1: Réutiliser jacoco.xml du job test/coverage au lieu de refaire les tests
      - name: Download JaCoCo Report from test job
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
      
      - name: Verify JaCoCo Report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found! Test job may have failed."
            echo "The test job should generate jacoco.xml via 'mvn verify jacoco:report'"
            exit 1
          else
            echo "✓ jacoco.xml found: $(ls -lh target/site/jacoco/jacoco.xml)"
            echo "✓ Coverage report will be used by SonarCloud"
          fi
      
      # Compiler uniquement les classes pour Sonar (nécessaire pour l'analyse statique)
      # Pas besoin de refaire les tests car jacoco.xml est déjà disponible
      - name: Compile classes for Sonar (no tests needed)
        run: mvn -B compile -DskipTests -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      # ⭐ AMÉLIORATION TOP 1: Sonar utilise jacoco.xml du job test (pas besoin de refaire les tests)
      - name: SonarCloud Scan (using coverage from test job)
        id: sonarcloud-scan
        uses: SonarSource/sonarcloud-github-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        # ⭐ AMÉLIORATION TOP 2: Bloquer sur main si Quality Gate fail
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}
      
      - name: Verify SonarCloud Analysis Sent
        if: always()
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "SonarCloud Analysis Verification"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Project Key: oumaymasaoudi_hotel-tickets-backend"
          echo "Organization: oumaymasaoudi"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo ""
          echo "Check SonarCloud dashboard:"
          echo "https://sonarcloud.io/project/overview?id=oumaymasaoudi_hotel-tickets-backend&branch=${{ github.ref_name }}"
          echo ""
          if [ "${{ job.status }}" == "success" ]; then
            echo "Status: Analysis sent successfully to SonarCloud"
            echo "The analysis should appear in SonarCloud within a few minutes"
          else
            echo "Status: Analysis may have failed - check logs above"
            echo "Exit code: ${{ steps.sonarcloud-scan.outcome }}"
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      

  # ============================================
  # SECURITY SCAN
  # ============================================
  dependency-check:
    name: Backend - OWASP Dependency Check
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # ⭐ AMÉLIORATION: Bloquer sur main si CVSS >= 7, permissif sur develop
    continue-on-error: ${{ github.ref != 'refs/heads/main' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'

      # ⭐ AMÉLIORATION: Utiliser une version valide de l'action OWASP
      # ⭐ AMÉLIORATION: Bloquer sur main si CVSS >= 7, permissif sur develop
      - name: Run OWASP Dependency Check
        uses: dependency-check/Dependency-Check_Action@v5
        with:
          project: 'hotel-ticket-hub-backend'
          path: '.'
          format: 'ALL'
          args: >
            --failOnCVSS 7
            --suppression owasp-dependency-check-suppressions.xml
            --enableRetired
            --enableExperimental
        continue-on-error: ${{ github.ref != 'refs/heads/main' }}

      - name: Upload Dependency Check results (HTML)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dependency-check-report
          path: reports/dependency-check-report.html
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true

      - name: Check if SARIF file exists
        id: check_sarif
        if: always()
        run: |
          if [ -f "reports/dependency-check-report.sarif" ]; then
            echo "sarif_exists=true" >> $GITHUB_OUTPUT
            echo "SARIF file found"
          else
            echo "sarif_exists=false" >> $GITHUB_OUTPUT
            echo "SARIF file not found (this is normal if no vulnerabilities were found)"
          fi
        continue-on-error: true

      - name: Upload Dependency Check results to GitHub Security (SARIF)
        uses: github/codeql-action/upload-sarif@v4
        if: always() && steps.check_sarif.outputs.sarif_exists == 'true'
        with:
          sarif_file: reports/dependency-check-report.sarif
        continue-on-error: true

  security-lint:
    name: Backend - Security Linting (Trivy)
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # ⭐ AMÉLIORATION: Bloquer sur main, permissif sur develop
    continue-on-error: ${{ github.ref != 'refs/heads/main' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ⭐ AMÉLIORATION: Pinner version Trivy au lieu de @master (plus stable)
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v4
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # ============================================
  # DOCKER BUILD & PUSH (for develop branch)
  # ============================================
  docker-build:
    name: Backend - Docker Build & Push
    runs-on: ubuntu-latest
    # ⭐ AMÉLIORATION BONUS: Docker build attend aussi les security checks (DevSecOps strict)
    needs: [build, dependency-check, security-lint]
    # ⭐ AMÉLIORATION: Docker build pour develop (staging) et main (release)
    if: (github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main') && github.event_name == 'push'
    concurrency:
      group: docker-build-backend
      cancel-in-progress: false
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ============================================
  # DEPLOY TO STAGING (for develop branch)
  # ============================================
  deploy-staging:
    name: Backend - Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker-build]
    # ⭐ AMÉLIORATION TOP 5: Staging uniquement sur develop (pas sur main)
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: staging
    timeout-minutes: 15
    # Permissif sur develop (staging peut échouer sans bloquer)
    continue-on-error: true
    concurrency:
      group: deploy-staging
      cancel-in-progress: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Optimisation : ne récupérer que le dernier commit

      - name: Verify docker-compose.yml exists
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found!"
            exit 1
          fi
          echo "docker-compose.yml found ($(du -h docker-compose.yml | cut -f1))"

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          # Ajouter l'host à known_hosts pour éviter les prompts
          ssh-keyscan -H ${{ secrets.STAGING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host backend-staging
            HostName ${{ secrets.STAGING_HOST }}
            User ${{ secrets.STAGING_USER }}
            IdentityFile ~/.ssh/deploy_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        continue-on-error: true
        id: ssh_test
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "SSH Connection Diagnostic"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo ""
          echo "Checking if secrets are configured..."
          if [ -z "${{ secrets.STAGING_HOST }}" ]; then
            echo "❌ ERROR: STAGING_HOST secret is not set"
            echo "   → Go to: Settings > Secrets and variables > Actions"
            echo "   → Add secret: STAGING_HOST = your-server-ip-or-domain"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          if [ -z "${{ secrets.STAGING_USER }}" ]; then
            echo "❌ ERROR: STAGING_USER secret is not set"
            echo "   → Go to: Settings > Secrets and variables > Actions"
            echo "   → Add secret: STAGING_USER = your-ssh-username (usually 'ubuntu' or 'ec2-user')"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          if [ -z "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" ]; then
            echo "❌ ERROR: STAGING_SSH_PRIVATE_KEY secret is not set"
            echo "   → Go to: Settings > Secrets and variables > Actions"
            echo "   → Add secret: STAGING_SSH_PRIVATE_KEY = your-private-key-content"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "✓ All required secrets are configured"
          echo ""
          echo "Connection details:"
          echo "  Host: ${{ secrets.STAGING_HOST }}"
          echo "  User: ${{ secrets.STAGING_USER }}"
          echo "  Key: [configured]"
          echo ""
          echo "Testing network connectivity..."
          if ping -c 3 -W 5 ${{ secrets.STAGING_HOST }} 2>&1; then
            echo "✓ Host is reachable via ping"
          else
            echo "⚠ WARNING: Host does not respond to ping"
            echo "  This might be normal if ICMP is blocked, but SSH should still work"
          fi
          echo ""
          echo "Testing SSH connection (timeout: 30s)..."
          SSH_OUTPUT=$(timeout 30 ssh -F ~/.ssh/config \
            -o ConnectTimeout=10 \
            -o StrictHostKeyChecking=no \
            -o BatchMode=yes \
            -o LogLevel=ERROR \
            -v backend-staging "echo 'SSH_SUCCESS'" 2>&1) || SSH_EXIT_CODE=$?
          
          if echo "$SSH_OUTPUT" | grep -q "SSH_SUCCESS"; then
            echo "✓ SSH connection successful!"
            echo "ssh_ok=true" >> $GITHUB_OUTPUT
          else
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "❌ SSH CONNECTION FAILED"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Exit code: ${SSH_EXIT_CODE:-unknown}"
            echo ""
            echo "SSH output:"
            echo "$SSH_OUTPUT" | tail -20
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "TROUBLESHOOTING STEPS:"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "1. VERIFY SECRETS IN GITHUB:"
            echo "   → Repository > Settings > Secrets and variables > Actions"
            echo "   → Ensure these secrets exist:"
            echo "     • STAGING_HOST (IP or domain)"
            echo "     • STAGING_USER (username)"
            echo "     • STAGING_SSH_PRIVATE_KEY (full private key)"
            echo ""
            echo "2. CHECK AWS SECURITY GROUP:"
            echo "   → AWS Console > EC2 > Security Groups"
            echo "   → Find the Security Group attached to your instance"
            echo "   → Inbound Rules > Add rule:"
            echo "     Type: SSH"
            echo "     Port: 22"
            echo "     Source: 0.0.0.0/0 (or GitHub Actions IP ranges)"
            echo ""
            echo "3. VERIFY INSTANCE STATUS:"
            echo "   → AWS Console > EC2 > Instances"
            echo "   → Ensure instance is 'running'"
            echo "   → Verify Public IP matches STAGING_HOST secret"
            echo ""
            echo "4. TEST SSH LOCALLY:"
            echo "   ssh -i your-key.pem ${{ secrets.STAGING_USER }}@${{ secrets.STAGING_HOST }}"
            echo ""
            echo "5. VERIFY SSH KEY FORMAT:"
            echo "   → STAGING_SSH_PRIVATE_KEY should start with:"
            echo "     '-----BEGIN OPENSSH PRIVATE KEY-----' or"
            echo "     '-----BEGIN RSA PRIVATE KEY-----'"
            echo "   → Include the BEGIN and END lines"
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            echo "::error::SSH connection failed. Check Security Group and secrets configuration."
            exit 1
          fi

      - name: Prepare staging directory
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p /opt/hotel-ticket-hub-backend-staging
            chmod 755 /opt/hotel-ticket-hub-backend-staging
      
      - name: Copy docker-compose to staging
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found in workspace"
            exit 1
          fi
          echo "Copying docker-compose.yml via SCP..."
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              docker-compose.yml \
              backend-staging:/opt/hotel-ticket-hub-backend-staging/docker-compose.yml; then
            echo "File copied successfully via SCP"
          else
            echo "SCP failed, trying alternative method via SSH..."
            if cat docker-compose.yml | ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=no \
                -o ConnectTimeout=10 \
                backend-staging "cat > /opt/hotel-ticket-hub-backend-staging/docker-compose.yml"; then
              echo "File copied successfully via SSH"
            else
              echo "ERROR: Both SCP and SSH copy methods failed"
              exit 1
            fi
          fi
      
      - name: Verify docker-compose file copied
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "/opt/hotel-ticket-hub-backend-staging/docker-compose.yml" ]; then
              echo "ERROR: docker-compose.yml not found after copy attempt"
              exit 1
            else
              echo "docker-compose.yml successfully copied"
              ls -lh /opt/hotel-ticket-hub-backend-staging/docker-compose.yml
            fi

      - name: Deploy to staging VM
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 600s  # 10 minutes pour la connexion SSH
          command_timeout: 300s  # 5 minutes pour le script (pull image + démarrage Spring Boot)
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail  # Mode strict : erreur si variable non définie
            cd /opt/hotel-ticket-hub-backend-staging
            
            echo "Stopping existing services..."
            # Stop old systemd service if it exists
            sudo systemctl stop hotel-ticket-hub-backend-staging 2>/dev/null || true
            sudo systemctl disable hotel-ticket-hub-backend-staging 2>/dev/null || true
            
            # Stop any container using port 8081
            CONTAINERS=$(docker ps -q --filter "publish=8081" 2>/dev/null || true)
            if [ -n "$CONTAINERS" ]; then
              docker stop $CONTAINERS 2>/dev/null || true
            fi
            
            # Stop any container using port 9100 (Node Exporter)
            CONTAINERS_9100=$(docker ps -q --filter "publish=9100" 2>/dev/null || true)
            if [ -n "$CONTAINERS_9100" ]; then
              docker stop $CONTAINERS_9100 2>/dev/null || true
              docker rm $CONTAINERS_9100 2>/dev/null || true
            fi
            
            echo "Checking .env file..."
            if [ ! -f .env ]; then
              echo "ERROR: .env file not found! Please create it first."
              exit 1
            fi
            
            echo "Connecting to GitHub Container Registry..."
            # Utiliser GHCR_TOKEN si défini (PAT), sinon GITHUB_TOKEN
            if [ -n "${{ secrets.GHCR_TOKEN }}" ]; then
              # PAT avec username explicite si défini, sinon github.actor
              GHCR_USER="${GHCR_USERNAME:-${{ github.actor }}}"
              echo "${{ secrets.GHCR_TOKEN }}" | docker login ghcr.io -u "$GHCR_USER" --password-stdin
            else
              # Fallback sur GITHUB_TOKEN (automatique)
              echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
            fi
            
            echo "Pulling Docker image..."
            export DOCKER_IMAGE=ghcr.io/${{ env.IMAGE_NAME }}:${{ github.ref_name }}
            docker pull $DOCKER_IMAGE || docker pull ghcr.io/${{ env.IMAGE_NAME }}:latest
            
            echo "Starting/Updating container (force recreate if needed)..."
            export DOCKER_IMAGE=$DOCKER_IMAGE
            # Utiliser --force-recreate pour recréer les conteneurs avec la nouvelle image
            # sans avoir besoin de faire un 'down' complet (plus rapide)
            docker compose --env-file .env up -d --force-recreate --remove-orphans
            
            echo "Waiting for container to start (30s)..."
            sleep 30
            
            echo "Checking container status..."
            CONTAINER_ID=$(docker ps -q --filter "name=hotel-ticket-hub-backend-staging" 2>/dev/null || true)
            if [ -z "$CONTAINER_ID" ]; then
              echo "ERROR: Container is not running"
              echo "Checking stopped containers..."
              docker ps -a | grep hotel-ticket-hub-backend-staging || true
              echo "Container logs (last 100 lines):"
              docker compose logs --tail=100 hotel-ticket-hub-backend-staging || docker logs hotel-ticket-hub-backend-staging --tail=100 2>&1 || true
              echo "Exit code:"
              docker inspect hotel-ticket-hub-backend-staging --format='{{.State.ExitCode}}' 2>/dev/null || echo "Container not found"
              exit 1
            fi
            
            echo "Container is running (ID: $CONTAINER_ID)"
            echo "Waiting for Spring Boot to fully start..."
            HEALTH_CHECK_URL="http://localhost:8081/actuator/health"
            MAX_RETRIES=12  # 12 tentatives × 10s = 2 minutes max
            RETRY_COUNT=0
            HEALTHY=false
            
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              sleep 10
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "Health check attempt $RETRY_COUNT/$MAX_RETRIES..."
              
              if curl -f -s "$HEALTH_CHECK_URL" > /dev/null 2>&1; then
                echo "✓ Application is healthy!"
                HEALTHY=true
                break
              else
                echo "  Application not ready yet, waiting..."
              fi
            done
            
            if [ "$HEALTHY" = false ]; then
              echo "WARNING: Health check failed after $MAX_RETRIES attempts, but container is running"
              echo "Recent logs:"
              docker compose logs --tail=50 hotel-ticket-hub-backend-staging || docker logs hotel-ticket-hub-backend-staging --tail=50 2>&1 || true
              echo "Container status:"
              docker ps --filter "name=hotel-ticket-hub-backend-staging" || true
            fi
            
            echo "Backend deployed successfully!"
            
            # Vérification finale de l'accessibilité depuis l'extérieur
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Final accessibility check"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Backend URL: http://${{ secrets.STAGING_HOST }}:8081"
            echo "Health endpoint: http://${{ secrets.STAGING_HOST }}:8081/actuator/health"
            echo "Prometheus endpoint: http://${{ secrets.STAGING_HOST }}:8081/actuator/prometheus"
            echo ""
            echo "⚠️  IMPORTANT: For Prometheus to scrape metrics, ensure:"
            echo "   1. Security Group allows port 8081 from Monitoring VM IP"
            echo "   2. Backend container stays running (check with: docker ps)"
            echo "   3. See scripts/fix-prometheus-connection-refused.md for troubleshooting"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/deploy_key ~/.ssh/config ~/.ssh/known_hosts
      
      - name: Deployment Summary
        if: always()
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          if [ "${{ job.status }}" == "success" ]; then
            echo "✅ DEPLOYMENT SUCCESSFUL"
            echo "Backend has been deployed to staging server"
          else
            echo "⚠️  DEPLOYMENT FAILED (but pipeline continues)"
            echo ""
            echo "Common causes:"
            echo "  • SSH secrets not configured in GitHub"
            echo "  • AWS Security Group blocking SSH (port 22)"
            echo "  • Server instance not running or IP changed"
            echo ""
            echo "Check the logs above for detailed diagnostics."
            echo "The pipeline will continue even if deployment fails."
          fi
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

  # ============================================
  # DEPLOY MONITORING (for develop branch)
  # Déployé après le backend pour s'assurer que le monitoring
  # peut scraper les métriques du backend déployé
  # ============================================
  deploy-monitoring:
    name: Backend - Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    # ⭐ AMÉLIORATION: Monitoring uniquement sur develop (staging)
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: monitoring
    timeout-minutes: 15
    continue-on-error: true
    concurrency:
      group: deploy-monitoring
      cancel-in-progress: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Verify monitoring files exist
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found!"
            exit 1
          fi
          echo "Monitoring directory found"
          echo "Files to deploy:"
          find monitoring -type f -name "*.yml" -o -name "*.yaml" -o -name "*.conf" | head -20

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.MONITORING_SSH_PRIVATE_KEY }}" > ~/.ssh/monitoring_key
          chmod 600 ~/.ssh/monitoring_key
          # Ajouter l'host à known_hosts
          ssh-keyscan -H ${{ secrets.MONITORING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host monitoring-vm
            HostName ${{ secrets.MONITORING_HOST }}
            User ${{ secrets.MONITORING_USER }}
            IdentityFile ~/.ssh/monitoring_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        run: |
          echo "Testing SSH connection to Monitoring VM..."
          timeout 10 ssh -F ~/.ssh/config -o ConnectTimeout=5 -o StrictHostKeyChecking=no monitoring-vm "echo 'SSH connection successful'" || {
            echo "ERROR: SSH connection failed"
            echo "Verify that:"
            echo "  - The Monitoring VM is accessible from GitHub Actions"
            echo "  - The AWS Security Group allows connections from GitHub"
            echo "  - The secrets MONITORING_HOST, MONITORING_USER, MONITORING_SSH_PRIVATE_KEY are correct"
            exit 1
          }
          echo "SSH connection OK"

      - name: Prepare monitoring directory
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p ${{ env.MONITORING_DIR }}
            chmod 755 ${{ env.MONITORING_DIR }}
            # Create grafana dashboards directory with correct permissions
            sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards
            sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards
            chmod 755 ${{ env.MONITORING_DIR }}/grafana/dashboards
      
      - name: Copy monitoring files to VM
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found in workspace"
            exit 1
          fi
          echo "Preparing directories on VM with correct permissions..."
          ssh -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              monitoring-vm "sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards" || true
          
          echo "Copying monitoring files via SCP..."
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              -r monitoring/* \
              monitoring-vm:${{ env.MONITORING_DIR }}/; then
            echo "Files copied successfully via SCP"
            # Fix permissions for grafana dashboards if needed
            ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=no \
                -o ConnectTimeout=10 \
                monitoring-vm "sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards 2>/dev/null || true"
          else
            echo "SCP failed, trying alternative method via SSH..."
            cd monitoring
            for file in $(find . -type f); do
              target_dir="${{ env.MONITORING_DIR }}/$(dirname "$file")"
              # Use sudo for grafana/dashboards directory
              if echo "$file" | grep -q "grafana/dashboards"; then
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo mkdir -p $target_dir && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo tee ${{ env.MONITORING_DIR }}/$file > /dev/null && sudo chown ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              else
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "mkdir -p $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=no \
                    -o ConnectTimeout=10 \
                    monitoring-vm "cat > ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              fi
            done
            echo "Files copied successfully via SSH"
          fi
      
      - name: Verify monitoring files copied
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "${{ env.MONITORING_DIR }}/docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found after copy attempt"
              exit 1
            else
              echo "Monitoring files successfully copied"
              ls -lh ${{ env.MONITORING_DIR }}/
            fi

      - name: Deploy Monitoring Stack
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 300s
          command_timeout: 120s
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail
            cd ${{ env.MONITORING_DIR }}
            
            echo "Verifying configuration files..."
            if [ ! -f "docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found!"
              exit 1
            fi
            if [ ! -f "prometheus/prometheus.yml" ]; then
              echo "ERROR: prometheus/prometheus.yml not found!"
              exit 1
            fi
            echo "Configuration files found"
            
            # OPTIMISATION: Utiliser --force-recreate au lieu de down complet
            # Cela évite de supprimer les volumes et réseaux inutilement
            echo "Updating monitoring services (force recreate if needed)..."
            # --force-recreate recrée les conteneurs avec les nouvelles images
            # --remove-orphans supprime les conteneurs orphelins
            # Pas besoin de 'down' complet sauf en cas de problème majeur
            
            echo "Checking disk space..."
            df -h / | tail -1
            
            echo "Safe Docker disk space cleanup (preserving volumes)..."
            
            # ⭐ AMÉLIORATION: Nettoyage sécurisé sans supprimer les volumes (Grafana/Prometheus)
            # Arrêter uniquement les conteneurs non critiques (pas ceux du monitoring)
            echo "Stopping non-critical containers..."
            docker ps --format "{{.Names}}" | grep -v -E "(grafana|prometheus|loki|alertmanager|node-exporter|cadvisor|promtail)" | xargs -r docker stop 2>/dev/null || true
            
            # Supprimer uniquement les conteneurs arrêtés non critiques
            echo "Removing stopped non-critical containers..."
            docker ps -a --format "{{.Names}}" | grep -v -E "(grafana|prometheus|loki|alertmanager|node-exporter|cadvisor|promtail)" | xargs -r docker rm 2>/dev/null || true
            
            # Supprimer les images non utilisées (sauf celles du monitoring)
            echo "Removing unused images (preserving monitoring images)..."
            docker image prune -af --filter "until=24h" || true
            
            # Nettoyer le build cache uniquement
            echo "Cleaning build cache..."
            docker builder prune -af --filter "until=24h" || true
            
            # Nettoyer les logs Docker (limité à 1 jour)
            echo "Cleaning Docker logs (older than 1 day)..."
            find /var/lib/docker/containers/ -type f -name "*.log" -mtime +1 -delete 2>/dev/null || true
            journalctl --vacuum-time=1d 2>/dev/null || true
            
            # ⭐ AMÉLIORATION: Ne JAMAIS supprimer les volumes (risque de perte de données Grafana/Prometheus)
            # docker volume prune est DANGEREUX - on ne le fait PAS
            
            # Nettoyer uniquement les réseaux non utilisés (pas ceux du monitoring)
            echo "Removing unused networks (preserving monitoring-network)..."
            docker network ls --format "{{.Name}}" | grep -v -E "(monitoring-network|bridge|host)" | xargs -r docker network rm 2>/dev/null || true
            
            # Nettoyage système sécurisé (SANS --volumes pour préserver les données)
            echo "Safe system prune (preserving volumes)..."
            docker system prune -af --filter "until=24h" || true
            
            # Vérifier l'espace utilisé par Docker
            echo "Docker disk usage:"
            docker system df || true
            
            echo "Disk space after cleanup:"
            df -h / | tail -1
            
            # Si le disque est toujours plein, essayer de nettoyer les snapshots containerd
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ "$DISK_USAGE" -gt 90 ]; then
              echo "WARNING: Disk still above 90%, cleaning containerd snapshots..."
              # Nettoyer les snapshots containerd (nécessite d'arrêter Docker, mais on essaie quand même)
              systemctl stop docker 2>/dev/null || true
              rm -rf /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/* 2>/dev/null || true
              systemctl start docker 2>/dev/null || true
              sleep 5
              echo "Final disk space:"
              df -h / | tail -1
            fi
            
            # Vérifier l'espace disque avant de pull les images
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ "$DISK_USAGE" -gt 95 ]; then
              echo "ERROR: Disk usage is still above 95% ($DISK_USAGE%), cannot pull images safely"
              echo "Current disk usage:"
              df -h /
              echo "Docker disk usage:"
              docker system df || true
              exit 1
            fi
            
            echo "Pulling Docker images (disk usage: ${DISK_USAGE}%)..."
            docker compose -f docker-compose.monitoring.yml pull
            
            echo "Starting monitoring stack..."
            # Démarrer avec gestion d'erreur pour le réseau
            set +e  # Ne pas arrêter sur les erreurs
            docker compose -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
            COMPOSE_EXIT_CODE=$?
            set -e  # Réactiver l'arrêt sur erreur
            
            if [ $COMPOSE_EXIT_CODE -ne 0 ]; then
              echo "WARNING: Error detected, checking network..."
              # Si l'erreur est liée au réseau, supprimer le réseau et réessayer
              if docker network ls | grep -q monitoring-network; then
                echo "Force removing problematic network..."
                # Arrêter tous les conteneurs qui utilisent ce réseau
                CONTAINER_IDS=$(docker ps --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$CONTAINER_IDS" ]; then
                  echo "$CONTAINER_IDS" | xargs docker stop 2>/dev/null || true
                  echo "$CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                OLD_CONTAINER_IDS=$(docker ps -a --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$OLD_CONTAINER_IDS" ]; then
                  echo "$OLD_CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                # Supprimer le réseau
                docker network rm monitoring-network 2>/dev/null || docker network prune -f
                sleep 3
                echo "Retrying startup..."
                docker compose -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
              else
                echo "ERROR: Non-network related error, see logs"
                docker compose -f docker-compose.monitoring.yml logs
                exit 1
              fi
            fi
            
            echo "Waiting for services to start (30s)..."
            sleep 30
            
            echo "Container status:"
            docker compose -f docker-compose.monitoring.yml ps
            
            echo "Checking services..."
            
            # Vérifier que les conteneurs sont en cours d'exécution
            echo "Verifying containers are running..."
            
            # Vérifier Prometheus
            if docker ps | grep -q prometheus; then
              PROMETHEUS_STATUS=$(docker ps --filter "name=prometheus" --format "{{.Status}}")
              echo "Prometheus is running: $PROMETHEUS_STATUS"
            else
              echo "ERROR: Prometheus container is not running"
              docker compose -f docker-compose.monitoring.yml logs prometheus | tail -30
              exit 1
            fi
            
            # Vérifier Grafana (peut prendre plus de temps pour le health check)
            GRAFANA_RUNNING=$(docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana && echo "yes" || echo "no")
            
            if [ "$GRAFANA_RUNNING" = "yes" ]; then
              GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
              echo "Grafana container found: $GRAFANA_STATUS"
              
              # Si Grafana est encore en "health: starting", attendre un peu plus
              if echo "$GRAFANA_STATUS" | grep -q "health: starting"; then
                echo "Grafana is still starting, waiting additional 30s..."
                sleep 30
                GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
                echo "Grafana status after wait: $GRAFANA_STATUS"
                
                # Vérifier à nouveau si Grafana est toujours en cours d'exécution
                if ! docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana; then
                  echo "ERROR: Grafana container stopped during startup"
                  docker compose -f docker-compose.monitoring.yml logs grafana | tail -50
                  exit 1
                fi
              fi
              
              echo "Grafana is running successfully"
            else
              echo "ERROR: Grafana container is not running"
              echo "Checking all containers:"
              docker ps -a | grep grafana || echo "No grafana container found"
              echo "Grafana logs:"
              docker compose -f docker-compose.monitoring.yml logs grafana | tail -50
              exit 1
            fi
            
            # Vérifier les autres services essentiels
            for service in alertmanager node-exporter cadvisor; do
              if docker ps | grep -q "$service"; then
                SERVICE_STATUS=$(docker ps --filter "name=$service" --format "{{.Status}}")
                echo "$service is running: $SERVICE_STATUS"
              else
                echo "WARNING: $service container is not running (non-critical)"
              fi
            done
            
            echo "Recent logs (last 10 lines):"
            docker compose -f docker-compose.monitoring.yml logs --tail=10
            
            echo "Monitoring stack deployed successfully!"
            echo "Prometheus should now scrape the backend on ${{ secrets.STAGING_HOST }}:8081"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/monitoring_key ~/.ssh/config ~/.ssh/known_hosts

  # ============================================
  # RELEASE (semantic versioning + changelog)
  # ============================================
  release:
    name: Release
    runs-on: ubuntu-latest
    needs: [lint, test, coverage, build]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    permissions:
      contents: write
      issues: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          # Cache désactivé car package-lock.json n'existe pas encore
          # cache: 'npm'
          # cache-dependency-path: package-lock.json

      - name: Install semantic-release dependencies
        run: |
          # Clean install to avoid corrupted node_modules
          echo "Cleaning previous installations..."
          rm -rf node_modules
          
          # Install dependencies with legacy peer deps to avoid conflicts
          echo "Installing dependencies..."
          npm install --legacy-peer-deps --no-audit --no-fund
          
          # Verify and reinstall critical dependencies if needed
          echo "Verifying critical dependencies..."
          if ! npm list semantic-release > /dev/null 2>&1; then
            echo "Reinstalling semantic-release..."
            npm install semantic-release --legacy-peer-deps --no-audit --no-fund
          fi
          
          if ! npm list @semantic-release/exec > /dev/null 2>&1; then
            echo "Installing @semantic-release/exec..."
            npm install @semantic-release/exec --legacy-peer-deps --no-audit --no-fund
          fi
          
          # Verify yargs is properly installed (it's a dependency of semantic-release)
          if ! npm list yargs > /dev/null 2>&1; then
            echo "Reinstalling yargs..."
            npm install yargs --legacy-peer-deps --no-audit --no-fund
          fi
          
          # Rebuild node_modules to fix any missing files
          echo "Rebuilding node_modules..."
          npm rebuild --legacy-peer-deps || true
          
          # Ensure binaries have execute permissions
          echo "Setting execute permissions..."
          chmod +x node_modules/.bin/* 2>/dev/null || true
          
          # Verify semantic-release binary exists and is executable
          if [ ! -f "node_modules/.bin/semantic-release" ]; then
            echo "ERROR: semantic-release binary not found"
            ls -la node_modules/.bin/ | head -20
            exit 1
          fi
          
          # Verify yargs module structure
          if [ ! -f "node_modules/yargs/build/lib/yargs-factory.js" ]; then
            echo "WARNING: yargs-factory.js not found, reinstalling yargs..."
            npm uninstall yargs
            npm install yargs --legacy-peer-deps --no-audit --no-fund
          fi
          
          echo "✓ Dependencies installed successfully"
          echo "Verifying installation..."
          npm list semantic-release yargs @semantic-release/exec

      - name: Setup JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'

      - name: Run semantic-release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: npx semantic-release

