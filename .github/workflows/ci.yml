name: Backend CI/CD Pipeline

# â­ AMÃ‰LIORATIONS APPLIQUÃ‰ES (structure mÃ©tier optimisÃ©e):
# 
# GRAPHE DE DÃ‰PENDANCES:
#   lint â”€â”
#   test â”€â”¼â”€â†’ coverage â”€â†’ sonar
#         â””â”€â†’ build â”€â†’ docker-build â”€â†’ deploy-staging â”€â†’ deploy-monitoring
#   dependency-check â”€â”˜
#   security-lint â”€â”€â”€â”€â”˜
#
# 1. âœ… Sonar dÃ©pend de coverage et rÃ©utilise jacoco.xml (pas de duplication)
# 2. âœ… Tests â†’ Coverage â†’ Sonar (chaÃ®ne claire)
# 3. âœ… Docker-build attend security checks (DevSecOps strict)
# 4. âœ… continue-on-error conditionnel: bloquant sur main, permissif sur develop
# 5. âœ… Nettoyage Docker sÃ©curisÃ© (prÃ©serve volumes Grafana/Prometheus)
# 6. âœ… Build accÃ©lÃ©rÃ© par cache Maven (chaque job = VM sÃ©parÃ©e, pas de rÃ©utilisation directe)
# 7. âœ… Staging uniquement sur develop, main pour release
# 8. âš ï¸ OWASP Dependency Check Action @main (RISQUE - Ã  pinner sur tag dÃ¨s disponible) et Trivy @0.28.0
# 9. âœ… OWASP et security-lint bloquants sur main

on:
  push:
    branches: [ main, develop ]
    # Optimisation : dÃ©clencher uniquement sur changements pertinents
    paths:
      - 'src/**'
      - 'pom.xml'
      - 'Dockerfile'
      - 'docker-compose*.yml'
      - '.github/workflows/**'
      - 'monitoring/**'
      - 'scripts/**'
      - 'checkstyle.xml'
      - 'spotbugs-exclude.xml'
      - 'owasp-dependency-check-suppressions.xml'
      - 'infrastructure/**'
      - '.env.example'
      - 'Makefile'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'pom.xml'
      - 'Dockerfile'
      - 'docker-compose*.yml'
      - '.github/workflows/**'
      - 'monitoring/**'
      - 'scripts/**'
      - 'checkstyle.xml'
      - 'spotbugs-exclude.xml'
      - 'owasp-dependency-check-suppressions.xml'
      - 'infrastructure/**'
      - '.env.example'
      - 'Makefile'
  schedule:
    # Security scan tous les lundis Ã  2h du matin
    - cron: '0 2 * * 1'

permissions:
  actions: read
  contents: read
  # â­ Security-events: write uniquement sur jobs qui upload SARIF (principe du moindre privilÃ¨ge)

# â­ AMÃ‰LIORATION CRITIQUE: Variables pour dÃ©terminer la branche (gÃ¨re PR correctement)
# En PR: github.ref = refs/pull/123/merge, donc on utilise github.base_ref (branche cible)
# En push: github.ref = refs/heads/main, donc on utilise github.ref_name
# Note: Pour les conditions if au niveau job, utiliser directement l'expression (env n'est pas disponible)
env:
  JAVA_VERSION: '17'
  MAVEN_OPTS: '-Xmx2048m -XX:+UseG1GC'
  REGISTRY: ghcr.io
  # â­ Format image: ghcr.io/<owner>/<repo>/backend:tag
  # Alternative possible: ghcr.io/<owner>/backend ou ghcr.io/<owner>/<repo>-backend
  # VÃ©rifier que ce format correspond Ã  votre configuration GHCR et docker-compose.yml
  IMAGE_NAME: ${{ github.repository }}/backend
  MONITORING_DIR: /opt/monitoring

jobs:
  # ============================================
  # LINT & CODE QUALITY
  # ============================================
  lint:
    name: Backend - Lint & Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # â­ AMÃ‰LIORATION: Bloquer sur main, permissif sur develop (gÃ¨re PR correctement)
      - name: Run Maven Checkstyle
        run: mvn -B checkstyle:check
        continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}
      
      - name: Run Maven SpotBugs
        run: mvn -B spotbugs:spotbugs -Duser.language=en -Duser.country=US
        continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}
        env:
          JAVA_TOOL_OPTIONS: '-Duser.language=en -Duser.country=US'
      
      - name: Upload SpotBugs report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: spotbugs-report
          path: target/spotbugsXml.xml
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true
      
      - name: Upload Checkstyle report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkstyle-report
          path: target/checkstyle-result.xml
          retention-days: 7
        continue-on-error: true

  # ============================================
  # TESTS
  # ============================================
  test:
    name: Backend - Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # â­ AMÃ‰LIORATION: Utiliser verify pour compiler + tester + gÃ©nÃ©rer coverage en une seule fois
      # Si jacoco-maven-plugin est configurÃ© avec prepare-agent dans le POM, verify gÃ©nÃ¨re dÃ©jÃ  le rapport
      # Sinon, jacoco:report est appelÃ© explicitement aprÃ¨s verify
      - name: Run tests and generate coverage report
        run: mvn -B clean verify -DskipITs jacoco:report
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Verify JaCoCo XML report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found after test execution!"
            exit 1
          else
            echo "âœ“ jacoco.xml generated: $(ls -lh target/site/jacoco/jacoco.xml)"
          fi
        continue-on-error: false
      
      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports
          path: target/surefire-reports
          retention-days: 7
      
      - name: Upload JaCoCo Report (for Sonar)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jacoco-report
          path: target/site/jacoco
          retention-days: 30

  # ============================================
  # COVERAGE (simplifiÃ© - jacoco.xml dÃ©jÃ  gÃ©nÃ©rÃ© dans test)
  # ============================================
  coverage:
    name: Backend - Code Coverage
    runs-on: ubuntu-latest
    needs: [test]
    timeout-minutes: 5
    
    steps:
      # Checkout pour Codecov (commit metadata) et stabilitÃ© gÃ©nÃ©rale
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download JaCoCo Report
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
      
      # â­ Compute branch context (rÃ©utilisable dans tous les scripts)
      - name: Compute branch context
        id: branch-context
        run: |
          # DÃ©terminer si on est sur main ou develop (gÃ¨re PR correctement)
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            IS_MAIN="${{ github.base_ref == 'main' }}"
            IS_DEVELOP="${{ github.base_ref == 'develop' }}"
            BRANCH_NAME="${{ github.base_ref }}"
          else
            IS_MAIN="${{ github.ref_name == 'main' }}"
            IS_DEVELOP="${{ github.ref_name == 'develop' }}"
            BRANCH_NAME="${{ github.ref_name }}"
          fi
          
          echo "IS_MAIN=${IS_MAIN}" >> $GITHUB_ENV
          echo "IS_DEVELOP=${IS_DEVELOP}" >> $GITHUB_ENV
          echo "BRANCH_NAME=${BRANCH_NAME}" >> $GITHUB_ENV
          
          echo "Branch context computed:"
          echo "  - IS_MAIN: ${IS_MAIN}"
          echo "  - IS_DEVELOP: ${IS_DEVELOP}"
          echo "  - BRANCH_NAME: ${BRANCH_NAME}"
      
      - name: Check coverage threshold
        run: |
          # â­ Initialiser COVERAGE_CHECK pour Ã©viter les erreurs dans Job Summary
          COVERAGE_CHECK=0
          
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found!"
            exit 1
          fi
          echo "âœ“ jacoco.xml found: $(ls -lh target/site/jacoco/jacoco.xml)"
          
          # VÃ©rifier le seuil de couverture rÃ©ellement
          # Parse jacoco.xml pour extraire le coverage total (utilise Python3 disponible sur ubuntu-latest)
          # Fallback sur LINE si INSTRUCTION absent (robustesse selon versions JaCoCo)
          # âš ï¸ Note: Si projet devient multi-module, il faudra merger les jacoco.xml ou pointer sonar vers plusieurs chemins
          COVERAGE_DATA=$(python3 -c "import xml.etree.ElementTree as ET; import sys; tree = ET.parse('target/site/jacoco/jacoco.xml'); root = tree.getroot(); counter = root.find('.//counter[@type=\"INSTRUCTION\"]'); counter_type = 'INSTRUCTION' if counter is not None else None; counter = counter if counter is not None else root.find('.//counter[@type=\"LINE\"]'); counter_type = counter_type if counter_type else ('LINE' if counter is not None else 'ERROR'); missed = int(counter.get('missed', 0)) if counter is not None else 0; covered = int(counter.get('covered', 0)) if counter is not None else 0; total = missed + covered; coverage_pct = (covered / total * 100) if total > 0 else 0.0; print(f'{coverage_pct:.2f}|{missed}|{covered}|{total}|{counter_type}')" 2>/dev/null || echo "0.00|0|0|0|ERROR")
          
          COVERAGE_THRESHOLD=${COVERAGE_THRESHOLD:-60.0}
          COVERAGE_NUM=$(echo "$COVERAGE_DATA" | cut -d'|' -f1)
          MISSED=$(echo "$COVERAGE_DATA" | cut -d'|' -f2)
          COVERED=$(echo "$COVERAGE_DATA" | cut -d'|' -f3)
          TOTAL=$(echo "$COVERAGE_DATA" | cut -d'|' -f4)
          COUNTER_TYPE=$(echo "$COVERAGE_DATA" | cut -d'|' -f5)
          
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Coverage Analysis"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Coverage Details:"
          echo "  - Counter type used: ${COUNTER_TYPE}"
          echo "  - Covered: ${COVERED}"
          echo "  - Missed: ${MISSED}"
          echo "  - Total: ${TOTAL}"
          echo "  - Coverage percentage: ${COVERAGE_NUM}%"
          echo "  - Threshold: ${COVERAGE_THRESHOLD}%"
          
          # Utiliser IS_MAIN depuis $GITHUB_ENV (calculÃ© dans step prÃ©cÃ©dent)
          
          # VÃ©rifier que le counter existe et que les donnÃ©es sont valides
          if [ "$TOTAL" = "0" ] || [ -z "$COVERAGE_NUM" ] || [ "$COUNTER_TYPE" = "ERROR" ]; then
            echo "âŒ ERROR: Could not parse coverage data from jacoco.xml"
            echo "   Counter types INSTRUCTION and LINE not found, or file format changed"
            echo "   This indicates a pipeline issue that needs investigation"
            echo ""
            echo "   Debug: Dumping first 10 <counter> elements from jacoco.xml for investigation:"
            python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('target/site/jacoco/jacoco.xml'); root = tree.getroot(); counters = root.findall('.//counter'); [print(f'  - {c.get(\"type\")}: missed={c.get(\"missed\")}, covered={c.get(\"covered\")}') for c in counters[:10]]" 2>/dev/null || echo "   (Could not dump counters - XML parsing failed)"
            COVERAGE_CHECK=0
            if [ "$IS_MAIN" == "true" ]; then
              echo "   âŒ Failing on main branch (DevSecOps strict: cannot proceed without coverage data)"
              exit 1
            else
              echo "   âš ï¸  Warning only (not on main branch)"
            fi
          else
            # Comparaison simple avec awk (pas besoin de bc)
            COVERAGE_CHECK=$(awk "BEGIN {print ($COVERAGE_NUM >= $COVERAGE_THRESHOLD)}")
            if [ "$COVERAGE_CHECK" = "1" ]; then
              echo "âœ“ Coverage ${COVERAGE_NUM}% meets threshold ${COVERAGE_THRESHOLD}%"
            else
              echo "âŒ Coverage ${COVERAGE_NUM}% is below threshold ${COVERAGE_THRESHOLD}%"
              if [ "$IS_MAIN" == "true" ]; then
                exit 1
              else
                echo "âš ï¸  Warning only (not on main branch)"
              fi
            fi
          fi
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          # â­ Job Summary: Afficher le coverage dans le rÃ©sumÃ© GitHub
          echo "## ğŸ“Š Code Coverage" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value | Threshold | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          if [ "$COVERAGE_CHECK" = "1" ]; then
            echo "| Coverage | **${COVERAGE_NUM}%** | ${COVERAGE_THRESHOLD}% | âœ… Pass |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Coverage | **${COVERAGE_NUM}%** | ${COVERAGE_THRESHOLD}% | âŒ Fail |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ“ˆ Coverage report: Download artifact \`jacoco-report\` to view the full JaCoCo HTML report" >> $GITHUB_STEP_SUMMARY
        env:
          COVERAGE_THRESHOLD: 60.0
        # continue-on-error gÃ©rÃ© par la logique interne du script (fail sur main, warn sur develop)
      
      # â­ Codecov : upload conditionnel (token requis pour repos privÃ©s uniquement)
      # Pour repos publics : Codecov fonctionne sans token (mais on peut quand mÃªme envoyer)
      # Pour repos privÃ©s : token requis (CODECOV_TOKEN secret)
      # Note: continue-on-error: true gÃ¨re les cas oÃ¹ CODECOV_TOKEN n'existe pas (repos publics)
      # Si le token n'existe pas, Codecov peut quand mÃªme fonctionner pour les repos publics
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./target/site/jacoco/jacoco.xml
          flags: backend
          name: backend-coverage
          fail_ci_if_error: false
          # â­ Token conditionnel : pour repos privÃ©s uniquement (public repos n'en ont pas besoin)
          # Si CODECOV_TOKEN n'existe pas, cette ligne sera vide et Codecov fonctionnera sans token (repos publics)
          token: ${{ secrets.CODECOV_TOKEN }}
        continue-on-error: true

  # ============================================
  # BUILD
  # ============================================
  build:
    name: Backend - Build
    runs-on: ubuntu-latest
    # â­ Optimisation : build dÃ©pend de lint + test seulement (coverage dÃ©pend dÃ©jÃ  de test)
    # docker-build dÃ©pend dÃ©jÃ  de sÃ©curitÃ© + sonar, donc le contrÃ´le DevSecOps est maintenu
    needs: [lint, test]
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # Note: Chaque job GitHub Actions s'exÃ©cute sur une VM sÃ©parÃ©e
      # Maven utilise le cache (~/.m2) pour accÃ©lÃ©rer la compilation
      # mais les classes compilÃ©es du job test ne sont pas rÃ©utilisÃ©es directement
      - name: Build JAR
        run: mvn -B package -DskipTests -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Check JAR size
        run: |
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            SIZE=$(du -sh "$JAR_FILE" | cut -f1)
            echo "JAR size: $SIZE"
            echo "Build successful: $JAR_FILE"
          else
            echo "Build failed - JAR file not found"
            exit 1
          fi
      
      - name: Prepare JAR artifact
        run: |
          mkdir -p artifacts
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            cp "$JAR_FILE" artifacts/
            echo "JAR artifact prepared: $(basename $JAR_FILE)"
          else
            echo "No JAR file found"
            exit 1
          fi
      
      - name: Upload JAR artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-jar
          path: artifacts/*.jar
          retention-days: 7

  # ============================================
  # SONARCloud ANALYSIS
  # ============================================
  sonar:
    name: Backend - SonarCloud Analysis
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # â­ AMÃ‰LIORATION TOP 2: Bloquer sur main si Quality Gate fail, permissif sur develop (gÃ¨re PR correctement)
    continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      # â­ AMÃ‰LIORATION TOP 1: RÃ©utiliser jacoco.xml du job test/coverage au lieu de refaire les tests
      - name: Download JaCoCo Report from test job
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
      
      - name: Verify JaCoCo Report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: jacoco.xml not found! Test job may have failed."
            echo "The test job should generate jacoco.xml via 'mvn verify jacoco:report'"
            exit 1
          else
            echo "âœ“ jacoco.xml found: $(ls -lh target/site/jacoco/jacoco.xml)"
            echo "âœ“ Coverage report will be used by SonarCloud"
          fi
      
      # Compiler les classes principales ET les classes de test pour Sonar
      # SonarCloud a besoin de target/classes ET target/test-classes
      # Pas besoin d'exÃ©cuter les tests car jacoco.xml est dÃ©jÃ  disponible
      # â­ Simplification: test-compile compile dÃ©jÃ  main + tests (via phases Maven)
      # Plus rapide et Ã©vite la double compilation
      - name: Compile classes and test classes for Sonar
        run: |
          # test-compile compile automatiquement les classes principales puis les classes de test
          mvn -B test-compile -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      # VÃ©rifier que tout est prÃªt pour SonarCloud (classes + coverage)
      - name: Verify SonarCloud prerequisites
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Verifying SonarCloud prerequisites..."
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          # VÃ©rifier les classes compilÃ©es
          if [ ! -d "target/classes" ]; then
            echo "âŒ ERROR: target/classes not found!"
            exit 1
          fi
          CLASS_COUNT=$(find target/classes -name "*.class" | wc -l)
          echo "âœ“ Compiled classes found: $CLASS_COUNT"
          
          # VÃ©rifier le rapport JaCoCo
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "âŒ ERROR: jacoco.xml not found!"
            echo "   Expected path: target/site/jacoco/jacoco.xml"
            exit 1
          fi
          echo "âœ“ JaCoCo XML report found: $(ls -lh target/site/jacoco/jacoco.xml)"
          
          # VÃ©rifier les test classes (requis par SonarCloud)
          if [ ! -d "target/test-classes" ]; then
            echo "âŒ ERROR: target/test-classes not found!"
            echo "   SonarCloud requires test classes to be compiled"
            exit 1
          fi
          TEST_CLASS_COUNT=$(find target/test-classes -name "*.class" | wc -l)
          echo "âœ“ Test classes found: $TEST_CLASS_COUNT"
          
          echo ""
          echo "âœ“ All prerequisites verified - SonarCloud can analyze with coverage"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # â­ AMÃ‰LIORATION TOP 1: Sonar utilise jacoco.xml du job test (pas besoin de refaire les tests)
      # âš ï¸ IMPORTANT: Si vous voyez l'erreur "Automatic Analysis is enabled", 
      # dÃ©sactivez l'analyse automatique dans SonarCloud :
      # 1. Allez sur https://sonarcloud.io
      # 2. Projet > Administration > Analysis Method
      # 3. DÃ©sactivez "Automatic Analysis"
      # Voir SONARCLOUD-FIX.md pour plus de dÃ©tails
      # Note: Le chemin JaCoCo est configurÃ© dans sonar-project.properties:
      # sonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml
      # â­ AMÃ‰LIORATION TOP 2: Utiliser Maven SonarScanner (recommandÃ© par SonarSource pour Maven)
      # Skip si PR depuis un fork (secrets.SONAR_TOKEN non disponible)
      - name: SonarCloud Analysis (Maven)
        id: sonarcloud-scan
        if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
        continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Starting SonarCloud Analysis..."
          echo "Organization: oumaymasaoudi"
          echo "Project Key: oumaymasaoudi_hotel-tickets-backend"
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "PR: #${{ github.event.pull_request.number }} (${{ github.event.pull_request.head.ref }} â†’ ${{ github.event.pull_request.base.ref }})"
          else
            echo "Branch: ${{ github.ref_name }}"
          fi
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          # VÃ©rifier que le token est disponible
          if [ -z "$SONAR_TOKEN" ]; then
            echo "âŒ ERROR: SONAR_TOKEN is not set!"
            exit 1
          fi
          
          # VÃ©rifier que jacoco.xml existe
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "âŒ ERROR: jacoco.xml not found at target/site/jacoco/jacoco.xml"
            exit 1
          fi
          
          # â­ Utiliser les paramÃ¨tres PR pour les Pull Requests (plus robuste que branch.name)
          # En PR : SonarCloud attache l'analyse Ã  la PR
          # En push : SonarCloud analyse la branche
          # âš ï¸ Note: sonar.coverage.jacoco.xmlReportPaths est dÃ©fini dans sonar-project.properties
          # Si un jour tu vois "0% coverage", vÃ©rifier que le chemin est correct et qu'il n'y a pas de conflit
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            mvn -B sonar:sonar \
              -DskipTests \
              -Dsonar.organization=oumaymasaoudi \
              -Dsonar.projectKey=oumaymasaoudi_hotel-tickets-backend \
              -Dsonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml \
              -Dsonar.host.url=https://sonarcloud.io \
              -Dsonar.pullrequest.key=${{ github.event.pull_request.number }} \
              -Dsonar.pullrequest.branch=${{ github.event.pull_request.head.ref }} \
              -Dsonar.pullrequest.base=${{ github.event.pull_request.base.ref }} \
              -Dsonar.qualitygate.wait=true \
              -Dsonar.qualitygate.timeout=600
          else
            mvn -B sonar:sonar \
              -DskipTests \
              -Dsonar.organization=oumaymasaoudi \
              -Dsonar.projectKey=oumaymasaoudi_hotel-tickets-backend \
              -Dsonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml \
              -Dsonar.host.url=https://sonarcloud.io \
              -Dsonar.branch.name=${{ github.ref_name }} \
              -Dsonar.qualitygate.wait=true \
              -Dsonar.qualitygate.timeout=600
          fi
          
          SONAR_EXIT_CODE=$?
          if [ $SONAR_EXIT_CODE -ne 0 ]; then
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "âš ï¸ SonarCloud analysis completed with exit code: $SONAR_EXIT_CODE"
            echo "This may indicate:"
            echo "  - Quality Gate failed (check https://sonarcloud.io)"
            echo "  - Analysis error (check logs above)"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            exit $SONAR_EXIT_CODE
          fi
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      # Note: Le Quality Gate SonarCloud est vÃ©rifiÃ© automatiquement par l'action sonarcloud-github-action
      # Le continue-on-error au niveau job permet de bloquer sur main et d'Ãªtre permissif sur develop
      # Si le Quality Gate Ã©choue, le job Ã©chouera automatiquement (sauf si continue-on-error est true)
      
      - name: Verify SonarCloud Analysis Sent
        if: always()
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "SonarCloud Analysis Verification"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Project Key: oumaymasaoudi_hotel-tickets-backend"
          echo "Organization: oumaymasaoudi"
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "PR: #${{ github.event.pull_request.number }} (${{ github.event.pull_request.head.ref }} â†’ ${{ github.event.pull_request.base.ref }})"
          else
            echo "Branch: ${{ github.ref_name }}"
          fi
          echo "Commit: ${{ github.sha }}"
          echo ""
          SONAR_URL="https://sonarcloud.io/project/overview?id=oumaymasaoudi_hotel-tickets-backend"
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            SONAR_URL="${SONAR_URL}&pullRequest=${{ github.event.pull_request.number }}"
          else
            SONAR_URL="${SONAR_URL}&branch=${{ github.ref_name }}"
          fi
          
          echo "Check SonarCloud dashboard:"
          echo "$SONAR_URL"
          echo ""
          if [ "${{ job.status }}" == "success" ]; then
            echo "Status: Analysis sent successfully to SonarCloud"
            echo "The analysis should appear in SonarCloud within a few minutes"
          else
            echo "Status: Analysis may have failed - check logs above"
            echo "Outcome: ${{ steps.sonarcloud-scan.outcome || 'skipped' }}"
          fi
          
          # â­ Job Summary: Afficher le lien SonarCloud dans le rÃ©sumÃ© GitHub
          echo "## ğŸ” SonarCloud Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Item | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Status | ${{ steps.sonarcloud-scan.outcome || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "| PR | #${{ github.event.pull_request.number }} (${{ github.event.pull_request.head.ref }} â†’ ${{ github.event.pull_request.base.ref }}) |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Branch | ${{ github.ref_name }} |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "| Dashboard | [View on SonarCloud]($SONAR_URL) |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # â­ Job Summary pour PR fork : informer que Sonar est skip
      - name: SonarCloud Skipped (Fork PR)
        if: github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name != github.repository
        run: |
          echo "## ğŸ” SonarCloud Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âš ï¸ **Skipped**: SonarCloud analysis is not available for fork pull requests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Reason: \`SONAR_TOKEN\` secret is not available for security reasons." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "To enable SonarCloud analysis, merge this PR into the main repository." >> $GITHUB_STEP_SUMMARY

  # ============================================
  # SECURITY SCAN
  # ============================================
  dependency-check:
    name: Backend - OWASP Dependency Check
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # â­ AMÃ‰LIORATION: Bloquer sur main si CVSS >= 7, permissif sur develop (gÃ¨re PR correctement)
    continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}
    permissions:
      contents: read
      security-events: write  # â­ NÃ©cessaire pour upload SARIF
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # â­ AMÃ‰LIORATION: Version pinÃ©e pour stabilitÃ© (main - derniÃ¨re version stable)
      # â­ AMÃ‰LIORATION: Bloquer sur main si CVSS >= 7, permissif sur develop
      # Note: L'action OWASP Dependency Check utilise Docker et n'a pas besoin de Java installÃ© sur le runner
      # Le path '.' scanne le repo entier, mais dependency-check dÃ©tecte automatiquement pom.xml
      # et scanne uniquement les dÃ©pendances Maven (les exclusions Ã©vitent de scanner node_modules)
      - name: Run OWASP Dependency Check
        # âš ï¸ RISQUE CRITIQUE: Utilisation de @main (temporaire) - RISQUE DE CASSURE DU PIPELINE
        # Une modification sur main peut casser la CI du jour au lendemain (breaking changes sans prÃ©venir)
        # â­ ACTION REQUISE: Pinner dÃ¨s que possible sur un tag release (@vX.Y.Z) ou un commit SHA stable
        # Pour vÃ©rifier les versions taguÃ©es: https://github.com/dependency-check/Dependency-Check_Action/releases
        # Pour trouver un commit SHA stable: https://github.com/dependency-check/Dependency-Check_Action/commits/main
        # Dependabot proposera automatiquement une PR avec la version taguÃ©e quand disponible
        # Exemple de version taguÃ©e: @v3.3.0 (Ã  utiliser dÃ¨s que disponible pour stabilitÃ© en production)
        # Format pour commit SHA: uses: dependency-check/Dependency-Check_Action@<commit-sha>
        uses: dependency-check/Dependency-Check_Action@main
        with:
          project: 'hotel-ticket-hub-backend'
          path: '.'
          format: 'ALL'
          # Exclure node_modules et fichiers non-Java pour scanner uniquement les dÃ©pendances Maven
          # dependency-check dÃ©tecte automatiquement pom.xml et scanne les dÃ©pendances Maven uniquement
          args: '--failOnCVSS=7 --enableRetired --enableExperimental --exclude "**/node_modules/**" --exclude "**/package*.json" --exclude "**/.git/**" --exclude "**/target/**"'
        continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}

      # Debug: Lister les fichiers gÃ©nÃ©rÃ©s par dependency-check (robustesse)
      - name: Debug Dependency-Check outputs
        if: always()
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Dependency-Check Output Files Debug"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Current directory: $(pwd)"
          echo ""
          echo "Listing reports directory:"
          ls -lah reports/ 2>/dev/null || echo "reports/ directory not found"
          echo ""
          echo "Searching for dependency-check files:"
          find . -maxdepth 4 -type f \( -iname "*dependency-check*" -o -iname "trivy-results.sarif" -o -iname "*.sarif" \) 2>/dev/null | head -20 || echo "No dependency-check files found"
          echo ""
          echo "Checking for SARIF file:"
          if [ -f "reports/dependency-check-report.sarif" ]; then
            echo "âœ“ SARIF file found: reports/dependency-check-report.sarif"
            ls -lh reports/dependency-check-report.sarif
          else
            echo "âš ï¸  SARIF file not found at reports/dependency-check-report.sarif"
          fi
          echo ""
          echo "Checking for HTML report:"
          if [ -f "reports/dependency-check-report.html" ]; then
            echo "âœ“ HTML report found: reports/dependency-check-report.html"
            ls -lh reports/dependency-check-report.html
          else
            echo "âš ï¸  HTML report not found at reports/dependency-check-report.html"
          fi
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

      - name: Upload Dependency Check results (HTML)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dependency-check-report
          path: reports/dependency-check-report.html
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true

      - name: Check if SARIF file exists
        id: check_sarif
        if: always()
        run: |
          # Chercher dynamiquement le fichier SARIF (le chemin peut varier selon la version de l'action OWASP)
          # Chercher d'abord dans reports/ (emplacement standard), puis globalement
          SARIF=$(find reports -type f -name "*.sarif" 2>/dev/null | head -n 1 || true)
          if [ -z "$SARIF" ]; then
            # Fallback : chercher globalement tous les SARIF
            SARIF=$(find . -type f -name "*.sarif" 2>/dev/null | head -n 1 || true)
          fi
          if [ -n "$SARIF" ]; then
            echo "sarif_file=$SARIF" >> $GITHUB_OUTPUT
            echo "sarif_exists=true" >> $GITHUB_OUTPUT
            echo "âœ“ SARIF file found: $SARIF"
          else
            echo "sarif_exists=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  SARIF file not found (peut arriver si l'action n'a pas gÃ©nÃ©rÃ© le SARIF selon config/version)"
          fi
        continue-on-error: true

      - name: Upload Dependency Check results to GitHub Security (SARIF)
        uses: github/codeql-action/upload-sarif@v4
        if: always() && steps.check_sarif.outputs.sarif_exists == 'true'
        with:
          sarif_file: ${{ steps.check_sarif.outputs.sarif_file }}
        continue-on-error: true
      
      # â­ Job Summary pour OWASP Dependency Check
      - name: OWASP Dependency Check Summary
        if: always()
        run: |
          echo "## ğŸ” OWASP Dependency Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          SCAN_OUTCOME="${{ job.status }}"
          if [ "$SCAN_OUTCOME" == "success" ]; then
            echo "| Status | âœ… **Scan OK** |" >> $GITHUB_STEP_SUMMARY
            echo "| Result | No CVSS >= 7 vulnerabilities found |" >> $GITHUB_STEP_SUMMARY
          elif [ "$SCAN_OUTCOME" == "failure" ]; then
            echo "| Status | âŒ **Scan Failed** |" >> $GITHUB_STEP_SUMMARY
            echo "| Result | CVSS >= 7 vulnerabilities found |" >> $GITHUB_STEP_SUMMARY
            if [ "${{ github.ref_name }}" == "main" ]; then
              echo "| Impact | ğŸš« **Blocking** (main branch - release blocked) |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| Impact | âš ï¸ **Warning** (develop branch - non-blocking) |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "| Status | â­ï¸ **Skipped** |" >> $GITHUB_STEP_SUMMARY
            echo "| Result | Scan was skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.check_sarif.outputs.sarif_exists }}" == "true" ]; then
            echo "ğŸ“Š View detailed results in the [Security tab](https://github.com/${{ github.repository }}/security/code-scanning)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

  security-lint:
    name: Backend - Security Linting (Trivy)
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' || github.event_name == 'pull_request' || github.event_name == 'schedule'
    # â­ AMÃ‰LIORATION: Bloquer sur main, permissif sur develop (gÃ¨re PR correctement)
    continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}
    permissions:
      contents: read
      security-events: write  # â­ NÃ©cessaire pour upload SARIF
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Installer Trivy pour l'Ã©tape de diagnostic
      - name: Setup Trivy for diagnostics
        uses: aquasecurity/setup-trivy@v0.2.1
        continue-on-error: true

      # ExÃ©cuter Trivy en mode table pour afficher les vulnÃ©rabilitÃ©s (diagnostic)
      - name: Run Trivy vulnerability scanner (table format for diagnostics)
        id: trivy-table
        continue-on-error: true
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Trivy Vulnerability Scan (Table Format - Diagnostic)"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          # Utiliser exit-code 0 pour voir les rÃ©sultats mÃªme s'il y a des vulnÃ©rabilitÃ©s
          # Note: --skip-dirs pour Ã©viter de scanner node_modules et autres dossiers non pertinents
          # --skip-files pour Ã©viter les fichiers de configuration non critiques
          trivy fs . \
            --severity CRITICAL,HIGH \
            --format table \
            --scanners vuln \
            --exit-code 0 \
            --skip-dirs node_modules,target,.git \
            --skip-files "**/*.md,**/*.txt" || echo "âš ï¸  Trivy table scan completed with warnings"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

      # â­ Cache Trivy DB pour amÃ©liorer la fiabilitÃ© (Ã©vite rate-limits et tÃ©lÃ©chargements rÃ©pÃ©tÃ©s)
      - name: Cache Trivy DB
        uses: actions/cache@v4
        with:
          path: ~/.cache/trivy
          key: trivy-db-${{ runner.os }}
          restore-keys: |
            trivy-db-${{ runner.os }}

      # â­ AMÃ‰LIORATION: Pinner version Trivy au lieu de @master (plus stable)
      # â­ AMÃ‰LIORATION DevSecOps: exit-code conditionnel - bloquant sur main, permissif sur develop
      - name: Run Trivy vulnerability scanner
        id: trivy-scan
        uses: aquasecurity/trivy-action@0.28.0
        # â­ Bloquer sur main, permissif sur develop et branches feature (gÃ¨re PR correctement)
        # Note: continue-on-error permet au job de continuer mÃªme si Trivy trouve des vulnÃ©rabilitÃ©s
        # Sur les branches feature/develop, c'est normal que Trivy retourne exit code 1 si des vulnÃ©rabilitÃ©s sont trouvÃ©es
        continue-on-error: ${{ (github.event_name == 'pull_request' && github.base_ref != 'main') || (github.event_name == 'push' && github.ref_name != 'main') }}
        env:
          TRIVY_CACHE_DIR: ~/.cache/trivy
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          # â­ AMÃ‰LIORATION: Bloquer sur main, permissif sur develop et branches feature (gÃ¨re PR correctement)
          # Sur main: strict (exit-code=1) - toutes les vulnÃ©rabilitÃ©s CRITICAL/HIGH bloquent
          # Sur develop/feature: permissif (exit-code=0) - warning seulement
          # Note: MÃªme avec exit-code='0', Trivy peut retourner exit code 1 si des vulnÃ©rabilitÃ©s sont trouvÃ©es
          # C'est pourquoi continue-on-error est nÃ©cessaire pour permettre au job de continuer
          exit-code: ${{ (github.event_name == 'pull_request' && github.base_ref == 'main' || github.event_name == 'push' && github.ref_name == 'main') && '1' || '0' }}
          # Sur main: strict (ignore-unfixed=false) - toutes les vulnÃ©rabilitÃ©s CRITICAL/HIGH bloquent
          # Sur develop/feature: tolÃ©rant (ignore-unfixed=true) - ignore les vulnÃ©rabilitÃ©s sans correctif (rÃ©duit le bruit)
          ignore-unfixed: ${{ github.event_name == 'pull_request' && github.base_ref != 'main' || github.event_name == 'push' && github.ref_name != 'main' }}
          # Scanner uniquement les vulnÃ©rabilitÃ©s (pas les secrets pour Ã©viter les faux positifs)
          scanners: 'vuln'
          # â­ Optimisation: Exclure les dossiers non pertinents pour amÃ©liorer la performance et rÃ©duire les faux positifs
          # Note: Trivy scanne automatiquement pom.xml et package.json, donc on peut exclure node_modules et target
          skip-dirs: 'node_modules,target,.git'
      
      # Afficher un rÃ©sumÃ© des rÃ©sultats Trivy pour diagnostic
      - name: Trivy Scan Summary
        if: always()
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Trivy Scan Results Summary"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          # â­ Job Summary pour Trivy
          echo "## ğŸ”’ Trivy Security Scan" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          SCAN_OUTCOME="${{ steps.trivy-scan.outcome }}"
          if [ "$SCAN_OUTCOME" == "success" ]; then
            echo "| Status | âœ… **Scan OK** |" >> $GITHUB_STEP_SUMMARY
            echo "| Result | No CRITICAL/HIGH vulnerabilities found |" >> $GITHUB_STEP_SUMMARY
          elif [ "$SCAN_OUTCOME" == "failure" ]; then
            echo "| Status | âŒ **Scan Failed** |" >> $GITHUB_STEP_SUMMARY
            echo "| Result | CRITICAL or HIGH vulnerabilities found |" >> $GITHUB_STEP_SUMMARY
            if [ "${{ github.ref_name }}" == "main" ]; then
              echo "| Impact | ğŸš« **Blocking** (main branch - release blocked) |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| Impact | âš ï¸ **Warning** (develop branch - non-blocking) |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "| Status | â­ï¸ **Skipped** |" >> $GITHUB_STEP_SUMMARY
            echo "| Result | Scan was skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ“Š View detailed results in the [Security tab](https://github.com/${{ github.repository }}/security/code-scanning)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$SCAN_OUTCOME" == "failure" ]; then
            echo "âŒ Trivy scan found CRITICAL or HIGH vulnerabilities"
            echo "Branch: ${{ github.ref_name }}"
            if [ "${{ github.ref_name }}" == "main" ]; then
              echo "âš ï¸  Pipeline bloquant sur main : les vulnÃ©rabilitÃ©s CRITICAL/HIGH bloquent la release"
            else
              echo "âš ï¸  Pipeline permissif sur develop : warning seulement (non-bloquant)"
            fi
            echo ""
            echo "âš ï¸  Note: Trivy may fail due to:"
            echo "   1. CRITICAL/HIGH vulnerabilities in dependencies"
            echo "   2. Maven dependency version detection issues (see warning above)"
            echo "   3. Secret scanning findings"
            echo ""
            echo "Check the Trivy logs above for details"
            echo "SARIF report uploaded to GitHub Security tab (if generated)"
            echo ""
            echo "To investigate:"
            echo "  - Check GitHub Security tab for detailed vulnerability report"
            echo "  - Review pom.xml dependencies for outdated versions"
            echo "  - Consider updating dependencies or adding suppressions"
            echo "  - Review the table format scan output above for quick overview"
          else
            echo "âœ“ Trivy scan completed successfully"
            echo "No CRITICAL or HIGH vulnerabilities found"
          fi
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

      # VÃ©rifier que le fichier SARIF existe avant upload (robustesse)
      - name: Check if Trivy SARIF exists
        id: trivy_sarif
        if: always()
        run: |
          if [ -f "trivy-results.sarif" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "âœ“ Trivy SARIF file found"
            ls -lh trivy-results.sarif
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  Trivy SARIF file not found (may occur if Trivy scan failed or timed out)"
          fi

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v4
        if: always() && steps.trivy_sarif.outputs.exists == 'true'
        with:
          sarif_file: 'trivy-results.sarif'

  # ============================================
  # DOCKER BUILD & PUSH (for develop branch)
  # ============================================
  docker-build:
    name: Backend - Docker Build & Push
    runs-on: ubuntu-latest
    # â­ AMÃ‰LIORATION BONUS: Docker build attend security checks + SonarCloud Quality Gate (DevSecOps strict)
    # Sur main : pas d'image si Sonar Quality Gate KO
    # âš ï¸ IMPORTANT: Utiliser needs.sonar.result pour permettre l'exÃ©cution mÃªme si sonar Ã©choue sur develop
    needs: [build, dependency-check, security-lint, sonar]
    # â­ AMÃ‰LIORATION: Docker build pour develop (staging) et main (release)
    # Sur develop: continue mÃªme si sonar Ã©choue (staging peut Ãªtre dÃ©ployÃ© pour tester)
    # Sur main: bloque si sonar Ã©choue (Quality Gate strict)
    if: |
      github.event_name == 'push' &&
      (github.ref_name == 'develop' || github.ref_name == 'main') &&
      (github.ref_name != 'main' || needs.sonar.result == 'success')
    concurrency:
      group: docker-build-backend-${{ github.ref_name }}
      cancel-in-progress: false
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          # â­ Utiliser secrets dÃ©diÃ©s si disponibles (plus fiable que github.actor)
          # Si GHCR_USERNAME secret existe, l'utiliser (doit correspondre au owner du PAT)
          # Sinon, fallback sur github.actor
          username: ${{ secrets.GHCR_USERNAME || github.actor }}
          password: ${{ secrets.GHCR_TOKEN || secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            # â­ CRITIQUE: format=long pour garantir SHA complet (40 chars) = alignÃ© avec deploy-staging
            # Sans format=long, docker/metadata-action peut gÃ©nÃ©rer des tags SHA courts (ex: develop-1a2b3c4)
            # ce qui causerait "image not found" dans deploy-staging qui cherche develop-<full-sha>
            type=sha,prefix={{branch}}-,format=long
            type=raw,value=run-${{ github.run_number }}
            # latest sera poussÃ© uniquement dans docker-tag-release aprÃ¨s succÃ¨s de release

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
      
      # â­ Job Summary global : rÃ©sumÃ© de tous les checks critiques
      - name: Pipeline Summary
        if: always()
        run: |
          echo "# ğŸ“Š Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          
          # Build (dÃ©pend de lint + test)
          if [ "${{ needs.build.result }}" == "success" ]; then
            echo "| âœ… Build | Pass |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.build.result }}" == "failure" ]; then
            echo "| âŒ Build | Fail |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| â­ï¸ Build | Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # SonarCloud
          if [ "${{ needs.sonar.result }}" == "success" ]; then
            echo "| âœ… SonarCloud Analysis | Pass (Quality Gate OK) |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.sonar.result }}" == "failure" ]; then
            echo "| âŒ SonarCloud Analysis | Fail (Quality Gate KO) |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.sonar.result }}" == "skipped" ]; then
            echo "| â­ï¸ SonarCloud Analysis | Skipped (fork PR, secrets absents, etc.) |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ **Note**: SonarCloud was skipped. On main branch, docker-build will be blocked (DevSecOps strict)." >> $GITHUB_STEP_SUMMARY
          else
            echo "| â­ï¸ SonarCloud Analysis | Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # OWASP Dependency Check
          if [ "${{ needs.dependency-check.result }}" == "success" ]; then
            echo "| âœ… OWASP Dependency Check | Pass |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.dependency-check.result }}" == "failure" ]; then
            echo "| âŒ OWASP Dependency Check | Fail |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| â­ï¸ OWASP Dependency Check | Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Trivy Security Scan
          if [ "${{ needs.security-lint.result }}" == "success" ]; then
            echo "| âœ… Trivy Security Scan | Pass |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.security-lint.result }}" == "failure" ]; then
            echo "| âŒ Trivy Security Scan | Fail |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| â­ï¸ Trivy Security Scan | Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Docker Build
          if [ "${{ job.status }}" == "success" ]; then
            echo "| âœ… Docker Build & Push | Pass |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Image tags pushed:**" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            echo "${{ steps.meta.outputs.tags }}" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ job.status }}" == "failure" ]; then
            echo "| âŒ Docker Build & Push | Fail |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| â­ï¸ Docker Build & Push | Skipped |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

  # ============================================
  # DEPLOY TO STAGING (for develop branch)
  # ============================================
  deploy-staging:
    name: Backend - Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker-build]
    # â­ Meilleure pratique : Staging uniquement sur develop (environnement de test)
    # Main est pour la production (ou validation finale sur staging si nÃ©cessaire)
    if: github.event_name == 'push' && github.ref_name == 'develop'
    environment: staging
    timeout-minutes: 15
    # Permissif sur develop (staging peut Ã©chouer sans bloquer)
    continue-on-error: true
    concurrency:
      group: deploy-staging-${{ github.ref_name }}
      cancel-in-progress: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Optimisation : ne rÃ©cupÃ©rer que le dernier commit

      - name: Verify docker-compose.yml exists
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found!"
            exit 1
          fi
          echo "docker-compose.yml found ($(du -h docker-compose.yml | cut -f1))"

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          # â­ SÃ©curitÃ© SSH : utiliser KNOWN_HOSTS depuis secret si disponible (protection MITM)
          # Sinon, fallback sur ssh-keyscan (acceptable pour staging, mais idÃ©alement pinner l'empreinte)
          if [ -n "${{ secrets.STAGING_KNOWN_HOSTS }}" ]; then
            echo "${{ secrets.STAGING_KNOWN_HOSTS }}" > ~/.ssh/known_hosts
            chmod 644 ~/.ssh/known_hosts
            STRICT_CHECK="yes"
            echo "âœ“ Using pinned SSH host fingerprint from secret (StrictHostKeyChecking=yes)"
          else
            # Fallback : ssh-keyscan (acceptable pour staging, mais moins sÃ©curisÃ©)
            ssh-keyscan -H ${{ secrets.STAGING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
            STRICT_CHECK="no"
            echo "âš ï¸  Using ssh-keyscan (NON-SECURE mode - consider pinning host fingerprint in STAGING_KNOWN_HOSTS secret for production)"
          fi
          # Configuration SSH optimisÃ©e
          cat >> ~/.ssh/config << EOF
          Host backend-staging
            HostName ${{ secrets.STAGING_HOST }}
            User ${{ secrets.STAGING_USER }}
            IdentityFile ~/.ssh/deploy_key
            StrictHostKeyChecking $STRICT_CHECK
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        continue-on-error: true
        id: ssh_test
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "SSH Connection Diagnostic"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "Checking if secrets are configured..."
          if [ -z "${{ secrets.STAGING_HOST }}" ]; then
            echo "âŒ ERROR: STAGING_HOST secret is not set"
            echo "   â†’ Go to: Settings > Secrets and variables > Actions"
            echo "   â†’ Add secret: STAGING_HOST = your-server-ip-or-domain"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          if [ -z "${{ secrets.STAGING_USER }}" ]; then
            echo "âŒ ERROR: STAGING_USER secret is not set"
            echo "   â†’ Go to: Settings > Secrets and variables > Actions"
            echo "   â†’ Add secret: STAGING_USER = your-ssh-username (usually 'ubuntu' or 'ec2-user')"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          if [ -z "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" ]; then
            echo "âŒ ERROR: STAGING_SSH_PRIVATE_KEY secret is not set"
            echo "   â†’ Go to: Settings > Secrets and variables > Actions"
            echo "   â†’ Add secret: STAGING_SSH_PRIVATE_KEY = your-private-key-content"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "âœ“ All required secrets are configured"
          echo ""
          echo "Connection details:"
          echo "  Host: ${{ secrets.STAGING_HOST }}"
          echo "  User: ${{ secrets.STAGING_USER }}"
          echo "  Key: [configured]"
          echo ""
          echo "Testing network connectivity..."
          if ping -c 3 -W 5 ${{ secrets.STAGING_HOST }} 2>&1; then
            echo "âœ“ Host is reachable via ping"
          else
            echo "âš  WARNING: Host does not respond to ping"
            echo "  This might be normal if ICMP is blocked, but SSH should still work"
          fi
          echo ""
          echo "Testing SSH connection (timeout: 30s)..."
          # Utiliser StrictHostKeyChecking selon la prÃ©sence de KNOWN_HOSTS
          STRICT_CHECK="$([ -n "${{ secrets.STAGING_KNOWN_HOSTS }}" ] && echo "yes" || echo "no")"
          SSH_OUTPUT=$(timeout 30 ssh -F ~/.ssh/config \
            -o ConnectTimeout=10 \
            -o StrictHostKeyChecking=$STRICT_CHECK \
            -o BatchMode=yes \
            -o LogLevel=ERROR \
            -v backend-staging "echo 'SSH_SUCCESS'" 2>&1) || SSH_EXIT_CODE=$?
          
          if echo "$SSH_OUTPUT" | grep -q "SSH_SUCCESS"; then
            echo "âœ“ SSH connection successful!"
            echo "ssh_ok=true" >> $GITHUB_OUTPUT
          else
            echo ""
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "âŒ SSH CONNECTION FAILED"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo ""
            echo "Exit code: ${SSH_EXIT_CODE:-unknown}"
            echo ""
            echo "SSH output:"
            echo "$SSH_OUTPUT" | tail -20
            echo ""
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "TROUBLESHOOTING STEPS:"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo ""
            echo "1. VERIFY SECRETS IN GITHUB:"
            echo "   â†’ Repository > Settings > Secrets and variables > Actions"
            echo "   â†’ Ensure these secrets exist:"
            echo "     â€¢ STAGING_HOST (IP or domain)"
            echo "     â€¢ STAGING_USER (username)"
            echo "     â€¢ STAGING_SSH_PRIVATE_KEY (full private key)"
            echo ""
            echo "2. CHECK AWS SECURITY GROUP:"
            echo "   â†’ AWS Console > EC2 > Security Groups"
            echo "   â†’ Find the Security Group attached to your instance"
            echo "   â†’ Inbound Rules > Add rule:"
            echo "     Type: SSH"
            echo "     Port: 22"
            echo "     Source: 0.0.0.0/0 (or GitHub Actions IP ranges)"
            echo ""
            echo "3. VERIFY INSTANCE STATUS:"
            echo "   â†’ AWS Console > EC2 > Instances"
            echo "   â†’ Ensure instance is 'running'"
            echo "   â†’ Verify Public IP matches STAGING_HOST secret"
            echo ""
            echo "4. TEST SSH LOCALLY:"
            echo "   ssh -i your-key.pem ${{ secrets.STAGING_USER }}@${{ secrets.STAGING_HOST }}"
            echo ""
            echo "5. VERIFY SSH KEY FORMAT:"
            echo "   â†’ STAGING_SSH_PRIVATE_KEY should start with:"
            echo "     '-----BEGIN OPENSSH PRIVATE KEY-----' or"
            echo "     '-----BEGIN RSA PRIVATE KEY-----'"
            echo "   â†’ Include the BEGIN and END lines"
            echo ""
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "ssh_ok=false" >> $GITHUB_OUTPUT
            echo "::error::SSH connection failed. Check Security Group and secrets configuration."
            exit 1
          fi

      - name: Prepare staging directory
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p /opt/hotel-ticket-hub-backend-staging
            chmod 755 /opt/hotel-ticket-hub-backend-staging
      
      - name: Copy docker-compose to staging
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found in workspace"
            exit 1
          fi
          echo "Copying docker-compose.yml via SCP..."
          STRICT_CHECK="$([ -n "${{ secrets.STAGING_KNOWN_HOSTS }}" ] && echo "yes" || echo "no")"
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=$STRICT_CHECK \
              -o ConnectTimeout=10 \
              docker-compose.yml \
              backend-staging:/opt/hotel-ticket-hub-backend-staging/docker-compose.yml; then
            echo "File copied successfully via SCP"
          else
            echo "SCP failed, trying alternative method via SSH..."
            if cat docker-compose.yml | ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=$STRICT_CHECK \
                -o ConnectTimeout=10 \
                backend-staging "cat > /opt/hotel-ticket-hub-backend-staging/docker-compose.yml"; then
              echo "File copied successfully via SSH"
            else
              echo "ERROR: Both SCP and SSH copy methods failed"
              exit 1
            fi
          fi
      
      - name: Verify docker-compose file copied
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "/opt/hotel-ticket-hub-backend-staging/docker-compose.yml" ]; then
              echo "ERROR: docker-compose.yml not found after copy attempt"
              exit 1
            else
              echo "docker-compose.yml successfully copied"
              ls -lh /opt/hotel-ticket-hub-backend-staging/docker-compose.yml
            fi

      - name: Deploy to staging VM
        if: steps.ssh_test.outputs.ssh_ok == 'true'
        uses: appleboy/ssh-action@v1.0.3
        env:
          # â­ Utiliser secrets dÃ©diÃ©s si disponibles (plus fiable que github.actor)
          # GHCR_USERNAME doit correspondre au owner du PAT (GHCR_TOKEN)
          GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
          GHCR_USERNAME: ${{ secrets.GHCR_USERNAME || github.actor }}
          GITHUB_ACTOR: ${{ github.actor }}
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 600s  # 10 minutes pour la connexion SSH
          command_timeout: 300s  # 5 minutes pour le script (pull image + dÃ©marrage Spring Boot)
          debug: false
          use_insecure_cipher: false
          envs: GHCR_TOKEN,GHCR_USERNAME,GITHUB_ACTOR
          script: |
            set -euo pipefail  # Mode strict : erreur si variable non dÃ©finie
            cd /opt/hotel-ticket-hub-backend-staging
            
            # â­ Compute branch context (pour scripts bash)
            if [ "${{ github.event_name }}" == "pull_request" ]; then
              IS_MAIN="${{ github.base_ref == 'main' }}"
              IS_DEVELOP="${{ github.base_ref == 'develop' }}"
            else
              IS_MAIN="${{ github.ref_name == 'main' }}"
              IS_DEVELOP="${{ github.ref_name == 'develop' }}"
            fi
            export IS_MAIN IS_DEVELOP
            
            echo "Stopping existing services..."
            # Stop old systemd service if it exists
            sudo systemctl stop hotel-ticket-hub-backend-staging 2>/dev/null || true
            sudo systemctl disable hotel-ticket-hub-backend-staging 2>/dev/null || true
            
            # Stop container by name (plus fiable que filter publish=)
            echo "Stopping backend container by name..."
            docker stop hotel-ticket-hub-backend-staging 2>/dev/null || true
            docker rm hotel-ticket-hub-backend-staging 2>/dev/null || true
            
            # Fallback: Stop any container using port 8081 (si le nom a changÃ©)
            CONTAINERS=$(docker ps -q --filter "publish=8081" 2>/dev/null || true)
            if [ -n "$CONTAINERS" ]; then
              echo "Found containers on port 8081, stopping them..."
              docker stop $CONTAINERS 2>/dev/null || true
              docker rm $CONTAINERS 2>/dev/null || true
            fi
            
            # Stop any container using port 9100 (Node Exporter) - fallback par port
            CONTAINERS_9100=$(docker ps -q --filter "publish=9100" 2>/dev/null || true)
            if [ -n "$CONTAINERS_9100" ]; then
              echo "Found containers on port 9100, stopping them..."
              docker stop $CONTAINERS_9100 2>/dev/null || true
              docker rm $CONTAINERS_9100 2>/dev/null || true
            fi
            
            echo "Checking .env file..."
            if [ ! -f .env ]; then
              echo "ERROR: .env file not found! Please create it first."
              exit 1
            fi
            
            echo "Connecting to GitHub Container Registry..."
            # Utiliser uniquement GHCR_TOKEN (PAT avec scope read:packages) passÃ© via env
            # Le secret est passÃ© via env: du step, pas interpolÃ© dans le script (meilleure sÃ©curitÃ©)
            # Note: GHCR_TOKEN doit avoir le scope 'read:packages' pour pull depuis GHCR sur une VM externe
            if [ -z "$GHCR_TOKEN" ]; then
              echo "ERROR: GHCR_TOKEN environment variable is required for pulling images from GHCR"
              echo "Create a PAT with 'read:packages' scope and add it as GHCR_TOKEN secret"
              exit 1
            fi
            # Utiliser GHCR_USERNAME (secret dÃ©diÃ© ou github.actor) pour correspondre au PAT
            GHCR_USER="${GHCR_USERNAME:-$GITHUB_ACTOR}"
            echo "$GHCR_TOKEN" | docker login ghcr.io -u "$GHCR_USER" --password-stdin
            
            echo "Pulling Docker image..."
            # Utiliser l'image par SHA pour garantir l'exactitude du commit dÃ©ployÃ© (zÃ©ro ambiguÃ¯tÃ©)
            # Note: docker-compose.yml doit utiliser ${DOCKER_IMAGE} pour que cette variable soit prise en compte
            # Exemple dans docker-compose.yml: services.backend.image: ${DOCKER_IMAGE}
            export DOCKER_IMAGE=ghcr.io/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-${{ github.sha }}
            docker pull $DOCKER_IMAGE || {
              echo "âš ï¸  Image by SHA not found, trying branch tag..."
              export DOCKER_IMAGE=ghcr.io/${{ env.IMAGE_NAME }}:${{ github.ref_name }}
              docker pull $DOCKER_IMAGE || docker pull ghcr.io/${{ env.IMAGE_NAME }}:latest
            }
            
            echo "Verifying Docker Compose version..."
            docker compose version || {
              echo "ERROR: docker compose (v2) not found. Checking docker-compose (v1)..."
              docker-compose version || {
                echo "ERROR: Neither docker compose nor docker-compose found"
                exit 1
              }
              COMPOSE_CMD="docker-compose"
            }
            COMPOSE_CMD="${COMPOSE_CMD:-docker compose}"
            
            echo "Starting/Updating container (force recreate if needed)..."
            # VÃ©rifier que docker-compose.yml utilise bien ${DOCKER_IMAGE} (sinon le tag SHA sera ignorÃ©)
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "Verifying docker-compose.yml configuration..."
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "Using DOCKER_IMAGE=$DOCKER_IMAGE"
            echo ""
            echo "Checking docker-compose.yml for image configuration:"
            grep -n "image:" docker-compose.yml || echo "âš ï¸  No 'image:' found in docker-compose.yml"
            # â­ Utiliser IS_MAIN depuis $GITHUB_ENV (calculÃ© au dÃ©but du script)
            if ! grep -q '\${DOCKER_IMAGE}' docker-compose.yml && ! grep -q '\$DOCKER_IMAGE' docker-compose.yml; then
              if [ "$IS_MAIN" == "true" ]; then
                echo "âŒ ERROR: main branch requires docker-compose.yml to use \${DOCKER_IMAGE} variable"
                echo "   This ensures deployment uses the exact commit SHA image (zero ambiguity)."
                echo "   Fix: Add 'services.backend.image: \${DOCKER_IMAGE}' to docker-compose.yml"
                exit 1
              else
                echo "âš ï¸  WARNING: docker-compose.yml may not use \${DOCKER_IMAGE} variable"
                echo "   To use the exact commit SHA image, ensure docker-compose.yml has:"
                echo "   services.backend.image: \${DOCKER_IMAGE}"
                echo "   Otherwise, Docker Compose will use a hardcoded image tag."
              fi
            else
              echo "âœ“ docker-compose.yml references DOCKER_IMAGE variable"
            fi
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            
            # Docker Compose charge automatiquement le fichier .env dans le mÃªme rÃ©pertoire
            # Pas besoin de source .env (Ã©vite les problÃ¨mes avec valeurs complexes/commentaires)
            # On exporte uniquement DOCKER_IMAGE qui doit Ãªtre disponible pour docker-compose.yml
            # Les autres variables du .env seront chargÃ©es automatiquement par Docker Compose
            
            # Utiliser un project name fixe pour Ã©viter les surprises de noms de containers
            # Cela rend les logs et ps plus prÃ©visibles
            COMPOSE_PROJECT_NAME="hotel-ticket-hub-backend-staging"
            
            # Utiliser --force-recreate pour recrÃ©er les conteneurs avec la nouvelle image
            # sans avoir besoin de faire un 'down' complet (plus rapide)
            $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" up -d --force-recreate --remove-orphans
            
            echo "Waiting for container to start (30s)..."
            sleep 30
            
            echo "Checking container status..."
            # RÃ©cupÃ©rer le nom/ID rÃ©el du container via docker compose ps (plus fiable que supposer un nom fixe)
            # Docker Compose peut nommer les containers comme <project>_<service>_1, donc on utilise compose ps
            echo "Listing containers managed by Docker Compose:"
            $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" ps
            
            # â­ Robustesse : dÃ©tecter le service ou utiliser un fallback explicite
            # Option 1 : dÃ©finir explicitement SERVICE_NAME=backend (plus fiable)
            # Option 2 : parser docker-compose.yml (fallback si SERVICE_NAME non dÃ©fini)
            SERVICE_NAME="${SERVICE_NAME:-}"
            if [ -z "$SERVICE_NAME" ]; then
              # Parser docker-compose.yml pour trouver le premier service
              SERVICE_NAME=$(
                awk '
                  $1=="services:" {in_services=1; next}
                  in_services && $1 ~ /^[a-zA-Z0-9_-]+:$/ {gsub(":","",$1); print $1; exit}
                  in_services && $1 ~ /^(networks|volumes):$/ {exit}
                ' docker-compose.yml
              )
            fi
            # Fallback final explicite (Ã©vite les erreurs si parsing Ã©choue)
            SERVICE_NAME="${SERVICE_NAME:-backend}"
            echo "Service name: $SERVICE_NAME (detected from docker-compose.yml or fallback)"
            
            # RÃ©cupÃ©rer l'ID du container via compose ps (avec project name pour cohÃ©rence)
            CONTAINER_ID=$($COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" ps -q "$SERVICE_NAME" 2>/dev/null | head -n 1 || true)
            
            if [ -z "$CONTAINER_ID" ]; then
              echo "ERROR: Container not found via docker compose ps"
              echo "Checking all containers..."
              docker ps -a
              echo "Container logs (last 100 lines):"
              $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" logs --tail=100 2>&1 || true
              exit 1
            fi
            
            # VÃ©rifier l'Ã©tat du container via docker inspect
            RUNNING=$(docker inspect -f '{{.State.Running}}' "$CONTAINER_ID" 2>/dev/null || echo "false")
            CONTAINER_NAME=$(docker inspect -f '{{.Name}}' "$CONTAINER_ID" 2>/dev/null | sed 's|/||' || echo "unknown")
            
            if [ "$RUNNING" != "true" ]; then
              echo "ERROR: Container is not running"
              echo "Container ID: $CONTAINER_ID"
              echo "Container name: $CONTAINER_NAME"
              echo "Running state: $RUNNING"
              echo "Checking stopped containers..."
              docker ps -a | grep "$CONTAINER_NAME" || docker ps -a | grep "$CONTAINER_ID" || true
              echo "Container logs (last 100 lines):"
              $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" logs --tail=100 "$SERVICE_NAME" 2>&1 || docker logs "$CONTAINER_ID" --tail=100 2>&1 || true
              echo "Exit code:"
              docker inspect "$CONTAINER_ID" --format='{{.State.ExitCode}}' 2>/dev/null || echo "Container not found"
              exit 1
            fi
            
            echo "âœ“ Container is running"
            echo "  Container ID: $CONTAINER_ID"
            echo "  Container name: $CONTAINER_NAME"
            echo "Waiting for Spring Boot to fully start..."
            MAX_RETRIES=12  # 12 tentatives Ã— 10s = 2 minutes max
            RETRY_COUNT=0
            HEALTHY=false
            
            # âš ï¸ Note: Healthcheck depuis l'hÃ´te nÃ©cessite que le port soit publiÃ© (ex: 8081:8080)
            # Si le port n'est pas publiÃ© ou si le mapping change, le check "host" tombera
            # et on dÃ©pendra du fallback docker exec (dÃ©jÃ  implÃ©mentÃ©, donc pas bloquant)
            # Si un jour tu changes le mapping de port, mettre Ã  jour cette valeur ou le check tombera
            echo "Note: Healthcheck requires published port 8081 (or will check inside container)"
            
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              sleep 10
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "Health check attempt $RETRY_COUNT/$MAX_RETRIES..."
              
              # VÃ©rifier d'abord le HEALTHCHECK Docker si dÃ©fini dans l'image
              HEALTH_STATUS=$(docker inspect "$CONTAINER_ID" --format='{{.State.Health.Status}}' 2>/dev/null || echo "none")
              if [ "$HEALTH_STATUS" = "healthy" ]; then
                echo "âœ“ Application is healthy (Docker HEALTHCHECK)!"
                HEALTHY=true
                break
              elif [ "$HEALTH_STATUS" = "starting" ]; then
                echo "  Docker HEALTHCHECK is starting (waiting for health probe)..."
              fi
              
              # Essayer depuis l'hÃ´te (si port publiÃ©)
              if curl -f -s "http://localhost:8081/actuator/health" > /dev/null 2>&1; then
                echo "âœ“ Application is healthy (host check)!"
                HEALTHY=true
                break
              # Sinon, essayer depuis l'intÃ©rieur du container avec curl
              elif docker exec "$CONTAINER_ID" sh -c "command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/actuator/health >/dev/null 2>&1" 2>/dev/null; then
                echo "âœ“ Application is healthy (container check via curl)!"
                HEALTHY=true
                break
              # Fallback sur wget si curl n'est pas disponible
              elif docker exec "$CONTAINER_ID" sh -c "command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/actuator/health >/dev/null 2>&1" 2>/dev/null; then
                echo "âœ“ Application is healthy (container check via wget)!"
                HEALTHY=true
                break
              else
                echo "  Application not ready yet, waiting..."
              fi
            done
            
            if [ "$HEALTHY" = false ]; then
              echo "WARNING: Health check failed after $MAX_RETRIES attempts, but container is running"
              echo "Recent logs:"
              $COMPOSE_CMD -p "$COMPOSE_PROJECT_NAME" logs --tail=50 "$SERVICE_NAME" 2>&1 || docker logs "$CONTAINER_ID" --tail=50 2>&1 || true
              echo "Container status:"
              docker ps -a | grep "$CONTAINER_NAME" || docker ps -a | grep "$CONTAINER_ID" || true
            fi
            
            echo "Backend deployed successfully!"
            
            # Nettoyer les credentials Docker (bonne pratique de sÃ©curitÃ©)
            docker logout ghcr.io || true
            
            # VÃ©rification finale de l'accessibilitÃ© depuis l'extÃ©rieur
            echo ""
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "Final accessibility check"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "Backend URL: http://${{ secrets.STAGING_HOST }}:8081"
            echo "Health endpoint: http://${{ secrets.STAGING_HOST }}:8081/actuator/health"
            echo "Prometheus endpoint: http://${{ secrets.STAGING_HOST }}:8081/actuator/prometheus"
            echo ""
            echo "âš ï¸  IMPORTANT: For Prometheus to scrape metrics, ensure:"
            echo "   1. Security Group allows port 8081 from Monitoring VM IP"
            echo "   2. Backend container stays running (check with: docker ps)"
            echo "   3. See scripts/fix-prometheus-connection-refused.md for troubleshooting"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/deploy_key ~/.ssh/config ~/.ssh/known_hosts
      
      - name: Deployment Summary
        if: always()
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          if [ "${{ job.status }}" == "success" ]; then
            echo "âœ… DEPLOYMENT SUCCESSFUL"
            echo "Backend has been deployed to staging server"
          else
            echo "âš ï¸  DEPLOYMENT FAILED (but pipeline continues)"
            echo ""
            echo "Common causes:"
            echo "  â€¢ SSH secrets not configured in GitHub"
            echo "  â€¢ AWS Security Group blocking SSH (port 22)"
            echo "  â€¢ Server instance not running or IP changed"
            echo ""
            echo "Check the logs above for detailed diagnostics."
            echo "The pipeline will continue even if deployment fails."
          fi
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

  # ============================================
  # DEPLOY MONITORING (for develop branch)
  # DÃ©ployÃ© aprÃ¨s le backend pour s'assurer que le monitoring
  # peut scraper les mÃ©triques du backend dÃ©ployÃ©
  # ============================================
  deploy-monitoring:
    name: Backend - Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    # â­ Meilleure pratique : Monitoring uniquement sur develop (avec staging)
    # Le monitoring surveille l'environnement staging
    if: github.event_name == 'push' && github.ref_name == 'develop'
    environment: monitoring
    timeout-minutes: 15
    continue-on-error: true
    concurrency:
      group: deploy-monitoring-${{ github.ref_name }}
      cancel-in-progress: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Verify monitoring files exist
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found!"
            exit 1
          fi
          echo "Monitoring directory found"
          echo "Files to deploy:"
          find monitoring -type f -name "*.yml" -o -name "*.yaml" -o -name "*.conf" | head -20

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.MONITORING_SSH_PRIVATE_KEY }}" > ~/.ssh/monitoring_key
          chmod 600 ~/.ssh/monitoring_key
          # â­ SÃ©curitÃ© SSH : utiliser KNOWN_HOSTS depuis secret si disponible (protection MITM)
          # Sinon, fallback sur ssh-keyscan (acceptable pour staging, mais idÃ©alement pinner l'empreinte)
          if [ -n "${{ secrets.MONITORING_KNOWN_HOSTS }}" ]; then
            echo "${{ secrets.MONITORING_KNOWN_HOSTS }}" > ~/.ssh/known_hosts
            chmod 644 ~/.ssh/known_hosts
            STRICT_CHECK="yes"
            echo "âœ“ Using pinned SSH host fingerprint from secret (StrictHostKeyChecking=yes)"
          else
            # Fallback : ssh-keyscan (acceptable pour staging, mais moins sÃ©curisÃ©)
            ssh-keyscan -H ${{ secrets.MONITORING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
            STRICT_CHECK="no"
            echo "âš ï¸  Using ssh-keyscan (NON-SECURE mode - consider pinning host fingerprint in MONITORING_KNOWN_HOSTS secret for production)"
          fi
          # Configuration SSH optimisÃ©e
          cat >> ~/.ssh/config << EOF
          Host monitoring-vm
            HostName ${{ secrets.MONITORING_HOST }}
            User ${{ secrets.MONITORING_USER }}
            IdentityFile ~/.ssh/monitoring_key
            StrictHostKeyChecking $STRICT_CHECK
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        run: |
          echo "Testing SSH connection to Monitoring VM..."
          STRICT_CHECK="$([ -n "${{ secrets.MONITORING_KNOWN_HOSTS }}" ] && echo "yes" || echo "no")"
          timeout 10 ssh -F ~/.ssh/config -o ConnectTimeout=5 -o StrictHostKeyChecking=$STRICT_CHECK monitoring-vm "echo 'SSH connection successful'" || {
            echo "ERROR: SSH connection failed"
            echo "Verify that:"
            echo "  - The Monitoring VM is accessible from GitHub Actions"
            echo "  - The AWS Security Group allows connections from GitHub"
            echo "  - The secrets MONITORING_HOST, MONITORING_USER, MONITORING_SSH_PRIVATE_KEY are correct"
            exit 1
          }
          echo "SSH connection OK"

      - name: Prepare monitoring directory
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 60s
          command_timeout: 30s
          script: |
            mkdir -p ${{ env.MONITORING_DIR }}
            chmod 755 ${{ env.MONITORING_DIR }}
            # Create grafana dashboards directory with correct permissions
            sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards
            sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards
            chmod 755 ${{ env.MONITORING_DIR }}/grafana/dashboards
      
      - name: Copy monitoring files to VM
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found in workspace"
            exit 1
          fi
          # DÃ©terminer StrictHostKeyChecking selon la prÃ©sence de KNOWN_HOSTS
          STRICT_CHECK="$([ -n "${{ secrets.MONITORING_KNOWN_HOSTS }}" ] && echo "yes" || echo "no")"
          
          echo "Preparing directories on VM with correct permissions..."
          ssh -F ~/.ssh/config \
              -o StrictHostKeyChecking=$STRICT_CHECK \
              -o ConnectTimeout=10 \
              monitoring-vm "sudo mkdir -p ${{ env.MONITORING_DIR }}/grafana/dashboards && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards" || true
          
          echo "Copying monitoring files via SCP..."
          if scp -F ~/.ssh/config \
              -o StrictHostKeyChecking=$STRICT_CHECK \
              -o ConnectTimeout=10 \
              -r monitoring/* \
              monitoring-vm:${{ env.MONITORING_DIR }}/; then
            echo "Files copied successfully via SCP"
            # Fix permissions for grafana dashboards if needed
            ssh -F ~/.ssh/config \
                -o StrictHostKeyChecking=$STRICT_CHECK \
                -o ConnectTimeout=10 \
                monitoring-vm "sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/grafana/dashboards 2>/dev/null || true"
          else
            echo "SCP failed, trying alternative method via SSH..."
            cd monitoring
            for file in $(find . -type f); do
              target_dir="${{ env.MONITORING_DIR }}/$(dirname "$file")"
              # Use sudo for grafana/dashboards directory
              if echo "$file" | grep -q "grafana/dashboards"; then
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=$STRICT_CHECK \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo mkdir -p $target_dir && sudo chown -R ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=$STRICT_CHECK \
                    -o ConnectTimeout=10 \
                    monitoring-vm "sudo tee ${{ env.MONITORING_DIR }}/$file > /dev/null && sudo chown ${{ secrets.MONITORING_USER }}:${{ secrets.MONITORING_USER }} ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              else
                ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=$STRICT_CHECK \
                    -o ConnectTimeout=10 \
                    monitoring-vm "mkdir -p $target_dir" || true
                cat "$file" | ssh -F ~/.ssh/config \
                    -o StrictHostKeyChecking=$STRICT_CHECK \
                    -o ConnectTimeout=10 \
                    monitoring-vm "cat > ${{ env.MONITORING_DIR }}/$file" || {
                  echo "ERROR: Failed to copy $file"
                  exit 1
                }
              fi
            done
            echo "Files copied successfully via SSH"
          fi
      
      - name: Verify monitoring files copied
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 30s
          command_timeout: 10s
          script: |
            if [ ! -f "${{ env.MONITORING_DIR }}/docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found after copy attempt"
              exit 1
            else
              echo "Monitoring files successfully copied"
              ls -lh ${{ env.MONITORING_DIR }}/
            fi

      - name: Deploy Monitoring Stack
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 300s
          command_timeout: 120s
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail
            cd ${{ env.MONITORING_DIR }}
            
            echo "Verifying configuration files..."
            if [ ! -f "docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found!"
              exit 1
            fi
            if [ ! -f "prometheus/prometheus.yml" ]; then
              echo "ERROR: prometheus/prometheus.yml not found!"
              exit 1
            fi
            echo "Configuration files found"
            
            # OPTIMISATION: Utiliser --force-recreate au lieu de down complet
            # Cela Ã©vite de supprimer les volumes et rÃ©seaux inutilement
            echo "Updating monitoring services (force recreate if needed)..."
            # --force-recreate recrÃ©e les conteneurs avec les nouvelles images
            # --remove-orphans supprime les conteneurs orphelins
            # Pas besoin de 'down' complet sauf en cas de problÃ¨me majeur
            
            echo "Checking disk space..."
            df -h / | tail -1
            DISK_USAGE_INITIAL=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            
            echo "Safe Docker disk space cleanup (preserving volumes)..."
            
            # â­ AMÃ‰LIORATION: Nettoyage sÃ©curisÃ© sans supprimer les volumes (Grafana/Prometheus)
            # ArrÃªter uniquement les conteneurs non critiques (pas ceux du monitoring)
            echo "Stopping non-critical containers..."
            docker ps --format "{{.Names}}" | grep -v -E "(grafana|prometheus|loki|alertmanager|node-exporter|cadvisor|promtail)" | xargs -r docker stop 2>/dev/null || true
            
            # Supprimer uniquement les conteneurs arrÃªtÃ©s non critiques
            echo "Removing stopped non-critical containers..."
            docker ps -a --format "{{.Names}}" | grep -v -E "(grafana|prometheus|loki|alertmanager|node-exporter|cadvisor|promtail)" | xargs -r docker rm 2>/dev/null || true
            
            # Supprimer les images dangling (sans tag) - SÃ‰CURISÃ‰
            echo "Removing dangling images..."
            docker image prune -f || true
            
            # Supprimer les images non utilisÃ©es (sans filtre de temps si disque plein)
            if [ "$DISK_USAGE_INITIAL" -gt 90 ]; then
              echo "âš ï¸  Disk usage > 90%, performing aggressive cleanup..."
              # Supprimer toutes les images non utilisÃ©es (pas seulement celles de +24h)
              echo "Removing all unused images..."
              docker image prune -af || true
            else
              echo "Removing unused images (older than 24h)..."
              docker image prune -af --filter "until=24h" || true
            fi
            
            # Nettoyer le build cache (plus agressif si disque plein)
            if [ "$DISK_USAGE_INITIAL" -gt 90 ]; then
              echo "Cleaning all build cache..."
              docker builder prune -af || true
            else
              echo "Cleaning build cache (older than 24h)..."
              docker builder prune -af --filter "until=24h" || true
            fi
            
            # Nettoyer les logs Docker (plus agressif si disque plein)
            if [ "$DISK_USAGE_INITIAL" -gt 90 ]; then
              echo "Cleaning Docker logs (all logs, not just 1 day old)..."
              find /var/lib/docker/containers/ -type f -name "*.log" -delete 2>/dev/null || true
              # Limiter la taille des logs actuels
              for container in $(docker ps --format "{{.Names}}"); do
                docker logs --tail 100 "$container" > /dev/null 2>&1 || true
              done
              journalctl --vacuum-time=1h 2>/dev/null || true
            else
              echo "Cleaning Docker logs (older than 1 day)..."
              find /var/lib/docker/containers/ -type f -name "*.log" -mtime +1 -delete 2>/dev/null || true
              journalctl --vacuum-time=1d 2>/dev/null || true
            fi
            
            # â­ AMÃ‰LIORATION: Ne JAMAIS supprimer les volumes (risque de perte de donnÃ©es Grafana/Prometheus)
            # docker volume prune est DANGEREUX - on ne le fait PAS
            
            # Nettoyer uniquement les rÃ©seaux non utilisÃ©s (pas ceux du monitoring)
            echo "Removing unused networks (preserving monitoring-network)..."
            docker network ls --format "{{.Name}}" | grep -v -E "(monitoring-network|bridge|host)" | xargs -r docker network rm 2>/dev/null || true
            
            # Nettoyage systÃ¨me sÃ©curisÃ© (SANS --volumes pour prÃ©server les donnÃ©es)
            if [ "$DISK_USAGE_INITIAL" -gt 90 ]; then
              echo "Performing aggressive system prune (preserving volumes)..."
              docker system prune -af || true
            else
              echo "Safe system prune (preserving volumes)..."
              docker system prune -af --filter "until=24h" || true
            fi
            
            # VÃ©rifier l'espace utilisÃ© par Docker
            echo "Docker disk usage:"
            docker system df || true
            
            echo "Disk space after cleanup:"
            df -h / | tail -1
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            
            # Si le disque est toujours plein aprÃ¨s nettoyage agressif, arrÃªter temporairement les services monitoring
            if [ "$DISK_USAGE" -gt 95 ]; then
              echo "âš ï¸  CRITICAL: Disk usage still above 95% ($DISK_USAGE%) after cleanup"
              echo "   All Docker images are in use by active containers"
              echo "   Attempting emergency cleanup: temporarily stopping monitoring services..."
              
              # Sauvegarder l'Ã©tat actuel des conteneurs monitoring
              MONITORING_CONTAINERS=$(docker ps --format "{{.Names}}" | grep -E "(grafana|prometheus|loki|alertmanager|node-exporter|cadvisor|promtail)" | tr '\n' ' ' || true)
              
              if [ -n "$MONITORING_CONTAINERS" ]; then
                echo "   Stopping monitoring containers temporarily: $MONITORING_CONTAINERS"
                # ArrÃªter les services monitoring (les volumes seront prÃ©servÃ©s)
                docker stop $MONITORING_CONTAINERS 2>/dev/null || true
                sleep 2
                
                # Maintenant on peut supprimer les images (elles ne sont plus utilisÃ©es)
                echo "   Removing unused images after stopping containers..."
                docker image prune -af || true
                
                # Nettoyer les anciennes versions d'images (garder seulement les plus rÃ©centes)
                echo "   Removing old image versions..."
                docker images --format "{{.Repository}}:{{.Tag}} {{.ID}}" | sort -V | head -n -7 | awk '{print $2}' | xargs -r docker rmi 2>/dev/null || true
                
                # Nettoyage systÃ¨me final
                docker system prune -af || true
                
                # VÃ©rifier l'espace libÃ©rÃ©
                DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
                echo "   Disk usage after emergency cleanup: ${DISK_USAGE}%"
                
                # RedÃ©marrer les services monitoring (sera fait plus tard dans le script)
                echo "   Monitoring services will be restarted after image pull"
              else
                echo "   No monitoring containers found, performing final cleanup..."
                docker image prune -af || true
                docker system prune -af || true
                DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
              fi
              
              # Si toujours plein, erreur critique
              if [ "$DISK_USAGE" -gt 95 ]; then
                echo "âŒ ERROR: Disk usage is still above 95% ($DISK_USAGE%) after emergency cleanup"
                echo "   The EBS volume is too small (6.8GB). Manual intervention required:"
                echo ""
                echo "   SOLUTION 1 (RECOMMANDÃ‰): Increase EBS volume size in AWS"
                echo "   1. Go to AWS Console > EC2 > Volumes"
                echo "   2. Select the volume attached to the monitoring instance"
                echo "   3. Actions > Modify Volume > Increase to 20GB minimum"
                echo "   4. SSH to the instance and run:"
                echo "      sudo growpart /dev/xvda 1"
                echo "      sudo resize2fs /dev/xvda1"
                echo ""
                echo "   SOLUTION 2 (TEMPORAIRE): Manual cleanup via SSH"
                echo "   SSH to the server and run:"
                echo "   docker system prune -af  # âš ï¸ This will remove unused images"
                echo ""
                echo "   Current disk usage:"
                df -h /
                echo "   Docker disk usage:"
                docker system df || true
                exit 1
              fi
            fi
            
            # VÃ©rifier l'espace disque avant de pull les images
            DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ "$DISK_USAGE" -gt 95 ]; then
              echo "ERROR: Disk usage is still above 95% ($DISK_USAGE%), cannot pull images safely"
              echo "Current disk usage:"
              df -h /
              echo "Docker disk usage:"
              docker system df || true
              exit 1
            fi
            
            echo "Verifying Docker Compose version..."
            docker compose version || {
              echo "ERROR: docker compose (v2) not found. Checking docker-compose (v1)..."
              docker-compose version || {
                echo "ERROR: Neither docker compose nor docker-compose found"
                exit 1
              }
              COMPOSE_CMD="docker-compose"
            }
            COMPOSE_CMD="${COMPOSE_CMD:-docker compose}"
            
            echo "Pulling Docker images (disk usage: ${DISK_USAGE}%)..."
            $COMPOSE_CMD -f docker-compose.monitoring.yml pull
            
            echo "Starting monitoring stack..."
            # DÃ©marrer avec gestion d'erreur pour le rÃ©seau
            set +e  # Ne pas arrÃªter sur les erreurs
            $COMPOSE_CMD -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
            COMPOSE_EXIT_CODE=$?
            set -e  # RÃ©activer l'arrÃªt sur erreur
            
            if [ $COMPOSE_EXIT_CODE -ne 0 ]; then
              echo "WARNING: Error detected, checking network..."
              # Si l'erreur est liÃ©e au rÃ©seau, supprimer le rÃ©seau et rÃ©essayer
              if docker network ls | grep -q monitoring-network; then
                echo "Force removing problematic network..."
                # ArrÃªter tous les conteneurs qui utilisent ce rÃ©seau
                CONTAINER_IDS=$(docker ps --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$CONTAINER_IDS" ]; then
                  echo "$CONTAINER_IDS" | xargs docker stop 2>/dev/null || true
                  echo "$CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                OLD_CONTAINER_IDS=$(docker ps -a --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$OLD_CONTAINER_IDS" ]; then
                  echo "$OLD_CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                # Supprimer le rÃ©seau
                docker network rm monitoring-network 2>/dev/null || docker network prune -f
                sleep 3
                echo "Retrying startup..."
                $COMPOSE_CMD -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
              else
                echo "ERROR: Non-network related error, see logs"
                $COMPOSE_CMD -f docker-compose.monitoring.yml logs
                exit 1
              fi
            fi
            
            echo "Waiting for services to start (30s)..."
            sleep 30
            
            echo "Container status:"
            $COMPOSE_CMD -f docker-compose.monitoring.yml ps
            
            echo "Checking services..."
            
            # VÃ©rifier que les conteneurs sont en cours d'exÃ©cution
            echo "Verifying containers are running..."
            
            # VÃ©rifier Prometheus
            if docker ps | grep -q prometheus; then
              PROMETHEUS_STATUS=$(docker ps --filter "name=prometheus" --format "{{.Status}}")
              echo "Prometheus is running: $PROMETHEUS_STATUS"
            else
              echo "ERROR: Prometheus container is not running"
              $COMPOSE_CMD -f docker-compose.monitoring.yml logs prometheus | tail -30
              exit 1
            fi
            
            # VÃ©rifier Grafana (peut prendre plus de temps pour le health check)
            GRAFANA_RUNNING=$(docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana && echo "yes" || echo "no")
            
            if [ "$GRAFANA_RUNNING" = "yes" ]; then
              GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
              echo "Grafana container found: $GRAFANA_STATUS"
              
              # Si Grafana est encore en "health: starting", attendre un peu plus
              if echo "$GRAFANA_STATUS" | grep -q "health: starting"; then
                echo "Grafana is still starting, waiting additional 30s..."
                sleep 30
                GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
                echo "Grafana status after wait: $GRAFANA_STATUS"
                
                # VÃ©rifier Ã  nouveau si Grafana est toujours en cours d'exÃ©cution
                if ! docker ps --filter "name=grafana" --format "{{.Names}}" | grep -q grafana; then
                  echo "ERROR: Grafana container stopped during startup"
                  $COMPOSE_CMD -f docker-compose.monitoring.yml logs grafana | tail -50
                  exit 1
                fi
              fi
              
              echo "Grafana is running successfully"
            else
              echo "ERROR: Grafana container is not running"
              echo "Checking all containers:"
              docker ps -a | grep grafana || echo "No grafana container found"
              echo "Grafana logs:"
              $COMPOSE_CMD -f docker-compose.monitoring.yml logs grafana | tail -50
              exit 1
            fi
            
            # VÃ©rifier les autres services essentiels
            for service in alertmanager node-exporter cadvisor; do
              if docker ps | grep -q "$service"; then
                SERVICE_STATUS=$(docker ps --filter "name=$service" --format "{{.Status}}")
                echo "$service is running: $SERVICE_STATUS"
              else
                echo "WARNING: $service container is not running (non-critical)"
              fi
            done
            
            echo "Recent logs (last 10 lines):"
            $COMPOSE_CMD -f docker-compose.monitoring.yml logs --tail=10
            
            echo "Monitoring stack deployed successfully!"
            echo "Prometheus should now scrape the backend on ${{ secrets.STAGING_HOST }}:8081"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/monitoring_key ~/.ssh/config ~/.ssh/known_hosts

  # ============================================
  # RELEASE (semantic versioning + changelog)
  # ============================================
  # â­ Note: Utilise semantic-release (Node.js) mÃªme pour un backend Java
  # C'est une approche valide pour automatiser les versions et le changelog
  # Le job skip proprement si package.json n'existe pas (pas de bruit dans la pipeline)
  # Alternative: utiliser maven-release-plugin si prÃ©fÃ©rence pour un workflow 100% Maven
  release:
    name: Release
    runs-on: ubuntu-latest
    needs: [lint, test, coverage, build, sonar]
    # â­ DevSecOps strict : Release uniquement si SonarCloud Quality Gate passe
    # Ã‰vite de crÃ©er une release si Quality Gate est KO
    if: github.event_name == 'push' && github.ref_name == 'main' && needs.sonar.result == 'success'
    concurrency:
      group: release-main
      cancel-in-progress: false
    outputs:
      package_exists: ${{ steps.verify_package.outputs.package_exists }}
    permissions:
      contents: write
      issues: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      
      # SÃ©curiser : vÃ©rifier que package.json existe (requis pour semantic-release)
      - name: Verify package.json exists
        id: verify_package
        run: |
          if [ ! -f "package.json" ]; then
            echo "package_exists=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  package.json not found. Release job requires package.json for semantic-release."
            echo "Skipping release job (this is expected if package.json doesn't exist)."
          else
            echo "package_exists=true" >> $GITHUB_OUTPUT
            echo "âœ“ package.json found"
          fi

      # Indiquer clairement que le job est skip si package.json n'existe pas
      # (Les steps suivants sont conditionnels avec if: package_exists == 'true', donc ils ne s'exÃ©cuteront pas)
      - name: Skip release job if package.json missing
        if: steps.verify_package.outputs.package_exists != 'true'
        run: |
          echo "Skipping release (no package.json)."

      - name: Setup Node.js
        if: steps.verify_package.outputs.package_exists == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          # Cache npm sans cache-dependency-path pour tolÃ©rance si package-lock.json absent
          # Le cache fonctionne quand mÃªme mais est moins prÃ©cis sans lockfile
          cache: 'npm'

      - name: Install semantic-release dependencies
        if: steps.verify_package.outputs.package_exists == 'true'
        run: |
          # Utiliser npm ci si package-lock.json existe (plus rapide et reproductible)
          # Sinon fallback sur npm install
          if [ -f "package-lock.json" ]; then
            echo "Using npm ci (package-lock.json found) - faster and more reproducible"
            npm ci --legacy-peer-deps --no-audit --no-fund
          else
            echo "Using npm install (no package-lock.json found)"
            rm -rf node_modules
            npm install --legacy-peer-deps --no-audit --no-fund
          fi
          
          # VÃ©rification basique : semantic-release doit Ãªtre disponible
          if [ ! -f "node_modules/.bin/semantic-release" ]; then
            echo "ERROR: semantic-release binary not found after installation"
            echo "This should not happen with npm ci. Checking installation..."
            npm list semantic-release || true
            exit 1
          fi
          
          echo "âœ“ Dependencies installed successfully"
          echo "Verifying semantic-release is available..."
          npm list semantic-release --depth=0 || true

      - name: Setup JDK ${{ env.JAVA_VERSION }}
        if: steps.verify_package.outputs.package_exists == 'true'
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'

      - name: Run semantic-release
        if: steps.verify_package.outputs.package_exists == 'true'
        id: semantic_release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: npx semantic-release

  # ============================================
  # DOCKER TAG RELEASE (tag versionnÃ© aprÃ¨s semantic-release)
  # ============================================
  docker-tag-release:
    name: Backend - Docker Tag Release Version
    runs-on: ubuntu-latest
    needs: [release, docker-build]
    # Ne s'exÃ©cute que si release a rÃ©ussi ET package.json existe (semantic-release a pu crÃ©er un tag)
    if: github.event_name == 'push' && github.ref_name == 'main' && needs.release.result == 'success' && needs.release.outputs.package_exists == 'true'
    concurrency:
      group: release-main
      cancel-in-progress: false
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get latest Git tag (version)
        id: git_tag
        run: |
          # RÃ©cupÃ©rer le tag Git qui correspond exactement au commit courant (plus sÃ»r que git describe)
          # semantic-release tag toujours HEAD, donc on vÃ©rifie que le tag existe sur ce commit
          # Si plusieurs tags existent sur HEAD, prendre le plus haut (v2.3.1 > v2.3.0)
          LATEST_TAG=$(git tag --points-at HEAD --sort=-v:refname | head -n 1 || echo "")
          if [ -z "$LATEST_TAG" ]; then
            echo "tag_exists=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  No Git tag found on current commit. Skipping Docker release tag."
            exit 0
          fi
          # Enlever le prÃ©fixe "v" pour le tag Docker (v1.2.3 -> 1.2.3)
          VERSION="${LATEST_TAG#v}"
          echo "âœ“ Found Git tag on current commit: $LATEST_TAG (docker tag: $VERSION)"
          echo "tag_exists=true" >> $GITHUB_OUTPUT
          echo "git_tag=$LATEST_TAG" >> $GITHUB_OUTPUT
          echo "version=$VERSION" >> $GITHUB_OUTPUT

      - name: Set up Docker Buildx
        if: steps.git_tag.outputs.tag_exists == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        if: steps.git_tag.outputs.tag_exists == 'true'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Tag Docker image with release version (from commit SHA)
        if: steps.git_tag.outputs.tag_exists == 'true'
        run: |
          set -euo pipefail
          
          # Image exacte produite par docker-build pour ce commit (plus sÃ»r que :main)
          SOURCE_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:main-${{ github.sha }}"
          TARGET_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.git_tag.outputs.version }}"
          LATEST_IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          
          echo "Tagging Docker image with version..."
          echo "Source: $SOURCE_IMAGE"
          echo "Target version: $TARGET_IMAGE"
          echo "Target latest: $LATEST_IMAGE"
          
          # Pull l'image source par SHA (garantit que c'est l'image du commit release)
          echo "Expected source tag: main-${{ github.sha }}"
          docker pull "$SOURCE_IMAGE" || {
            echo "âŒ ERROR: Source image not found. docker-build may have failed for this commit."
            echo "Expected image: $SOURCE_IMAGE"
            echo "If missing: check docker-build logs and GHCR permissions."
            exit 1
          }
          
          # Tag l'image avec la version (sans le prÃ©fixe "v")
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "âœ“ Successfully pushed version tag: $TARGET_IMAGE"
          
          # Tag et push latest uniquement aprÃ¨s succÃ¨s de release (plus propre que dans docker-build)
          docker tag "$SOURCE_IMAGE" "$LATEST_IMAGE"
          docker push "$LATEST_IMAGE"
          echo "âœ“ Successfully pushed latest tag: $LATEST_IMAGE"
          
          # â­ Job Summary: Afficher les tags Docker dans le rÃ©sumÃ© GitHub
          echo "## ğŸ³ Docker Release Tags" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Tag | Image |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Version | \`$TARGET_IMAGE\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Latest | \`$LATEST_IMAGE\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ“¦ Git Tag: \`${{ steps.git_tag.outputs.git_tag }}\`" >> $GITHUB_STEP_SUMMARY