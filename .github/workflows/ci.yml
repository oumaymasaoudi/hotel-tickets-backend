name: Backend CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  JAVA_VERSION: '17'
  MAVEN_OPTS: '-Xmx2048m -XX:+UseG1GC'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/backend
  MONITORING_DIR: /opt/monitoring

jobs:
  # ============================================
  # LINT & CODE QUALITY
  # ============================================
  lint:
    name: Backend - Lint & Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Run Maven Checkstyle
        run: mvn checkstyle:check
        continue-on-error: true
      
      - name: Run Maven SpotBugs
        run: mvn spotbugs:spotbugs -Duser.language=en -Duser.country=US
        continue-on-error: true
        env:
          JAVA_TOOL_OPTIONS: '-Duser.language=en -Duser.country=US'
      
      - name: Upload SpotBugs report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: spotbugs-report
          path: target/spotbugsXml.xml
          retention-days: 7
          if-no-files-found: ignore
        continue-on-error: true
      
      - name: Upload Checkstyle report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: checkstyle-report
          path: target/checkstyle-result.xml
          retention-days: 7
        continue-on-error: true

  # ============================================
  # TESTS
  # ============================================
  test:
    name: Backend - Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Run tests with JaCoCo agent
        run: mvn clean test -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports
          path: target/surefire-reports
          retention-days: 7
      
      - name: Upload compiled classes and JaCoCo data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-classes-jacoco
          path: |
            target/classes
            target/test-classes
            target/jacoco.exec
          retention-days: 7

  # ============================================
  # COVERAGE
  # ============================================
  coverage:
    name: Backend - Code Coverage
    runs-on: ubuntu-latest
    needs: [test]
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Download compiled classes and JaCoCo data
        uses: actions/download-artifact@v4
        with:
          name: test-classes-jacoco
          path: target
      
      - name: Generate coverage report
        run: mvn -B jacoco:report
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Check coverage threshold (optional)
        run: |
          if mvn -B jacoco:check; then
            echo "Coverage threshold met!"
          else
            echo "Coverage threshold not met, but continuing..."
          fi
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
        continue-on-error: true
      
      - name: Upload JaCoCo Report
        uses: actions/upload-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
          retention-days: 30
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./target/site/jacoco/jacoco.xml
          flags: backend
          name: backend-coverage
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}
        continue-on-error: true

  # ============================================
  # BUILD
  # ============================================
  build:
    name: Backend - Build
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Build with Maven
        run: mvn clean package -DskipTests -DskipITs
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Check JAR size
        run: |
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            SIZE=$(du -sh "$JAR_FILE" | cut -f1)
            echo "JAR size: $SIZE"
            echo "Build successful: $JAR_FILE"
          else
            echo "Build failed - JAR file not found"
            exit 1
          fi
      
      - name: Prepare JAR artifact
        run: |
          mkdir -p artifacts
          JAR_FILE=$(find target -name "*.jar" -not -name "*-sources.jar" -not -name "*-javadoc.jar" | head -1)
          if [ -f "$JAR_FILE" ]; then
            cp "$JAR_FILE" artifacts/
            echo "JAR artifact prepared: $(basename $JAR_FILE)"
          else
            echo "No JAR file found"
            exit 1
          fi
      
      - name: Upload JAR artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-jar
          path: artifacts/*.jar
          retention-days: 7

  # ============================================
  # SONARQUBE ANALYSIS
  # ============================================
  sonar:
    name: Backend - SonarQube Analysis
    runs-on: ubuntu-latest
    needs: [lint, test, coverage]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    timeout-minutes: 25
    continue-on-error: true
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'
      
      - name: Download test classes and JaCoCo data
        uses: actions/download-artifact@v4
        with:
          name: test-classes-jacoco
          path: target
      
      - name: Download coverage report
        uses: actions/download-artifact@v4
        with:
          name: jacoco-report
          path: target/site/jacoco
          continue-on-error: true
      
      - name: Generate JaCoCo report if missing
        if: failure()
        run: |
          echo "Coverage report not found in artifacts, generating it..."
          mvn -B jacoco:report
        env:
          MAVEN_OPTS: ${{ env.MAVEN_OPTS }}
      
      - name: Verify JaCoCo Report exists
        run: |
          if [ ! -f "target/site/jacoco/jacoco.xml" ]; then
            echo "ERROR: JaCoCo report not found!"
            echo "Checking target directory structure:"
            find target -name "*.xml" -o -name "jacoco*" | head -10
            exit 1
          else
            echo "JaCoCo report found: target/site/jacoco/jacoco.xml"
            ls -lh target/site/jacoco/jacoco.xml
          fi
      
      - name: SonarCloud Scan
        uses: SonarSource/sonarcloud-github-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        continue-on-error: true
      
      - name: Verify SonarCloud Analysis
        if: always()
        run: |
          echo "SonarCloud analysis completed"
          echo "Check SonarCloud dashboard: https://sonarcloud.io/project/overview?id=oumaymasaoudi_hotel-tickets-backend"
          if [ "${{ job.status }}" != "success" ]; then
            echo "WARNING: SonarCloud analysis completed with warnings or errors"
            echo "Check the logs above for details"
          fi

  # ============================================
  # DOCKER BUILD & PUSH (for develop branch)
  # ============================================
  docker-build:
    name: Backend - Docker Build & Push
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ============================================
  # DEPLOY TO STAGING (for develop branch)
  # ============================================
  deploy-staging:
    name: Backend - Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker-build]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: staging
    timeout-minutes: 15  # Augmenté pour gérer les connexions SSH lentes
    concurrency:
      group: deploy-staging
      cancel-in-progress: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Optimisation : ne récupérer que le dernier commit

      - name: Verify docker-compose.yml exists
        run: |
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found!"
            exit 1
          fi
          echo "docker-compose.yml found ($(du -h docker-compose.yml | cut -f1))"

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          # Ajouter l'host à known_hosts pour éviter les prompts
          ssh-keyscan -H ${{ secrets.STAGING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host backend-staging
            HostName ${{ secrets.STAGING_HOST }}
            User ${{ secrets.STAGING_USER }}
            IdentityFile ~/.ssh/deploy_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        run: |
          echo "Testing SSH connection..."
          timeout 10 ssh -F ~/.ssh/config -o ConnectTimeout=5 -o StrictHostKeyChecking=no backend-staging "echo 'SSH connection successful'" || {
            echo "ERROR: SSH connection failed"
            echo "Verify that:"
            echo "  - The VM is accessible from GitHub Actions"
            echo "  - The AWS Security Group allows connections from GitHub"
            echo "  - The secrets STAGING_HOST, STAGING_USER, STAGING_SSH_PRIVATE_KEY are correct"
            exit 1
          }
          echo "SSH connection OK"

      - name: Copy docker-compose to staging
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          source: "docker-compose.yml"
          target: "/opt/hotel-ticket-hub-backend-staging/"
          strip_components: 0
          timeout: 120s  # Augmenté pour gérer les connexions lentes
          command_timeout: 60s
          debug: true  # Activé pour diagnostiquer les problèmes
          use_insecure_cipher: false

      - name: Deploy to staging VM
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_PRIVATE_KEY }}
          timeout: 300s
          command_timeout: 120s
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail  # Mode strict : erreur si variable non définie
            cd /opt/hotel-ticket-hub-backend-staging
            
            echo "Stopping existing services..."
            # Stop old systemd service if it exists
            sudo systemctl stop hotel-ticket-hub-backend-staging 2>/dev/null || true
            sudo systemctl disable hotel-ticket-hub-backend-staging 2>/dev/null || true
            
            # Stop any container using port 8081
            CONTAINERS=$(docker ps -q --filter "publish=8081" 2>/dev/null || true)
            if [ -n "$CONTAINERS" ]; then
              docker stop $CONTAINERS 2>/dev/null || true
            fi
            
            # Stop any container using port 9100 (Node Exporter)
            CONTAINERS_9100=$(docker ps -q --filter "publish=9100" 2>/dev/null || true)
            if [ -n "$CONTAINERS_9100" ]; then
              docker stop $CONTAINERS_9100 2>/dev/null || true
              docker rm $CONTAINERS_9100 2>/dev/null || true
            fi
            
            OLD_CONTAINERS=$(docker ps -aq --filter "name=hotel-ticket-hub-backend-staging" --filter "name=node-exporter-backend" 2>/dev/null || true)
            if [ -n "$OLD_CONTAINERS" ]; then
              docker rm -f $OLD_CONTAINERS 2>/dev/null || true
            fi
            
            # Stop and remove old docker-compose containers
            docker compose down 2>/dev/null || true
            
            echo "Checking .env file..."
            if [ ! -f .env ]; then
              echo "ERROR: .env file not found! Please create it first."
              exit 1
            fi
            
            echo "Connecting to GitHub Container Registry..."
            echo "${{ secrets.GHCR_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
            
            echo "Pulling Docker image..."
            export DOCKER_IMAGE=ghcr.io/${{ env.IMAGE_NAME }}:develop
            docker pull $DOCKER_IMAGE || docker pull ghcr.io/${{ env.IMAGE_NAME }}:latest
            
            echo "Starting container..."
            export DOCKER_IMAGE=$DOCKER_IMAGE
            docker compose --env-file .env up -d
            
            echo "Waiting for container to start (30s)..."
            sleep 30
            
            echo "Checking container status..."
            CONTAINER_ID=$(docker ps -q --filter "name=hotel-ticket-hub-backend-staging" 2>/dev/null || true)
            if [ -z "$CONTAINER_ID" ]; then
              echo "ERROR: Container is not running"
              echo "Checking stopped containers..."
              docker ps -a | grep hotel-ticket-hub-backend-staging || true
              echo "Container logs (last 100 lines):"
              docker compose logs --tail=100 hotel-ticket-hub-backend-staging || docker logs hotel-ticket-hub-backend-staging --tail=100 2>&1 || true
              echo "Exit code:"
              docker inspect hotel-ticket-hub-backend-staging --format='{{.State.ExitCode}}' 2>/dev/null || echo "Container not found"
              exit 1
            fi
            
            echo "Container is running (ID: $CONTAINER_ID)"
            echo "Waiting for Spring Boot to fully start (30s)..."
            sleep 30
            
            echo "Checking application health..."
            HEALTH_CHECK_URL="http://localhost:8081/actuator/health"
            if curl -f -s "$HEALTH_CHECK_URL" > /dev/null 2>&1; then
              echo "Application is healthy!"
            else
              echo "WARNING: Health check failed, but container is running"
              echo "Recent logs:"
              docker compose logs --tail=50 hotel-ticket-hub-backend-staging || docker logs hotel-ticket-hub-backend-staging --tail=50 2>&1 || true
            fi
            
            echo "Backend deployed successfully!"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/deploy_key ~/.ssh/config ~/.ssh/known_hosts

  # ============================================
  # DEPLOY MONITORING (for develop branch)
  # Déployé après le backend pour s'assurer que le monitoring
  # peut scraper les métriques du backend déployé
  # ============================================
  deploy-monitoring:
    name: Backend - Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment: monitoring
    timeout-minutes: 15
    concurrency:
      group: deploy-monitoring
      cancel-in-progress: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Verify monitoring files exist
        run: |
          if [ ! -d "monitoring" ]; then
            echo "ERROR: monitoring directory not found!"
            exit 1
          fi
          echo "Monitoring directory found"
          echo "Files to deploy:"
          find monitoring -type f -name "*.yml" -o -name "*.yaml" -o -name "*.conf" | head -20

      - name: Setup SSH Config
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.MONITORING_SSH_PRIVATE_KEY }}" > ~/.ssh/monitoring_key
          chmod 600 ~/.ssh/monitoring_key
          # Ajouter l'host à known_hosts
          ssh-keyscan -H ${{ secrets.MONITORING_HOST }} >> ~/.ssh/known_hosts 2>/dev/null || true
          # Configuration SSH optimisée
          cat >> ~/.ssh/config << EOF
          Host monitoring-vm
            HostName ${{ secrets.MONITORING_HOST }}
            User ${{ secrets.MONITORING_USER }}
            IdentityFile ~/.ssh/monitoring_key
            StrictHostKeyChecking no
            UserKnownHostsFile ~/.ssh/known_hosts
            ConnectTimeout 10
            ServerAliveInterval 60
            ServerAliveCountMax 3
          EOF
          chmod 600 ~/.ssh/config

      - name: Test SSH Connection
        run: |
          echo "Testing SSH connection to Monitoring VM..."
          timeout 10 ssh -F ~/.ssh/config -o ConnectTimeout=5 -o StrictHostKeyChecking=no monitoring-vm "echo 'SSH connection successful'" || {
            echo "ERROR: SSH connection failed"
            echo "Verify that:"
            echo "  - The Monitoring VM is accessible from GitHub Actions"
            echo "  - The AWS Security Group allows connections from GitHub"
            echo "  - The secrets MONITORING_HOST, MONITORING_USER, MONITORING_SSH_PRIVATE_KEY are correct"
            exit 1
          }
          echo "SSH connection OK"

      - name: Copy monitoring files to VM
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          source: "monitoring/"
          target: "${{ env.MONITORING_DIR }}/"
          strip_components: 1  # Enlever le préfixe "monitoring/"
          timeout: 120s
          command_timeout: 60s
          debug: false
          use_insecure_cipher: false

      - name: Deploy Monitoring Stack
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.MONITORING_HOST }}
          username: ${{ secrets.MONITORING_USER }}
          key: ${{ secrets.MONITORING_SSH_PRIVATE_KEY }}
          timeout: 300s
          command_timeout: 120s
          debug: false
          use_insecure_cipher: false
          script: |
            set -euo pipefail
            cd ${{ env.MONITORING_DIR }}
            
            echo "Verifying configuration files..."
            if [ ! -f "docker-compose.monitoring.yml" ]; then
              echo "ERROR: docker-compose.monitoring.yml not found!"
              exit 1
            fi
            if [ ! -f "prometheus/prometheus.yml" ]; then
              echo "ERROR: prometheus/prometheus.yml not found!"
              exit 1
            fi
            echo "Configuration files found"
            
            echo "Stopping existing services..."
            docker compose -f docker-compose.monitoring.yml down --remove-orphans 2>/dev/null || true
            
            # Nettoyer complètement : arrêter tous les conteneurs monitoring
            echo "Cleaning up monitoring containers..."
            for container in prometheus grafana alertmanager node-exporter cadvisor; do
              docker ps -a --filter "name=$container" --format "{{.ID}}" | while read id; do
                [ -n "$id" ] && docker rm -f "$id" 2>/dev/null || true
              done
            done
            
            # Supprimer le réseau existant (forcer si nécessaire)
            echo "Removing monitoring-network..."
            # Essayer plusieurs fois avec différentes méthodes
            for i in 1 2 3; do
              if docker network ls | grep -q monitoring-network; then
                echo "  Attempt $i: Removing network..."
                docker network rm monitoring-network 2>/dev/null && break || {
                  if [ $i -eq 1 ]; then
                    echo "  WARNING: Network still in use, cleaning unused networks..."
                    docker network prune -f
                    sleep 2
                  elif [ $i -eq 2 ]; then
                    echo "  Retrying after cleanup..."
                    sleep 2
                  else
                    echo "  WARNING: Cannot remove network, continuing anyway..."
                  fi
                }
              else
                echo "  Network removed"
                break
              fi
            done
            sleep 2
            
            echo "Checking disk space..."
            df -h / | tail -1
            
            echo "Cleaning up Docker disk space..."
            # Nettoyer les images non utilisées (sauf celles en cours d'utilisation)
            docker image prune -af --filter "until=24h" || true
            
            # Nettoyer les conteneurs arrêtés
            docker container prune -f || true
            
            # Nettoyer les volumes non utilisés (attention: peut supprimer des données)
            docker volume prune -f || true
            
            # Nettoyer le système Docker (images, conteneurs, réseaux, volumes)
            docker system prune -af --volumes --filter "until=24h" || true
            
            echo "Disk space after cleanup:"
            df -h / | tail -1
            
            echo "Pulling Docker images..."
            docker compose -f docker-compose.monitoring.yml pull
            
            echo "Starting monitoring stack..."
            # Démarrer avec gestion d'erreur pour le réseau
            set +e  # Ne pas arrêter sur les erreurs
            docker compose -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
            COMPOSE_EXIT_CODE=$?
            set -e  # Réactiver l'arrêt sur erreur
            
            if [ $COMPOSE_EXIT_CODE -ne 0 ]; then
              echo "WARNING: Error detected, checking network..."
              # Si l'erreur est liée au réseau, supprimer le réseau et réessayer
              if docker network ls | grep -q monitoring-network; then
                echo "Force removing problematic network..."
                # Arrêter tous les conteneurs qui utilisent ce réseau
                CONTAINER_IDS=$(docker ps --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$CONTAINER_IDS" ]; then
                  echo "$CONTAINER_IDS" | xargs docker stop 2>/dev/null || true
                  echo "$CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                OLD_CONTAINER_IDS=$(docker ps -a --filter network=monitoring-network -q 2>/dev/null || true)
                if [ -n "$OLD_CONTAINER_IDS" ]; then
                  echo "$OLD_CONTAINER_IDS" | xargs docker rm -f 2>/dev/null || true
                fi
                # Supprimer le réseau
                docker network rm monitoring-network 2>/dev/null || docker network prune -f
                sleep 3
                echo "Retrying startup..."
                docker compose -f docker-compose.monitoring.yml up -d --force-recreate --remove-orphans
              else
                echo "ERROR: Non-network related error, see logs"
                docker compose -f docker-compose.monitoring.yml logs
                exit 1
              fi
            fi
            
            echo "Waiting for services to start (30s)..."
            sleep 30
            
            echo "Container status:"
            docker compose -f docker-compose.monitoring.yml ps
            
            echo "Checking services..."
            
            # Vérifier que les conteneurs sont en cours d'exécution
            echo "Verifying containers are running..."
            
            # Vérifier Prometheus
            if docker ps | grep -q prometheus; then
              PROMETHEUS_STATUS=$(docker ps --filter "name=prometheus" --format "{{.Status}}")
              echo "Prometheus is running: $PROMETHEUS_STATUS"
            else
              echo "ERROR: Prometheus container is not running"
              docker compose -f docker-compose.monitoring.yml logs prometheus | tail -30
              exit 1
            fi
            
            # Vérifier Grafana (peut prendre plus de temps pour le health check)
            if docker ps | grep -q grafana; then
              GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
              echo "Grafana is running: $GRAFANA_STATUS"
              
              # Si Grafana est encore en "health: starting", attendre un peu plus
              if echo "$GRAFANA_STATUS" | grep -q "health: starting"; then
                echo "Grafana is still starting, waiting additional 30s..."
                sleep 30
                GRAFANA_STATUS=$(docker ps --filter "name=grafana" --format "{{.Status}}")
                echo "Grafana status after wait: $GRAFANA_STATUS"
              fi
              
              # Vérifier que Grafana est toujours en cours d'exécution après l'attente
              if ! docker ps | grep -q grafana; then
                echo "ERROR: Grafana container stopped after startup"
                docker compose -f docker-compose.monitoring.yml logs grafana | tail -50
                exit 1
              fi
            else
              echo "ERROR: Grafana container is not running"
              docker compose -f docker-compose.monitoring.yml logs grafana | tail -50
              exit 1
            fi
            
            # Vérifier les autres services essentiels
            for service in alertmanager node-exporter cadvisor; do
              if docker ps | grep -q "$service"; then
                SERVICE_STATUS=$(docker ps --filter "name=$service" --format "{{.Status}}")
                echo "$service is running: $SERVICE_STATUS"
              else
                echo "WARNING: $service container is not running (non-critical)"
              fi
            done
            
            echo "Recent logs (last 10 lines):"
            docker compose -f docker-compose.monitoring.yml logs --tail=10
            
            echo "Monitoring stack deployed successfully!"
            echo "Prometheus should now scrape the backend on ${{ secrets.STAGING_HOST }}:8081"

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/monitoring_key ~/.ssh/config ~/.ssh/known_hosts

  # ============================================
  # RELEASE (semantic versioning + changelog)
  # ============================================
  release:
    name: Release
    runs-on: ubuntu-latest
    needs: [build, test]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    permissions:
      contents: write
      issues: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          # Cache désactivé car package-lock.json n'existe pas encore
          # cache: 'npm'
          # cache-dependency-path: package-lock.json

      - name: Install semantic-release dependencies
        run: npm install

      - name: Setup JDK ${{ env.JAVA_VERSION }}
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'

      - name: Run semantic-release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: npx semantic-release

